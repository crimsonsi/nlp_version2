Type,Category,Question,Answer
Technical,Statistics & Probability,Explain the central limit theorem and give examples of when you can use it in a real-world problem. ###,"The center limit theorem states that if any random variable, regardless of the distribution, is sampled a large enough time, the sample mean will be approximately normally distributed. This allows for studying the properties of any statistical distribution as long as there is a large enough sample size.Important remark from Adrian Olszewski: we can rely on the CLT with means (because it applies to any unbiased statistic) only if expressing data in this way makes sense. And it makes sense *ONLY* in the case of unimodal and symmetric data, coming from additive processes. So forget skewed, multi-modal data with mixtures of distributions, coming from multiplicative processes, and non-trivial mean-variance relationships. That are the places where arithmetic means is meaningless. Thus, using the CLT of e.g. bootstrap will give some valid answers to an invalid question. the distribution of means isn't enough. Every single kind of inference requires the entire test statistic to follow a certain distribution. And the test statistic consists also of the estimate of variance. Never assume the same sample size sufficient for means will suffice for the entire test statistic. See an excerpt from Rand Wilcox attached. Especially do never believe in magic numbers like N=30. think first about how to sensible describe your data, state the hypothesis of interest and then apply a valid method.Examples of real-world usage of CLT:1. The CLT can be used at any company with a large amount of data. Consider companies like Uber/Lyft wants to test whether adding a new feature will increase the booked rides or not using hypothesis testing. So if we have a large number of individual ride X, which in this case is a Bernoulli random variable (since the rider will book a ride or not), we can estimate the statistical properties of the total number of bookings. Understanding and estimating these statistical properties play a significant role in applying hypothesis testing to your data and knowing whether adding a new feature will increase the number of booked riders or not.2. Manufacturing plants often use the central limit theorem to estimate how many products produced by the plant are defective."
Technical,Statistics & Probability,Briefly explain the A/B testing and its application? What are some common pitfalls encountered in A/B testing? ###,"A/B testing helps us to determine whether a change in something will cause a change in performance significantly or not. So in other words you aim to statistically estimate the impact of a given change within your digital product (for example). You measure success and counter metrics on at least 1 treatment vs 1 control group (there can be more than 1 XP group for multivariate tests).Applications:1. Consider the example of a general store that sells bread packets but not butter, for a year. If we want to check whether its sale depends on the butter or not, then suppose the store also sells butter and sales for next year are observed. Now we can determine whether selling butter can significantly increase/decrease or doesn't affect the sale of bread.2. While developing the landing page of a website you create 2 different versions of the page. You define a criteria for success eg. conversion rate. Then define your hypothesisNull hypothesis(H): No difference between the performance of the 2 versions. Alternative hypothesis(H'): version A will perform better than B.NOTE: You will have to split your traffic randomly(to avoid sample bias) into 2 versions. The split doesn't have to be symmetric, you just need to set the minimum sample size for each version to avoid undersample bias.Now if version A gives better results than version B, we will still have to statistically prove that results derived from our sample represent the entire population. Now one of the very common tests used to do so is 2 sample t-test where we use values of significance level (alpha) and p-value to see which hypothesis is right. If p-value<alpha, H is rejected.Common pitfalls:1. Wrong success metrics inadequate to the business problem2. Lack of counter metric, as you might add friction to the product regardless along with the positive impact3. Sample mismatch: heterogeneous control and treatment, unequal variances4. Underpowered test: too small sample or XP running too short 5. Not accounting for network effects (introduce bias within measurement)"
Technical,Statistics & Probability,Describe briefly the hypothesis testing and p-value in laymans term? And give a practical application for them ? ###,"In Layman's terms:- Hypothesis test is where you have a current state (null hypothesis) and an alternative state (alternative hypothesis). You assess the results of both of the states and see some differences. You want to decide whether the difference is due to the alternative approach or not.You use the p-value to decide this, where the p-value is the likelihood of getting the same results the alternative approach achieved if you keep using the existing approach. It's the probability to find the result in the gaussian distribution of the results you may get from the existing approach.The rule of thumb is to reject the null hypothesis if the p-value < 0.05, which means that the probability to get these results from the existing approach is <95%. But this % changes according to task and domain.To explain the hypothesis testing in Layman's term with an example, suppose we have two drugs A and B, and we want to determine whether these two drugs are the same or different. This idea of trying to determine whether the drugs are the same or different is called hypothesis testing. The null hypothesis is that the drugs are the same, and the p-value helps us decide whether we should reject the null hypothesis or not.p-values are numbers between 0 and 1, and in this particular case, it helps us to quantify how confident we should be to conclude that drug A is different from drug B. The closer the p-value is to 0, the more confident we are that the drugs A and B are different."
Technical,Statistics & Probability,"Given a left-skewed distribution that has a median of 60, what conclusions can we draw about the mean and the mode of the data? ###","Left skewed distribution means the tail of the distribution is to the left and the tip is to the right. So the mean which tends to be near outliers (very large or small values) will be shifted towards the left or in other words, towards the tail.While the mode (which represents the most repeated value) will be near the tip and the median is the middle element independent of the distribution skewness, therefore it will be smaller than the mode and more than the mean.Mean < 60Mode > 60![Alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/1657144303401.jpg)"
Technical,Statistics & Probability,What is the meaning of selection bias and how to avoid it? ###,"Sampling bias is the phenomenon that occurs when a research study design fails to collect a representative sample of a target population. This typically occurs because the selection criteria for respondents failed to capture a wide enough sampling frame to represent all viewpoints.The cause of sampling bias almost always owes to one of two conditions.1. Poor methodology: In most cases, non-representative samples pop up when researchers set improper parameters for survey research. The most accurate and repeatable sampling method is simple random sampling where a large number of respondents are chosen at random. When researchers stray from random sampling (also called probability sampling), they risk injecting their own selection bias into recruiting respondents.2. Poor execution: Sometimes data researchers craft scientifically sound sampling methods, but their work is undermined when field workers cut corners. By reverting to convenience sampling (where the only people studied are those who are easy to reach) or giving up on reaching non-responders, a field worker can jeopardize the careful methodology set up by data scientists.The best way to avoid sampling bias is to stick to probability-based sampling methods. These include simple random sampling, systematic sampling, cluster sampling, and stratified sampling. In these methodologies, respondents are only chosen through processes of random selectioneven if they are sometimes sorted into demographic groups along the way.![Alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/Sampling%20bias.png)"
Technical,Statistics & Probability,Explain the long-tailed distribution and provide three examples of relevant phenomena that have long tails. Why are they important in classification and regression problems?,"A long-tailed distribution is a type of heavy-tailed distribution that has a tail (or tails) that drop off gradually and asymptotically.Three examples of relevant phenomena that have long tails:1. Frequencies of languages spoken2. Population of cities3. Pageviews of articlesAll of these follow something close to 80-20 rule: 80% of outcomes (or outputs) result from 20% of all causes (or inputs) for any given event. This 20% forms the long tail in the distribution.Its important to be mindful of long-tailed distributions in classification and regression problems because the least frequently occurring values make up the majority of the population. This can ultimately change the way that you deal with outliers, and it also conflicts with some machine learning techniques with the assumption that the data is normally distributed.![Alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/long-tailed%20distribution.jpg)"
Technical,Statistics & Probability,What is the meaning of KPI in statistics,"**KPI** stands for key performance indicator, a quantifiable measure of performance over time for a specific objective. KPIs provide targets for teams to shoot for, milestones to gauge progress, and insights that help people across the organization make better decisions. From finance and HR to marketing and sales, key performance indicators help every area of the business move forward at the strategic level.KPIs are an important way to ensure your teams are supporting the overall goals of the organization. Here are some of the biggest reasons why you need key performance indicators.* Keep your teams aligned: Whether measuring project success or employee performance, KPIs keep teams moving in the same direction.* Provide a health check: Key performance indicators give you a realistic look at the health of your organization, from risk factors to financial indicators.* Make adjustments: KPIs help you clearly see your successes and failures so you can do more of whats working, and less of whats not.* Hold your teams accountable: Make sure everyone provides value with key performance indicators that help employees track their progress and help managers move things along.Types of KPIsKey performance indicators come in many flavors. While some are used to measure monthly progress against a goal, others have a longer-term focus. The one thing all KPIs have in common is that theyre tied to strategic goals. Heres an overview of some of the most common types of KPIs.* **Strategic**: These big-picture key performance indicators monitor organizational goals. Executives typically look to one or two strategic KPIs to find out how the organization is doing at any given time. Examples include return on investment, revenue and market share.* **Operational:** These KPIs typically measure performance in a shorter time frame, and are focused on organizational processes and efficiencies. Some examples include sales by region, average monthly transportation costs and cost per acquisition (CPA).* **Functional Unit:** Many key performance indicators are tied to specific functions, such finance or IT. While IT might track time to resolution or average uptime, finance KPIs track gross profit margin or return on assets. These functional KPIs can also be classified as strategic or operational.* **Leading vs Lagging:** Regardless of the type of key performance indicator you define, you should know the difference between leading indicators and lagging indicators. While leading KPIs can help predict outcomes, lagging KPIs track what has already happened. Organizations use a mix of both to ensure theyre tracking whats most important.![Alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/KPI.png)"
Technical,Statistics & Probability,Say you flip a coin 10 times and observe only one head. What would be the null hypothesis and p-value for testing whether the coin is fair or not? ###,"The null hypothesis is that the coin is fair, and the alternative hypothesis is that the coin is biased. The p-value is the probability of observing the results obtained given that the null hypothesis is true, in this case, the coin is fair.In total for 10 flips of a coin, there are 2^10 = 1024 possible outcomes and in only 10 of them are there 9 tails and one head.Hence, the exact probability of the given result is the p-value, which is 10/1024 = 0.0098. Therefore, with a significance level set, for example, at 0.05, we can reject the null hypothesis."
Technical,Statistics & Probability,"You are testing hundreds of hypotheses, each with a t-test. What considerations would you take into account when doing this?","The main consideration when we have a large number of tests is that probability of getting a significant test due to chance alone increases. This will increase the type 1 error (rejecting the null hypothesis when it's actually true).Therefore we need to consider the Bonferroni Effect which happens when we make many tests. Ex. If our significance level is 0.05 but we made a 100 test it means that the probability of getting a value inside the rejection rejoin is 0.0005, not 0.05 so here we need to use another significance level which's called alpha star = significance level /K Where K is the number of the tests."
Technical,Statistics & Probability,What general conditions must be satisfied for the central limit theorem to hold? ###,"In order to apply the central limit theorem, there are four conditions that must be met:1.** Randomization:** The data must be sampled randomly such that every member in a population has an equal probability of being selected to be in the sample.2. **Independence:** The sample values must be independent of each other.3. **The 10% Condition:** When the sample is drawn without replacement, the sample size should be no larger than 10% of the population.4. **Large Sample Condition:** The sample size needs to be sufficiently large."
Technical,Statistics & Probability,What is skewness discuss two methods to measure it?,"Skewness refers to a distortion or asymmetry that deviates from the symmetrical bell curve, or normal distribution, in a set of data. If the curve is shifted to the left or to the right, it is said to be skewed.Skewness can be quantified as a representation of the extent to which a given distribution varies from a normal distribution. There are two main types of skewness negative skew which refers to a longer or fatter tail on the left side of the distribution, while positive skew refers to a longer or fatter tail on the right. These two skews refer to the direction or weight of the distribution.The mean of positively skewed data will be greater than the median. In a negatively skewed distribution, the exact opposite is the case: the mean of negatively skewed data will be less than the median. If the data graphs symmetrically, the distribution has zero skewness, regardless of how long or fat the tails are.There are several ways to measure skewness. Pearsons first and second coefficients of skewness are two common methods. Pearsons first coefficient of skewness, or Pearson mode skewness, subtracts the mode from the mean and divides the difference by the standard deviation. Pearsons second coefficient of skewness, or Pearson median skewness, subtracts the median from the mean, multiplies the difference by three, and divides the product by the standard deviation.![1663943424873](https://user-images.githubusercontent.com/72076328/191984720-5b267ab0-9ed3-4315-8443-62d662822796.jpg)"
Technical,Statistics & Probability,"Discuss the Chi-square, ANOVA, and t-test","Chi-square test A statistical method is used to find the difference or correlation between the observed and expected categorical variables in the dataset.Example: A food delivery company wants to find the relationship between gender, location, and food choices of people.It is used to determine whether the difference between 2 categorical variables is:* Due to chance or* Due to relationshipAnalysis of Variance (ANOVA) is a statistical formula used to compare variances across the means (or average) of different groups. A range of scenarios uses it to determine if there is any difference between the means of different groups.t_test is a statistical method for the comparison of the mean of the two groups of the normally distributed sample(s).It comes in various types such as:1. One sample t-test:Used to compare the mean of a sample and the population.2. Two sample t-tests:Used to compare the mean of two independent samples and whether their population is statistically different.3. Paired t-test:Used to compare means of different samples from the same group."
Technical,Statistics & Probability,What is the relationship between the significance level and the confidence level in Statistics?###,Confidence level = 1 - significance level.It's closely related to hypothesis testing and confidence intervals. Significance Level according to the hypothesis testing literature means the probability of Type-I error one is willing to tolerate. Confidence Level according to the confidence interval literature means the probability in terms of the true parameter value lying inside the confidence interval. They are usually written in percentages.
Technical,Statistics & Probability,What is the Law of Large Numbers in statistics and how it can be used in data science ?,"The law of large numbers states that as the number of trials in a random experiment increases, the average of the results obtained from the experiment approaches the expected value. In statistics, it's used to describe the relationship between sample size and the accuracy of statistical estimates.In data science, the law of large numbers is used to understand the behavior of random variables over many trials. It's often applied in areas such as predictive modeling, risk assessment, and quality control to ensure that data-driven decisions are based on a robust and accurate representation of the underlying patterns in the data.The law of large numbers helps to guarantee that the average of the results from a large number of independent and identically distributed trials will converge to the expected value, providing a foundation for statistical inference and hypothesis testing."
Technical,Statistics & Probability,"What is the difference between a confidence interval and a prediction interval, and how do you calculate them? ###","A confidence interval is a range of values that is likely to contain the true value of a population parameter with a certain level of confidence. It is used to estimate the precision or accuracy of a sample statistic, such as a mean or a proportion, based on a sample from a larger population.For example, if we want to estimate the average height of all adults in a certain region, we can take a random sample of individuals from that region and calculate the sample mean height. Then we can construct a confidence interval for the true population mean height, based on the sample mean and the sample size, with a certain level of confidence, such as 95%. This means that if we repeat the sampling process many times, 95% of the resulting intervals will contain the true population mean height.The formula for a confidence interval is:confidence interval = sample statistic +/- margin of errorThe margin of error depends on the sample size, the standard deviation of the population (or the sample, if the population standard deviation is unknown), and the desired level of confidence. For example, if the sample size is larger or the standard deviation is smaller, the margin of error will be smaller, resulting in a narrower confidence interval.A prediction interval is a range of values that is likely to contain a future observation or outcome with a certain level of confidence. It is used to estimate the uncertainty or variability of a future value based on a statistical model and the observed data.For example, if we have a regression model that predicts the sales of a product based on its price and advertising budget, we can use a prediction interval to estimate the range of possible sales for a new product with a certain price and advertising budget, with a certain level of confidence, such as 95%. This means that if we repeat the prediction process many times, 95% of the resulting intervals will contain the true sales value.The formula for a prediction interval is:prediction interval = point estimate +/- margin of errorThe point estimate is the predicted value of the outcome variable based on the model and the input variables. The margin of error depends on the residual standard deviation of the model, which measures the variability of the observed data around the predicted values, and the desired level of confidence. For example, if the residual standard deviation is larger or the level of confidence is higher, the margin of error will be larger, resulting in a wider prediction interval.![4](https://user-images.githubusercontent.com/72076328/227254955-b57bd42a-b51b-4b4a-abab-1adb059eca98.png)"
Technical,SQL,What are joins in SQL and discuss its types?,"A JOIN clause is used to combine rows from two or more tables, based on a related column between them. It is used to merge two tables or retrieve data from there. There are 4 types of joins: inner join left join, right join, and full join.* Inner join: Inner Join in SQL is the most common type of join. It is used to return all the rows from multiple tables where the join condition is satisfied. * Left Join: Left Join in SQL is used to return all the rows from the left table but only the matching rows from the right table where the join condition is fulfilled.* Right Join: Right Join in SQL is used to return all the rows from the right table but only the matching rows from the left table where the join condition is fulfilled.* Full Join: Full join returns all the records when there is a match in any of the tables. Therefore, it returns all the rows from the left-hand side table and all the rows from the right-hand side table."
Technical,SQL,"Define the primary, foreign, and unique keys and the differences between them?","**Primary key:** Is a key that is used to uniquely identify each row or record in the table, it can be a single column or composite pk that contains more than one column* The primary key doesn't accept null or repeated values* The purpose of the primary key is to keep the Entity's integrity* There is only one PK in each table* Every row must have a unique primary key**Foreign key:** Is a key that is used to identify, show or describe the relationship between tuples of two tables. It acts as a cross-reference between tables because it references the primary key of another table, thereby establishing a link between them.* The purpose of the foreign key is to keep data integrity* It can contain null values or primary key values**Unique key:** It's a key that can identify each row in the table as the primary key but it can contain one null value* Every table can have more than one Unique key"
Technical,SQL,What is the difference between BETWEEN and IN operators in SQL?,"The SQL **BETWEEN** operator selects values within a given range. It is inclusive of both the ranges, begin and end values are included. The values can be text, date, numbers, or otherFor example, select * from tablename where price BETWEEN 10 and 100;The **IN** operator is used to select rows in which a certain value exists in a given field. It is used with the WHERE clause to match values in a list.For example, select COLUMN from tablename where 'USA' in (country);IN is mainly best for categorical variables(it can be used with Numerical as well) whereas Between is for Numerical Variables"
Technical,SQL,Assume you have the given table below which contains information on user logins. Write a query to obtain the number of reactivated users (Users who did not log in the previous month and then logged in the current month),"First, we look at all the users who did not log in during the previous month. To obtain the last month's data, we subtract an of 1 month from the current month's login date. Then, we use against the previous month's interval to check whether there was login in the previous month. Finally, we the number of users satisfying this condition.```SELECT DATE_TRUNC('month', current_month.login_date) AS current_month, COUNT(*) AS num_reactivated_users FROM user_logins current_monthWHERE NOT EXISTS ( SELECT * FROM user_logins last_month WHERE DATE_TRUNC('month', last_month.login_date) BETWEEN DATE_TRUNC('month', current_month.login_date) AND DATE_TRUNC('month', current_month.login_date) - INTERVAL '1 month')```"
Technical,SQL,Describe the advantages and disadvantages of relational database vs NoSQL databases,"Advantages of Relational Databases:** Ensure data integrity through a defined schema and ACID properties. Easy to get started with and use for small-scale applications. Lends itself well to vertical scaling. Uses an almost standard query language, making learning or switching between types of relational databases easy.**Advantages of NoSQL Databases:** Offers more flexibility in data format and representations, which makes working with Unstructured or semistructured data easier. Hence, useful when still the data schema or adding new features/functionality rapidly like in a startup environment to scale with horizontal scaling. Lends itself better to applications that need to be highly available. **Disadvantages of Relational Databases:** Data schema needs to be known in advance. Ale schemas is possible, but frequent changes to the schema for large tables can cause performance issues. Horizontal scaling is relatively difficult, leading to eventual performance bottlenecks**Disadvantages of NoSQL Databases:** As outlined by the BASE framework, weaker guarantees of data correctness are made due to the soft-state and eventual consistency property. Managing consistency can also be difficult due to the lack of a predefined schema that's strictly adhered to. Depending on the type of NoSQL database, it can be challenging for the database to handle its types of complex queries or access patterns."
Technical,SQL,Assume you are given the table below on user transactions. Write a query to obtain the third transaction of every user,"First, we obtain the transaction numbers for each user. We can do this by using the ROW_NUMBER window function, where we PARTITION by the user_id and ORDER by the transaction_date fields, calling the resulting field a transaction number. From there, we can simply take all transactions having a transaction number equal to 3."
Technical,SQL,What do you understand by Self Join? Explain using an example,"Self-join is as its name implies, joining a table to itself on a database, this process may come in handy in a number of cases, such as:1- comparing the table's rows to themselves:It's like we have two copies of the same table and join them together on a given condition to reach the required output query.Ex. If we have a store database with a client's data table holding a bunch of demographics, we could self-join the client's table to get clients who are located in the same city/made a purchase on the same day/etc.2- querying a table that has hierarchical data:Meaning, the table has a primary key that has a one-to-many relationship with another foreign key inside the same table, in other words, the table has data that refers to the same table. We could use self-join in order to have a clear look at the data by matching its keys.Ex. The organizational structure of a company may contain an employee table that has an employee id and his manager id (who is also an employee, hence has an employee id too) in the same table. Using self-join on this table would allow us to reference every employee directly to his manager.P.S. we would need to take care of duplicates that may occur and consider them in the conditions."
Technical,SQL,What is the difference between temporary tables and common table expressions?,"A temporary table is a physical table that is created in the database and persists until it is explicitly dropped or the session ends. A CTE is a virtual table that is defined only within the scope of a single SQL statement.: Temporary tables are stored in the database and occupy physical disk space. CTEs are not stored on disk and exist only in memory for the duration of the query.: Temporary tables can be accessed from any session that has the appropriate privileges. CTEs are only accessible within the scope of the query in which they are defined.: Temporary tables persist until they are explicitly dropped or the session ends. CTEs are only available for the duration of the query in which they are defined and are then discarded.: Temporary tables are created using the CREATE TEMPORARY TABLE statement, while CTEs are defined using the WITH clause.: Temporary tables are typically used to store intermediate results that will be used in multiple queries, while CTEs are used to simplify complex queries by breaking them down into smaller, more manageable parts.In summary, temporary tables are physical tables that persist in the database and can be accessed from any session, while CTEs are virtual tables that exist only within the scope of a single query and are discarded once the query is complete. Both temporary tables and CTEs can be useful tools for simplifying complex queries and storing intermediate results."
Technical,SQL,Why use Right Join When Left Join can suffice the requirement?,"In MySQL, the are used to retrieve data from multiple tables by joining them based on a specified condition.Generally, the is used more frequently than the because it returns all the rows from the left table and matching rows from the right table, or NULL values if there is no match.In most cases, a is sufficient to meet the requirement of retrieving all the data from the left table and matching data from the right table.However, there may be situations where using a is more appropriate.Here are a few examples:. : If the right table contains the primary data that needs to be retrieved, and the left table contains supplementary data, a can be used to retrieve all the data from the right table and matching data from the left table.. : In some cases, a may be more efficient than a because the database optimizer can choose the most efficient join order based on the query structure and the available indexes.. : If the query requires an outer join, a may be used to return all the rows from the right table, including those with no matching rows in the left table.It's important to note that while a can provide additional functionality in certain cases, it may also make the query more complex and difficult to read. In most cases, a is the preferred method for joining tables in MySQL."
Technical,SQL,Why Rank skips sequence?,"In MySQL, the rank function may skip a sequence of numbers when using the `DENSE_RANK()` function or the `RANK()` function, depending on the data and the query. The `DENSE_RANK()` function assigns a unique rank to each distinct value in a result set, whereas the `RANK()` function assigns the same rank to the duplicate values.Here are some of the reasons why the rank function may skip a sequence in MySQL:1. `_()` function skips ranks when there are ties. For example, if there are two rows with the same values in the ranking column, both will be assigned the same rank, and the next rank will be incremented by 1.2. `()` function skips ranks when there are gaps between the duplicate values. For example, if there are three rows with the same values in the ranking column, and then the next row has a higher value, the `RANK()` function will skip over the fourth rank.3. The query may have filtering or grouping clauses that affect the ranking. For example, if a query filters out some rows or groups them by a different column, the ranking may not be sequential.It's important to note that the ranking function in MySQL behaves differently from the ranking function in other databases, so the same query may produce different results in different database systems."
Technical,Python,"Given two arrays, write a python function to return the intersection of the two? For example, X = [1,5,9,0] and Y = [3,0,2,9] it should return [9,0]",set(X).intersect (set(Y))
Technical,Python,"Given an array, find all the duplicates in this array? For example: input: [1,2,3,1,3,6,5] output: [1,3]",set1=set()res=set()for i in list: if i in set1: res.add(i) else: set1.add(i)print(res)
Technical,Python,"Given an integer array, return the maximum product of any three numbers in the array?","import heapqdef max_three(arr): a = heapq.nlargest(3, arr) # largerst 3 numbers for postive case b = heapq.nsmallest(2, arr) # for negative case return max(a[2]*a[1]*a[0], b[1]*b[0]*a[0])"
Technical,Python,"Given an integer array, find the sum of the largest contiguous subarray within the array. For example, given the array A = [0,-1,-5,-2,3,14] it should return 17 because of [3,14]. Note that if all the elements are negative it should return zero.","def max_subarray(arr): n = len(arr) max_sum = arr[0] #max curr_sum = 0 for i in range(n): curr_sum += arr[i] max_sum = max(max_sum, curr_sum) if curr_sum <0: curr_sum = 0 return max_sum"
Technical,Python,Define tuples and lists in Python What are the major differences between them?,"Lists:In Python, a list is created by placing elements inside square brackets [], separated by commas. A list can have any number of items and they may be of different types (integer, float, string, etc.). A list can also have another list as an item. This is called a nested list.1. Lists are mutable2. Lists are better for performing operations, such as insertion and deletion.3. Lists consume more memory4. Lists have several built-in methodsTuples:A tuple is a collection of objects which ordered and immutable. Tuples are sequences, just like lists. The differences between tuples and lists are, the tuples cannot be changed unlike lists and tuples use parentheses, whereas lists use square brackets.1. Tuples are immutable2. Tuple data type is appropriate for accessing the elements3. Tuples consume less memory as compared to the list4. Tuple does not have many built-in methods.* Mutable = we can change, add, delete and modify stuff* Immutable = we cannot change, add, delete and modify stuff"
Technical,Python,"Given an integer n and an integer K, output a list of all of the combination of k numbers chosen from 1 to n. For example, if n=3 and k=2, return [1,2],[1,3],[2,3]","from itertools import combinationsdef find_combintaion(k,n): list_num = [] comb = combinations([x for x in range(1, n+1)],k) for i in comb: list_num.append(i) print(""(K:{},n:{}):"".format(k,n)) print(list_num,""\n"")```"
Technical,Python,Write a function to generate N samples from a normal distribution and plot them on the histogram,"Using bultin Libraries:```import numpy as npimport matplotlib.pyplot as pltx = np.random.randn((N,))plt.hist(x)```"
Technical,Python,What is the difference between apply and applymap function in pandas?,"Both the methods only accept callables as arguments but what sets them apart is that applymap is defined on dataframes and works element-wise. While apply can be defined on data frames as well as series and can work row/column-wise as well as element-wise. In terms of use case, applymap is used for transformations while apply is used for more complex operations and aggregations. Applymap only returns a dataframe while apply can return a scalar value, series, or dataframe."
Technical,Python,"Given a string, return the first recurring character in it, or None if there is no recurring character. Example: input = ""pythoninterviewquestion"" , output = ""n""","input_string = ""pythoninterviewquestion""def first_recurring(input_str): a_str = """" for letter in input_str: a_str = a_str + letter if a_str.count(letter) > 1: return letter return Nonefirst_recurring(input_string)"
Technical,Python,"Given a positive integerX return an integer that is a factorial ofX. If a negative integer is provided, return -1. Implement the solution by using a recursive function.",def factorial(x): # Edge cases if x < 0: return -1 if x == 0: return 1 # Exit condition - x = 1 if x == 1: return x else: # Recursive part return x * factorial(x - 1)
Technical,Python,"Given an m-by-n matrix with positive integers, determine the length of the longest path of increasing within the matrix. For example, consider the input matrix: [ 1 2 3 ] [ 4 5 6 ] [ 7 8 9 ] The answer should be 5 since the longest path would be 1-2-5-6-9","MAX = 10def Longest_Increasing_Path(dp, mat, n, m, x, y): # If value not calculated yet. if (dp[x][y] < 0): result = 0 # // If reach bottom right cell, return 1 if (x == n - 1 and y == m - 1): dp[x][y] = 1 return dp[x][y] # If reach the corner # of the matrix. if (x == n - 1 or y == m - 1): result = 1 # If value greater than below cell. if (x + 1 < n and mat[x][y] < mat[x + 1][y]): result = 1 + LIP(dp, mat, n, m, x + 1, y) # If value greater than left cell. if (y + 1 < m and mat[x][y] < mat[x][y + 1]): result = max(result, 1 + LIP(dp, mat, n, m, x, y + 1)) dp[x][y] = result return dp[x][y] # Wrapper functiondef wrapper(mat, n, m): dp = [[-1 for i in range(MAX)] for i in range(MAX)] return Longest_Increasing_Path(dp, mat, n, m, 0, 0)```"
Technical,Statistics & Probability,"You and your friend are playing a game with a fair coin. The two of you will continue to toss the coin until the sequence HH or TH shows up. If HH shows up first, you win, and if TH shows up first your friend win. What is the probability of you winning the game?","First flip is either heads or tails. If the second flip is heads we have a winner no matter what. Hence we have a 1/2 chance of game ending on the second flip.If first flip is H, and the second flip is H, then player 1 wins. If first flip is H, and second flip is T, the game goes on. Generalizing, if the last flip was T, then HH will never occur and player 1 has no chance of wining. Either the game goes on OR player 2 wins OR the game goes on AND player 2 wins. Player 1 can only win if the first flip is H and the second flip is H. Consider the following four scenarios- HH : Player 1 wins- HT ... ? : HH will never occur before TH. Player 2 wins.- TT ... ? : HH will never occur before TH. Player 2 wins.- TH : Player 2 wins.Hence probability of player 1 wining is 1/4 and probability of player 2 wining is 3/4."
Technical,Statistics & Probability,"If you roll a dice three times, what is the probability to get two consecutive threes?","The right answer is 11/216There are different ways to answer this question:1. If we roll a dice three times we can get two consecutive 3s in three ways:1. The first two rolls are 3s and the third is any other number with a probability of 1/6 * 1/6 * 5/6.2. The first one is not three while the other two rolls are 3s with a probability of 5/6 * 1/6 * 1/63. The last one is that the three rolls are 3s with probability 1/6 ^ 3So the final result is 2 * (5/6 * (1/6)^2) + (1/6)*3 = 11/216By Inclusion-Exclusion Principle:Probability of at least two consecutive threes= Probability of two consecutive threes in first two rolls + Probability of two consecutive threes in last two rolls - Probability of three consecutive threes= 2 * Probability of two consecutive threes in first two rolls - Probability of three consecutive threes= 2 * (1/6) * (1/6) - (1/6) * (1/6) * (1/6) = 11/216It can be seen also like this:The sample space is made of (x, y, z) tuples where each letter can take a value from 1 to 6, therefore the sample space has 6x6x6=216 values, and the number of outcomes that are considered two consecutive threes is (3,3, X) or (X, 3, 3), the number of possible outcomes is therefore 6 for the first scenario (3,3,1) till (3,3,6) and 6 for the other scenario (1,3,3) till (6,3,3) and subtract the duplicate (3,3,3) which appears in both, and this leaves us with a probability of 11/216."
Technical,Statistics & Probability,"Suppose you have ten fair dice. If you randomly throw them simultaneously, what is the probability that the sum of all of the top faces is divisible by six?","1/6Explanation:With 10 dices, the possible sums divisible by 6 are 12, 18, 24, 30, 36, 42, 48, 54, and 60. You don't actually need to calculate the probability of getting each of these numbers as the final sums from 10 dices because no matter what the sum of the first 9 numbers is, you can still choose a number between 1 to 6 on the last die and add to that previous sum to make the final sum divisible by 6. Therefore, we only care about the last die. And the probability to get that number on the last die is 1/6. So the answer is 1/6"
Technical,Statistics & Probability,"If you have three draws from a uniformly distributed random variable between 0 and 2, what is the probability that the median of three numbers is greater than 1.5?","The right answer is 5/32 or 0.156. There are different methods to solve it:* **Method 1:**To get a median greater than 1.5 at least two of the three numbers must be greater than 1.5. The probability of one number being greater than 1.5 in this distribution is 0.25. Then, using the binomial distribution with three trials and a success probability of 0.25 we compute the probability of 2 or more successes to get the probability of the median is more than 1.5, which would be about 15.6%.* **Method2 :**A median greater than 1.5 will occur when o all three uniformly distributed random numbers are greater than 1.5 or 1 uniform distributed random number between 0 and 1.5 and the other two are greater than 1.5.So, the probability of the above event is= {(2 - 1.5) / 2}^3 + (3 choose 1)(1.5/2)(0.5/2)^2= 10/64 = 5/32* **Method3:**Using the Monte Carlo method as shown in the figure below:![Alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/Monte%20Carlo%20Methods.png)"
Technical,Statistics & Probability,"Assume you have a deck of 100 cards with values ranging from 1 to 100 and you draw two cards randomly without replacement, what is the probability that the number of one of them is double the other?",There are a total of (100 C 2) = 4950 ways to choose two cards at random from the 100 cards and there are only 50 pairs of these 4950 ways that you will get one number and it's double. Therefore the probability that the number of one of them is double the other is 50/4950.
Technical,Statistics & Probability,What is the difference between the Bernoulli and Binomial distribution?,"Bernoulli and Binomial are both types of probability distributions.The function of Bernoulli is given byp(x) =p^x * q^(1-x) , x=[0,1] Mean is pVariance p*(1-p)The function Binomial is given by:p(x) = nCx p^x q^(n-x) x=[0,1,2...n]Mean : npVariance :npqWhere p and q are the probability of success and probability of failure respectively, n is the number of independent trials and x is the number of successes.As we can see sample space( x ) for Bernoulli distribution is Binary (2 outcomes), and just a single trial. Eg: A loan sanction for a person can be either a success or a failure, with no other possibility. (Hence single trial).Whereas for Binomial the sample space(x) ranges from 0 -n.Eg. Tossing a coin 6 times, what is the probability of getting 2 or a few heads? Here sample space is x=[0,1,2] and more than 1 trial and n=6(finite)In short, Bernoulli Distribution is a single trial version of Binomial Distribution."
Technical,Statistics & Probability,"If there are 30 people in a room, what is the probability that everyone has different birthdays?","The sample space is 365^30 and the number of events is 365p30 because we need to choose persons without replacement to get everyone to have a unique birthday therefore the Prob = 356p30 / 365^30 = 0.2936A theoretical explanation is provided in the figure below thanks to Fazil Mohammed.> Note: Why do we use permutations and not combinations here? <br> <br>When calculating 365C30, you are saying: Out of 365 days, I'm choosing 30 distinct days, but I don't care in what order they are assigned to people. This treats the selection of birthdays as unordered, which isn't the case in the birthday problem, because who gets which birthday is important. <br>For example, if you selected 30 distinct birthdays (as in a combination), this would only tell you which 30 birthdays are used, but it wouldn't account for the fact that different people being assigned different birthdays creates different outcomes. In contrast, with permutations, we are considering the specific assignment of each person to a particular birthday, where the order matters because we care about which person gets which birthday. i.e. if Person A is born on 01/01 and person B is born on 02/02, its different than if Person A is born on 02/02 and person B is born on 01/01.Interesting facts provided by Rishi Dey Chowdhury:1. With just 23 people there is over 50% chance of a birthday match and with 57 people the match probability exceeds 99%. One intuition to think of why with such a low number of people the probability of a match is so high. It's because for a match we require a pair of people and 23 choose 2 is 23*11 = 253 which is a relatively big number and ya 50% sounds like a decent probability of a match for this case.2. Another interesting fact is if the assumption of equal probability of birthday of a person on any day out of 365 is violated and there is a non-equal probability of birthday of a person among days of the year then, it is even more likely to have a birthday match.![Alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/Therotical%20Explanation%20Q%207%20Probability.jfif)"
Technical,Statistics & Probability,"Assume two coins, one fair and the other is unfair. You pick one at random, flip it five times, and observe that it comes up as tails all five times. What is the probability that you are fliping the unfair coin? Assume that the unfair coin always results in tails.","Let's use Bayes theorem let U denote the case where you are flipping the unfair coin and F denote the case where you are flipping the fair coin. Since the coin is chosen randomly, we know that P(U)=P(F)=0.5. Let 5T denote the event of flipping 5 tails in a row.Then, we are interested in solving for P(U|5T) (the probability that you are flipping the unfair coin given that you obtained 5 tails). Since the unfair coin always results in tails, therefore P(5T|U) = 1 and also P(5T|F) =1/2 = 1/32 by the definition of a fair coin.Lets apply Bayes theorem where P(U|5T) = P(5T|U) * P(U) / P(5T|U)* P(U) + P(5T|F)* P(F) = 0.5 / 0.5 +0.5* 1/32 = 0.97Therefore the probability that you picked the unfair coin is 97%"
Technical,Statistics & Probability,Assume you take a stick of length 1 and you break it uniformly at random into three parts. What is the probability that the three pieces can be used to form a triangle?,"The right answer is 0.25Let's say, x and y are the lengths of the two parts, so the length of the third part will be 1-x-yAs per the triangle inequality theorem, the sum of two sides should always be greater than the third side. Therefore, no two lengths can be more than 1/2.x<1/2y<1/2To achieve this the first breaking point (X) should before the 0.5 mark on the stick and the second breaking point (Y) should be after the 0.5 mark on the stick.P(X < 0.5) = (0.5-0) / (1-0) = 0.5P(Y > 0.5) = (1 - 0.5) / (1-0) = 0.5Hence, overal probability = P(X < 0.5) * P(Y > 0.5) = 1/5 = 0.25"
Technical,Statistics & Probability,Say you draw a circle and choose two chords at random. What is the probability that those chords will intersect?,"For making 2 chords, 4 points are necessary and from 4 points there are 3 different combinations of pairs of chords can be made. From the 3 combinations, there is only one combination in which the two chords intersect hence answer is 1/3.Let's assume that P1, P2, P3, and P4 are four points then 3 different combinations are possible for pairs of chords: (P1 P2) (P3 P4) or (P1 P3) (P4 P2) or (P1 P4) (P2 P3) there the 3rd one will only intersect.![Probability question 70](https://user-images.githubusercontent.com/72076328/189387820-1a4fb356-d8a9-4054-9475-09e3ad5bc872.png)"
Technical,Statistics & Probability,"If theres a 15% probability that you might see at least one airplane in a five-minute interval, what is the probability that you might see at least one airplane in a period of half an hour?",Probability of at least one plane in 5 mins interval=0.15Probability of no plane in 5 mins interval=0.85Probability of seeing at least one plane in 30 mins=1 - Probability of not seeing any plane in 30 minutes=1-(0.85)^6 = 0.6228This problem can also be solved using Poisson distribution. Refer this [blog post](https://towardsdatascience.com/shooting-star-problem-simple-solution-and-poisson-process-demonstration-739e94184edf).
Technical,Statistics & Probability,"According to hospital records, 75% of patients suffering from a disease die from that disease. Find out the probability that 4 out of the 6 randomly selected patients survive.","This has to be a binomial since there are only 2 outcomes death or life. Here n =6, and x=4. p=0.25 (probability if life) q = 0.75(probability of death)Using probability mass function equation:P(X) = nCx * p^x * q^(n-x) Then:P(4) = 6C4 * (0.25)^4 * (0.75)^2 = 0.032"
Technical,Statistics & Probability,Discuss some methods you will use to estimate the Parameters of a Probability Distribution,"There are different ways you can go about this. Following are some methods, one may choose only one of these or a combination depending on the observed data.- Method of moments- Maximum Likelihood Estimatation- Bayesian Estimation- Least Squares Estimation- Method of Least Absolute Deviation- Chi-squared Test"
Technical,Statistics & Probability,"You have 40 cards in four colors, 10 reds, 10 greens, 10 blues, and ten yellows. Each color has a number from 1 to 10. When you pick two cards without replacement, what is the probability that the two cards are not in the same color and not in the same number?","Since it doesn't matter how you choose the first card, so, choose one card at random.Now, all we have to care about is the restriction on the second card. It can't be the same number (i.e. 3 cards from the other colors can't be chosen in favorable cases) and also can't be the same color (i.e. 9 cards from the same color can't be chosen keep in mind we have already picked one).So, the number of favorable choices for the 2nd card is (39-12)/39 = 27/39 = 9/13![1668961881451](https://user-images.githubusercontent.com/72076328/202913961-f94f17b1-dc41-45b2-ba51-389583431d7b.jpg)"
Technical,Statistics & Probability,Can you explain the difference between frequentist and Bayesian probability approaches?,"The frequentist approach to probability defines probability as the long-run relative frequency of an event in an infinite number of trials. It views probabilities as fixed and objective, determined by the data at hand. In this approach, the parameters of a model are treated as fixed and unknown and estimated using methods like maximum likelihood estimation.On the other hand, Bayesian probability defines probability as a degree of belief, or the degree of confidence, in an event. It views probabilities as subjective and personal, representing an individual's beliefs. In this approach, the parameters of a model are treated as random variables with prior beliefs, which are updated as new data becomes available to form a posterior belief.In summary, the frequentist approach deals with fixed and objective probabilities and uses methods like estimation, while the Bayesian approach deals with subjective and personal probabilities and uses methods like updating prior beliefs with new data."
Technical,Statistics & Probability,Explain the Difference Between Probability and Likelihood,"Probability and likelihood are two concepts that are often used in statistics and data analysis, but they have different meanings and uses.Probability is the measure of the likelihood of an event occurring. It is a number between 0 and 1, with 0 indicating an impossible event and 1 indicating a certain event. For example, the probability of flipping a coin and getting heads is 0.5.The likelihood, on the other hand, is the measure of how well a statistical model or hypothesis fits a set of observed data. It is not a probability, but rather a measure of how plausible the data is given the model or hypothesis. For example, if we have a hypothesis that the average height of people in a certain population is 6 feet, the likelihood of observing a random sample of people with an average height of 5 feet would be low."
Technical,ML,Mention three ways to make your model robust to outliers.,"Investigating the outliers is always the first step in understanding how to treat them. After you understand the nature of why the outliers occurred you can apply one of the several methods mentioned [here](https://365datascience.com/career-advice/job-interview-tips/machine-learning-interview-questions-and-answers/#11:~:text=for%20large%20datasets.-,Bonus%20Question%3A%20Discuss%20how%20to%20make%20your%20model%20robust%20to%20outliers.,-There%20are%20several)."
Technical,ML,Describe the motivation behind random forests and mention two reasons why they are better than individual decision trees.,"The motivation behind random forest or ensemble models in general in layman's terms, Let's say we have a question/problem to solve we bring 100 people and ask each of them the question/problem and record their solution. The rest of the answer is [here](https://365datascience.com/career-advice/job-interview-tips/machine-learning-interview-questions-and-answers/#2:~:text=2.%20Describe%20the%20motivation%20behind%20random%20forests%20and%20mention%20two%20reasons%20why%20they%20are%20better%20than%20individual%20decision%C2%A0trees.)"
Technical,ML,What are the differences and similarities between gradient boosting and random forest? and what are the advantages and disadvantages of each when compared to each other?,Similarities:1. Both these algorithms are decision-tree-based algorithms2. Both these algorithms are ensemble algorithms3. Both are flexible models and do not need much data preprocessing.
Technical,ML,What are L1 and L2 regularization? What are the differences between the two?,Regularization is a technique used to avoid overfitting by trying to make the model more simple.
Technical,ML,What are the Bias and Variance in a Machine Learning Model and explain the bias-variance trade-off?,The goal of any supervised machine learning model is to estimate the mapping function (f) that predicts the target variable (y) given input (x). The prediction error can be broken down into three parts:The rest of the answer is [here](https://365datascience.com/career-advice/job-interview-tips/machine-learning-interview-questions-and-answers/#2:~:text=8.%20What%20are%20the%20bias%20and%20variance%20in%20a%20machine%20learning%20model%20and%20explain%20the%20bias%2Dvariance%20trade%2Doff%3F)
Technical,ML,Mention three ways to handle missing or corrupted data in a dataset.,"In general, real-world data often has a lot of missing values. The cause of missing values can be data corruption or failure to record data."
Technical,ML,Explain briefly the logistic regression model and state an example of when you have used it recently.,"Logistic regression is used to calculate the probability of occurrence of an event in the form of a dependent output variable based on independent input variables. Logistic regression is commonly used to estimate the probability that an instance belongs to a particular class. If the probability is bigger than 0.5 then it will belong to that class (positive) and if it is below 0.5 it will belong to the other class. This will make it a binary classifier.It is important to remember that the Logistic regression isn't a classification model, it's an ordinary type of regression algorithm, and it was developed and used before machine learning, but it can be used in classification when we put a threshold to determine specific categories""There is a lot of classification applications to it:Classify email as spam or not, To identify whether the patient is healthy or not, and so on."
Technical,ML,"Explain briefly batch gradient descent, stochastic gradient descent, and mini-batch gradient descent. and what are the pros and cons for each of them?","Gradient descent is a generic optimization algorithm cable for finding optimal solutions to a wide range of problems. The general idea of gradient descent is to tweak parameters iteratively in order to minimize a cost function.Batch Gradient Descent:In Batch Gradient descent the whole training data is used to minimize the loss function by taking a step toward the nearest minimum by calculating the gradient (the direction of descent)Pros:Since the whole data set is used to calculate the gradient it will be stable and reach the minimum of the cost function without bouncing (if the learning rate is chosen cooreclty)Cons:Since batch gradient descent uses all the training set to compute the gradient at every step, it will be very slow especially if the size of the training data is large.Stochastic Gradient Descent:Stochastic Gradient Descent picks up a random instance in the training data set at every step and computes the gradient based only on that single instance.Pros:1. It makes the training much faster as it only works on one instance at a time.2. It become easier to train large datasetsCons:Due to the stochastic (random) nature of this algorithm, this algorithm is much less regular than the batch gradient descent. Instead of gently decreasing until it reaches the minimum, the cost function will bounce up and down, decreasing only on average. Over time it will end up very close to the minimum, but once it gets there it will continue to bounce around, not settling down there. So once the algorithm stops the final parameters are good but not optimal. For this reason, it is important to use a training schedule to overcome this randomness.Mini-batch Gradient:At each step instead of computing the gradients on the whole data set as in the Batch Gradient Descent or using one random instance as in the Stochastic Gradient Descent, this algorithm computes the gradients on small random sets of instances called mini-batches.Pros: 1. The algorithm's progress space is less erratic than with Stochastic Gradient Descent, especially with large mini-batches.2. You can get a performance boost from hardware optimization of matrix operations, especially when using GPUs.Cons: 1. It might be difficult to escape from local minima.![alt text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/gradient%20descent%20vs%20batch%20gradient%20descent.png)"
Technical,ML,Explain what is information gain and entropy in the context of decision trees.,"Entropy and Information Gain are two key metrics used in determining the relevance of decision-making when constructing a decision tree model and determining the nodes and the best way to split.The idea of a decision tree is to divide the data set into smaller data sets based on the descriptive features until we reach a small enough set that contains data points that fall under one label.Entropy is the measure of impurity, disorder, or uncertainty in a bunch of examples. Entropy controls how a Decision Tree decides to split the data.Information gain calculates the reduction in entropy or surprise from transforming a dataset in some way. It is commonly used in the construction of decision trees from a training dataset, by evaluating the information gain for each variable and selecting the variable that maximizes the information gain, which in turn minimizes the entropy and best splits the dataset into groups for effective classification."
Technical,ML,Explain the linear regression model and discuss its assumption.,"Linear regression is a supervised statistical model to predict dependent variable quantity based on independent variables.Linear regression is a parametric model and the objective of linear regression is that it has to learn coefficients using the training data and predict the target value given only independent values.Some of the linear regression assumptions and how to validate them:1. Linear relationship between independent and dependent variables2. Independent residuals and the constant residuals at every xWe can check for 1 and 2 by plotting the residuals(error terms) against the fitted values (upper left graph). Generally, we should look for a lack of patterns and a consistent variance across the horizontal line.3. Normally distributed residualsWe can check for this using a couple of methods:* Q-Q-plot(upper right graph): If data is normally distributed, points should roughly align with the 45-degree line.* Boxplot: it also helps visualize outliers* ShapiroWilk test: If the p-value is lower than the chosen threshold, then the null hypothesis (Data is normally distributed) is rejected.4. Low multicollinearity* you can calculate the VIF (Variable Inflation Factors) using your favorite statistical tool. If the value for each covariate is lower than 10 (some say 5), you're good to go.The figure below summarizes these assumptions.![alt text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/Linear%20regression%20assumptions.jpg)"
Technical,ML,Explain briefly the K-Means clustering and how can we find the best value of K?,K-Means is a well-known clustering algorithm. K-means clustering is often used because it is easy to interpret and implement.
Technical,ML,"Define Precision, recall, and F1 and discuss the trade-off between them?",Precision and recall are two classification evaluation metrics that are used beyond accuracy.
Technical,ML,What are the differences between a model that minimizes squared error and the one that minimizes the absolute error? and in which cases each error metric would be more appropriate?,"Both mean square error (MSE) and mean absolute error (MAE) measures the distances between vectors and express average model prediction in units of the target variable. Both can range from 0 to infinity, the lower they are the better the model.The main difference between them is that in MSE the errors are squared before being averaged while in MAE they are not. This means that a large weight will be given to large errors. MSE is useful when large errors in the model are trying to be avoided. This means that outliers affect MSE more than MAE, that is why MAE is more robust to outliers. Computation-wise MSE is easier to use as the gradient calculation will be more straightforward than MAE, which requires linear programming to calculate it."
Technical,ML,Define and compare parametric and non-parametric models and give two examples for each of them?,"**Parametric models** assume that the dataset comes from a certain function with some set of parameters that should be tuned to reach the optimal performance. For such models, the number of parameters is determined prior to training, thus the degree of freedom is limited and reduces the chances of overfitting.Ex. Linear Regression, Logistic Regression, LDA**Nonparametric models** don't assume anything about the function from which the dataset was sampled. For these models, the number of parameters is not determined prior to training, thus they are free to generalize the model based on the data. Sometimes these models overfit themselves while generalizing. To generalize they need more data in comparison with Parametric Models. They are relatively more difficult to interpret compared to Parametric Models.Ex. Decision Tree, Random Forest."
Technical,ML,Explain the kernel trick in SVM. Why do we use it and how to choose what kernel to use?,"Kernels are used in SVM to map the original input data into a particular higher dimensional space where it will be easier to find patterns in the data and train the model with better performance.For eg.: If we have binary class data which form a ring-like pattern (inner and outer rings representing two different class instances) when plotted in 2D space, a linear SVM kernel will not be able to differentiate the two classes well when compared to an RBF (radial basis function) kernel, mapping the data into a particular higher dimensional space where the two classes are clearly separable.Typically without the kernel trick, in order to calculate support vectors and support vector classifiers, we need first to transform data points one by one to the higher dimensional space, do the calculations based on SVM equations in the higher dimensional space, and then return the results. The trick in the kernel trick is that we design the kernels based on some conditions as mathematical functions that are equivalent to a dot product in the higher dimensional space without even having to transform data points to the higher dimensional space. i.e. we can calculate support vectors and support vector classifiers in the same space where the data is provided which saves a lot of time and calculations.Having domain knowledge can be very helpful in choosing the optimal kernel for your problem, however, in the absence of such knowledge following this default rule can be helpful:For linear problems, we can try linear or logistic kernels, and for nonlinear problems, we can use RBF or Gaussian kernels."
Technical,ML,Define the cross-validation process and the motivation behind using it.,"Cross-validation is a technique used to assess the performance of a learning model in several subsamples of training data. In general, we split the data into train and test sets where we use the training data to train our model and the test data to evaluate the performance of the model on unseen data and validation set for choosing the best hyperparameters. Now, a random split in most cases(for large datasets) is fine. However, for smaller datasets, it is susceptible to loss of important information present in the data in which it was not trained. Hence, cross-validation though computationally expensive combats this issue.The process of cross-validation is as follows:1. Define k or the number of folds2. Randomly shuffle the data into K equally-sized blocks (folds)3. For each i in fold 1 to k train the data using all the folds except for fold i and test on the fold i.3. Average the K validation/test error from the previous step to get an estimate of the error.This process aims to accomplish the following:1- Prevent overfitting during training by avoiding training and testing on the same subset of the data points2- Avoid information loss by using a certain subset of the data for validation only. This is important for small datasets.Cross-validation is always good to be used for small datasets, and if used for large datasets the computational complexity will increase depending on the number of folds."
Technical,ML,"You are building a binary classifier and you found that the data is imbalanced, what should you do to handle this situation?","If there is a data imbalance there are several measures we can take to train a fairer binary classifier:**1. Pre-Processing:*** Check whether you can get more data or not.* Use sampling techniques (Sample minority class, Downsample majority class, can take the hybrid approach as well). We can also use data augmentation to add more data points for the minority class but with little deviations/changes leading to new data points that are similar to the ones they are derived from. The most common/popular technique is SMOTE (Synthetic Minority Oversampling technique)* Suppression: Though not recommended, we can drop off some features directly responsible for the imbalance.* Learning Fair Representation: Projecting the training examples to a subspace or plane minimizes the data imbalance.* Re-Weighting: We can assign some weights to each training example to reduce the imbalance in the data.**2. In-Processing:*** Regularisation: We can add score terms that measure the data imbalance in the loss function and therefore minimizing the loss function will also minimize the degree of imbalance concerning the score chosen which also indirectly minimizes other metrics that measure the degree of data imbalance.* Adversarial Debiasing: Here we use the adversarial notion to train the model where the discriminator tries to detect if there are signs of data imbalance in the predicted data by the generator and hence the generator learns to generate data that is less prone to imbalance.**3. Post-Processing:*** Odds-Equalization: Here we try to equalize the odds for the classes with respect to the data is imbalanced for correct imbalance in the trained model. Usually, the F1 score is a good choice, if both precision and recall scores are important* Choose appropriate performance metrics. For example, accuracy is not a correct metric to use when classes are imbalanced. Instead, use precision, recall, F1 score, and ROC curve."
Technical,ML,"You are working on a clustering problem, what are different evaluation metrics that can be used, and how to choose between them?","Clusters are evaluated based on some similarity or dissimilarity measure such as the distance between cluster points. If the clustering algorithm separates dissimilar observations and similar observations together, then it has performed well. The two most popular metrics evaluation metrics for clustering algorithms are the and . The Silhouette Coefficient is defined for each sample and is composed of two scores:a: The mean distance between a sample and all other points in the same cluster.b: The mean distance between a sample and all other points in the next nearest cluster.S = (b-a) / max(a,b)The for a set of samples is given as the mean of the Silhouette Coefficient for each sample. The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters. The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster.Dunns IndexDunns Index (DI) is another metric for evaluating a clustering algorithm. Dunns Index is equal to the minimum inter-cluster distance divided by the maximum cluster size. Note that large inter-cluster distances (better separation) and smaller cluster sizes (more compact clusters) lead to a higher DI value. A higher DI implies better clustering. It assumes that better clustering means that clusters are compact and well-separated from other clusters."
Technical,ML,What is the ROC curve and when should you use it?,"ROC curve, Receiver Operating Characteristic curve, is a graphical representation of the model's performance where we plot the True Positive Rate (TPR) against the False Positive Rate (FPR) for different threshold values, for hard classification, between 0 to 1 based on model output.This ROC curve is mainly used to compare two or more models as shown in the figure below. Now, it is easy to see that a reasonable model will always give FPR less (since it's an error) than TPR so, the curve hugs the upper left corner of the square box 0 to 1 on the TPR axis and 0 to 1 on the FPR axis.The more the AUC(area under the curve) for a model's ROC curve, the better the model in terms of prediction accuracy in terms of TPR and FPR.Here are some benefits of using the ROC Curve :* Can help prioritize either true positives or true negatives depending on your case study (Helps you visually choose the best hyperparameters for your case)* Can be very insightful when we have unbalanced datasets* Can be used to compare different ML models by calculating the area under the ROC curve (AUC)"
Technical,ML,What is the difference between hard and soft voting classifiers in the context of ensemble learners?,* Hard Voting: We take into account the class predictions for each classifier and then classify an input based on the maximum votes to a particular class.* Soft Voting: We take into account the probability predictions for each class by each classifier and then classify an input to the class with maximum probability based on the average probability (averaged over the classifier's probabilities) for that class.
Technical,ML,What is boosting in the context of ensemble learners discuss two famous boosting methods,"Boosting refers to any Ensemble method that can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor.There are many boosting methods available, but by far the most popular are: * Adaptive Boosting: One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor under-fitted. This results in new predictors focusing more and more on the hard cases.* Gradient Boosting: Another very popular Boosting algorithm is Gradient Boosting. Just like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration as AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictor."
Technical,ML,How can you evaluate the performance of a dimensionality reduction algorithm on your dataset?,"Intuitively, a dimensionality reduction algorithm performs well if it eliminates a lot of dimensions from the dataset without losing too much information. One way to measure this is to apply the reverse transformation and measure the reconstruction error. However, not all dimensionality reduction algorithms provide a reverse transformation.Alternatively, if you are using dimensionality reduction as a preprocessing step before another Machine Learning algorithm (e.g., a Random Forest classifier), then you can simply measure the performance of that second algorithm; if dimensionality reduction did not lose too much information, then the algorithm should perform just as well as when using the original dataset."
Technical,ML,Define the curse of dimensionality and how to solve it.,"Curse of dimensionality represents the situation when the amount of data is too few to be represented in a high-dimensional space, as it will be highly scattered in that high-dimensional space and it becomes more probable that we overfit this data. If we increase the number of features, we are implicitly increasing model complexity and if we increase model complexity we need more data. Possible solutions are:Remove irrelevant features not discriminating classes correlated or features not resulting in much improvement, we can use:* Feature selection(select the most important ones).* Feature extraction(transform current feature dimensionality into a lower dimension preserving the most possible amount of information like PCA )."
Technical,ML,"In what cases would you use vanilla PCA, Incremental PCA, Randomized PCA, or Kernel PCA?","Regular PCA is the default, but it works only if the dataset fits in memory. Incremental PCA is useful for large datasets that don't fit in memory, but it is slower than regular PCA, so if the dataset fits in memory you should prefer regular PCA. Incremental PCA is also useful for online tasks when you need to apply PCA on the fly, every time a new instance arrives. Randomized PCA is useful when you want to considerably reduce dimensionality and the dataset fits in memory; in this case, it is much faster than regular PCA. Finally, Kernel PCA is useful for nonlinear datasets."
Technical,ML,Discuss two clustering algorithms that can scale to large datasets,"**Minibatch Kmeans:** Instead of using the full dataset at each iteration, the algorithmis capable of using mini-batches, moving the centroids just slightly at each iteration.This speeds up the algorithm typically by a factor of 3 or 4 and makes itpossible to cluster huge datasets that do not fit in memory. Scikit-Learn implementsthis algorithm in the MiniBatchKMeans class.**Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH)**is a clustering algorithm that can cluster large datasets by first generating a small and compact summary of the large dataset that retains as much information as possible. This smaller summary is then clustered instead of clustering the larger dataset."
Technical,ML,Do you need to scale your data if you will be using the SVM classifier and discus your answer,"Yes, feature scaling is required for SVM and all margin-based classifiers since the optimal hyperplane (the decision boundary) is dependent on the scale of the input features. In other words, the distance between two observations will differ for scaled and non-scaled cases, leading to different models being generated. This can be seen in the figure below, when the features have different scales, we can see that the decision boundary and the support vectors are only classifying the X1 features without taking into consideration the X0 feature, however after scaling the data to the same scale the decision boundaries and support vectors are looking much better and the model is taking into account both features.To scale the data, normalization, and standardization are the most popular approaches."
Technical,ML,What are Loss Functions and Cost Functions? Explain the key Difference Between them.,"The loss function is the measure of the performance of the model on a single training example, whereas the cost function is the average loss function over all training examples or across the batch in the case of mini-batch gradient descent.Some examples of loss functions are Mean Squared Error, Binary Cross Entropy, etc.Whereas, the cost function is the average of the above loss functions over training examples."
Technical,ML,What is the importance of batch in machine learning and explain some batch-dependent gradient descent algorithms?,"In the memory, the dataset can load either completely at once or in the form of a set. If we have a huge size of the dataset, then loading the whole data into memory will reduce the training speed, hence batch term is introduced.Example: image data contains 1,00,000 images, we can load this into 3125 batches where 1 batch = 32 images. So instead of loading the whole 1,00,000 images in memory, we can load 32 images 3125 times which requires less memory.In summary, a batch is important in two ways: (1) Efficient memory consumption. (2) Improve training speed.There are 3 types of gradient descent algorithms based on batch size: (1) Stochastic gradient descent (2) Batch gradient descent (3) Mini Batch gradient descentIf the whole data is in a single batch, it is called batch gradient descent. If the single data points are equal to one batch i.e. number of batches = number of data instances, it is called stochastic gradient descent. If the number of batches is less than the number of data points or greater than 1, it is known as mini-batch gradient descent."
Technical,ML,What are the different methods to split a tree in a decision tree algorithm?,"Decision trees can be of two types regression and classification.For classification, classification accuracy created a lot of instability. So the following loss functions are used:- Gini's IndexGini impurity is used to predict the likelihood of a randomly chosen example being incorrectly classified by a particular node. Its referred to as an impurity measure because it demonstrates how the model departs from a simple division.- Cross-entropy or Information GainInformation gain refers to the process of identifying the most important features/attributes that convey the most information about a class. The entropy principle is followed with the goal of reducing entropy from the root node to the leaf nodes. Information gain is the difference in entropy before and after splitting, which describes the impurity of in-class items.For regression, the good old mean squared error serves as a good loss function which is minimized by splits of the input features and predicting the mean value of the target feature on the subspaces resulting from the split. But finding the split that results in the minimum possible residual sum of squares is computationally infeasible, so a greedy top-down approach is taken i.e. the splits are made at a level from top to down which results in maximum reduction of RSS. We continue this until some maximum depth or number of leaves is attained."
Technical,ML,Why boosting is a more stable algorithm as compared to other ensemble algorithms?,Boosting algorithms focus on errors found in previous iterations until they become obsolete. Whereas in bagging there is no corrective loop. Thats why boosting is a more stable algorithm compared to other ensemble algorithms.
Technical,ML,What is active learning and discuss one strategy of it?,"Active learning is a special case of machine learning in which a learning algorithm can interactively query a user (or some other information source) to label new data points with the desired outputs. In statistics literature, it is sometimes referred to as optimal experimental design.1. Stream-based sampling In stream-based selective sampling, unlabelled data is continuously fed to an active learning system, where the learner decides whether to send the same to a human oracle or not based on a predefined learning strategy. This method is apt in scenarios where the model is in production and the data sources/distributions vary over time. 2. Pool-based samplingIn this case, the data samples are chosen from a pool of unlabelled data based on the informative value scores and sent for manual labeling. Unlike stream-based sampling, oftentimes, the entire unlabelled dataset is scrutinized for the selection of the best instances."
Technical,ML,What are the different approaches to implementing recommendation systems?,"1. Content-based filtering: Content-Based Filtering depends on similarities of items and users' past activities on the website to recommend any product or service.This filter helps in avoiding a cold start for any new products as it doesn't rely on other users' feedback, it can recommend products based on similarity factors. However, content-based filtering needs a lot of domain knowledge so that the recommendations made are 100 percent accurate.2. collaborative filtering: The primary job of a collaborative filtering system is to overcome the shortcomings of content-based filtering.So, instead of focusing on just one user, the collaborative filtering system focuses on all the users and clusters them according to their interests.Basically, it recommends a product 'x' to user 'a' based on the interest of user 'b'; users 'a' and 'b' must have had similar interests in the past, which is why they are clustered together.The domain knowledge that is required for collaborative filtering is less, recommendations made are more accurate and it can adapt to the changing tastes of users over time. However, collaborative filtering faces the problem of a cold start as it heavily relies on feedback or activity from other users.3. Hybrid filtering: A mixture of content and collaborative methods. Uses descriptors and interactions.More modern approaches typically fall into the hybrid filtering category and tend to work in two stages:1). A candidate generation phase where we coarsely generate candidates from a corpus of hundreds of thousands, millions, or billions of items down to a few hundred or thousand2) A ranking phase where we re-rank the candidates into a final top-n set to be shown to the user. Some systems employ multiple candidate generation methods and rankers."
Technical,ML,What are the evaluation metrics that can be used for multi-label classification?,"Multi-label classification is a type of classification problem where each instance can be assigned to multiple classes or labels simultaneously.The evaluation metrics for multi-label classification are designed to measure the performance of a multi-label classifier in predicting the correct set of labels for each instance.Some commonly used evaluation metrics for multi-label classification are:1. Hamming Loss: Hamming Loss is the fraction of labels that are incorrectly predicted. It is defined as the average number of labels that are predicted incorrectly per instance.2. Accuracy: Accuracy is the fraction of instances that are correctly predicted. In multi-label classification, accuracy is calculated as the percentage of instances for which all labels are predicted correctly.3. Precision, Recall, F1-Score: These metrics can be applied to each label separately, treating the classification of each label as a separate binary classification problem. Precision measures the proportion of predicted positive labels that are correct, recall measures the proportion of actual positive labels that are correctly predicted, and F1-score is the harmonic mean of precision and recall.4. Macro-F1, Micro-F1: Macro-F1 and Micro-F1 are two types of F1-score metrics that take into account the label imbalance in the dataset. Macro-F1 calculates the F1-score for each label and then averages them, while Micro-F1 calculates the overall F1-score by aggregating the true positive, false positive, and false negative counts across all labels.There are other metrics that can be used such as:* Precision at k (P@k)* Average precision at k (AP@k)* Mean average precision at k (MAP@k)"
Technical,ML,What is the difference between concept and data drift and how to overcome each of them?,"Concept drift and data drift are two different types of problems that can occur in machine learning systems.Concept drift refers to changes in the underlying relationships between the input data and the target variable over time. This means that the distribution of the data that the model was trained on no longer matches the distribution of the data it is being tested on. For example, a spam filter model that was trained on emails from several years ago may not be as effective at identifying spam emails from today because the language and tactics used in spam emails may have changed.Data drift, on the other hand, refers to changes in the input data itself over time. This means that the values of the input feature that the model was trained on no longer match the values of the input features in the data it is being tested on. For example, a model that was trained on data from a particular geographical region may not be as effective at predicting outcomes for data from a different region.To overcome concept drift, one approach is to use online learning methods that allow the model to adapt to new data as it arrives. This involves continually training the model on the most recent data while using historical data to maintain context. Another approach is to periodically retrain the model using a representative sample of the most recent data.To overcome data drift, one approach is to monitor the input data for changes and retrain the model when significant changes are detected. This may involve setting up a monitoring system that alerts the user when the data distribution changes beyond a certain threshold.Another approach is to preprocess the input data to remove or mitigate the effects of the features changing over time so that the model can continue learning from the remaining features."
Technical,ML,Can you explain the ARIMA model and its components?,"The ARIMA model, which stands for Autoregressive Integrated Moving Average, is a widely used time series forecasting model. It combines three key components: Autoregression (AR), Differencing (I), and Moving Average (MA).* Autoregression (AR):The autoregressive component captures the relationship between an observation in a time series and a certain number of lagged observations. It assumes that the value at a given time depends linearly on its own previous values. The ""p"" parameter in ARIMA(p, d, q) represents the order of autoregressive terms. For example, ARIMA(1, 0, 0) refers to a model with one autoregressive term.* Differencing (I):Differencing is used to make a time series stationary by removing trends or seasonality. It calculates the difference between consecutive observations to eliminate any non-stationary behavior. The ""d"" parameter in ARIMA(p, d, q) represents the order of differencing. For instance, ARIMA(0, 1, 0) indicates that differencing is applied once.* Moving Average (MA):The moving average component takes into account the dependency between an observation and a residual error from a moving average model applied to lagged observations. It assumes that the value at a given time depends linearly on the error terms from previous time steps. The ""q"" parameter in ARIMA(p, d, q) represents the order of the moving average terms. For example, ARIMA(0, 0, 1) signifies a model with one moving average term.By combining these three components, the ARIMA model can capture both autoregressive patterns, temporal dependencies, and stationary behavior in a time series. The parameters p, d, and q are typically determined through techniques like the Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC).It's worth noting that there are variations of the ARIMA model, such as SARIMA (Seasonal ARIMA), which incorporates additional seasonal components for modeling seasonal patterns in the data.ARIMA models are widely used in forecasting applications, but they do make certain assumptions about the underlying data, such as linearity and stationarity. It's important to validate these assumptions and adjust the model accordingly if they are not met."
Technical,ML,What are the assumptions made by the ARIMA model?,"The ARIMA model makes several assumptions about the underlying time series data. These assumptions are important to ensure the validity and accuracy of the model's results. Here are the key assumptions:Stationarity: The ARIMA model assumes that the time series is stationary. Stationarity means that the statistical properties of the data, such as the mean and variance, remain constant over time. This assumption is crucial for the autoregressive and moving average components to hold. If the time series is non-stationary, differencing (the ""I"" component) is applied to transform it into a stationary series.Linearity: The ARIMA model assumes that the relationship between the observations and the lagged values is linear. It assumes that the future values of the time series can be modeled as a linear combination of past values and error terms.No Autocorrelation in Residuals: The ARIMA model assumes that the residuals (the differences between the predicted values and the actual values) do not exhibit any autocorrelation. In other words, the errors are not correlated with each other.Normally Distributed Residuals: The ARIMA model assumes that the residuals follow a normal distribution with a mean of zero. This assumption is necessary for statistical inference, parameter estimation, and hypothesis testing.It's important to note that while these assumptions are commonly made in ARIMA modeling, they may not always hold in real-world scenarios. It's essential to assess the data and, if needed, apply transformations or consider alternative models that relax some of these assumptions. Additionally, diagnostics tools, such as residual analysis and statistical tests, can help evaluate the adequacy of the assumptions and the model's fit to the data."
Technical,Python,Write a Python function to reverse a string.,"To reverse a string in Python, you can use slicing. Heres how you can do it:def reverse_string(s):return s[::-1]The slicing notation s[::-1] starts from the end of the string and moves to the beginning, effectively reversing it. Its a concise and efficient way to achieve this."
Technical,Python,Explain the difference between a list and a tuple in Python,"The main difference between a list and a tuple in Python is mutability. A list is mutable, meaning you can change its content after its created. You can add, remove, or modify elements. Heres an example:my_list = [1, 2, 3]my_list.append(4) # Now my_list is [1, 2, 3, 4]On the other hand, a tuple is immutable. Once its created, you cant change its content. Tuples are defined using parentheses. Heres an example:my_tuple = (1, 2, 3)# my_tuple.append(4) would raise an error because tuples dont support item assignmentChoosing between a list and a tuple depends on whether you need to modify the data. Tuples can also be slightly faster and are often used when the data should not change."
Technical,Python,Write a Python function to check if a given number is prime.,"To check if a number is prime, you need to test if its only divisible by 1 and itself. Heres a simple function to do that:def is_prime(n):if n <= 1:return Falsefor i in range(2, int(n**0.5) + 1):if n % i == 0:return Falsereturn TrueThis function first checks if the number is less than or equal to 1, which are not prime numbers. Then it checks divisibility from 2 up to the square root of the number. If any number divides evenly, its not prime."
Technical,Python,Write a Python function to calculate the factorial of a number.,"Calculating the factorial of a number can be done using either a loop or recursion. Heres an example using a loop:def factorial(n):if n < 0:return ""Invalid input""result = 1for i in range(1, n + 1):result *= Ireturn resultThis function initializes the result to 1 and multiplies it by each integer up to n. Its straightforward and avoids the risk of stack overflow with large numbers that recursion might encounter."
Technical,Python,What is a generator in Python? Provide an example.,"Generators are a special type of iterator in Python that allows you to iterate through a sequence of values lazily, meaning they generate values on the fly and save memory. You create a generator using a function and the yield keyword. Heres a simple example:def my_generator():for i in range(1, 4):yield Igen = my_generator()print(next(gen)) # 1print(next(gen)) # 2print(next(gen)) # 3Using yield instead of return allows the function to produce a series of values over time, pausing and resuming as needed. This is very useful for handling large datasets or streams of data."
Technical,Python,Explain the difference between map and filter functions in Python.,"Both map and filter are built-in functions in Python used for functional programming, but they serve different purposes. The map function applies a given function to all items in an input list (or any iterable) and returns a new list of results. For example:def square(x):return x * xnumbers = [1, 2, 3, 4]squared = map(square, numbers)print(list(squared)) # [1, 4, 9, 16]On the other hand, the filter function applies a given function to all items in an input list and returns only the items for which the function returns True. Heres an example:def is_even(x):return x % 2 == 0numbers = [1, 2, 3, 4]evens = filter(is_even, numbers)print(list(evens)) # [2, 4]So, map transforms each item, while filter selects items based on a condition. Both are very powerful tools for processing data efficiently."
Technical,Data Structures & Algorithms,Implement a binary search algorithm in Python.,"Binary search is an efficient algorithm for finding an item from a sorted list of items. It works by repeatedly dividing the search interval in half. If the value of the search key is less than the item in the middle of the interval, narrow the interval to the lower half. Otherwise, narrow it to the upper half. Heres how you can implement it in Python:def binary_search(arr, target):left, right = 0, len(arr) - 1while left <= right:mid = (left + right) // 2if arr[mid] == target:return midelif arr[mid] < target:left = mid + 1else:right = mid - 1return -1 # Target not foundIn this function, we initialize two pointers, left and right, to the start and end of the list, respectively. We then repeatedly check the middle element and adjust the pointers based on the comparison with the target value."
Technical,Data Structures & Algorithms,Explain how a hash table works. Provide an example.,"A hash table is a data structure that stores key-value pairs. It uses a hash function to compute an index into an array of buckets or slots, from which the desired value can be found. The main advantage of hash tables is their efficient data retrieval, as they allow for average-case constant-time complexity, O(1), for lookups, insertions, and deletions.Heres a simple example in Python using a dictionary, which is essentially a hash table:# Creating a hash table (dictionary)hash_table = {}# Adding key-value pairshash_table[""name""] = ""Alice""hash_table[""age""] = 25hash_table[""city""] = ""New York""# Retrieving valuesprint(hash_table[""name""]) # Output: Aliceprint(hash_table[""age""]) # Output: 25print(hash_table[""city""]) # Output: New YorkIn this example, the hash function is implicitly handled by Pythons dictionary implementation. Keys are hashed to produce a unique index where the corresponding value is stored."
Technical,Data Structures & Algorithms,Implement a bubble sort algorithm in Python.,"Bubble sort is a simple sorting algorithm that repeatedly steps through the list, compares adjacent elements, and swaps them if they are in the wrong order. The pass through the list is repeated until the list is sorted. Heres a Python implementation:def bubble_sort(arr):n = len(arr)for i in range(n):for j in range(0, n-i-1):if arr[j] > arr[j+1]:arr[j], arr[j+1] = arr[j+1], arr[j]# Example usagearr = [64, 34, 25, 12, 22, 11, 90]bubble_sort(arr)print(""Sorted array:"", arr)In this function, we have two nested loops. The inner loop performs the comparisons and swaps, and the outer loop ensures that the process is repeated until the entire list is sorted."
Technical,Data Structures & Algorithms,Implement a linked list in Python,"A linked list is a data structure in which elements are stored in nodes, and each node points to the next node in the sequence. Heres how you can implement a simple singly linked list in Python:class Node:def __init__(self, data):self.data = dataself.next = Noneclass LinkedList:def __init__(self):self.head = Nonedef append(self, data):new_node = Node(data)if not self.head:self.head = new_nodereturnlast_node = self.headwhile last_node.next:last_node = last_node.nextlast_node.next = new_nodedef print_list(self):current = self.headwhile current:print(current.data, end="" -> "")current = current.nextprint(""None"")# Example usagell = LinkedList()ll.append(1)ll.append(2)ll.append(3)ll.print_list() # Output: 1 -> 2 -> 3 -> NoneIn this implementation, we have a Node class to represent each element in the list and a LinkedList class to manage the nodes. The append method adds a new node to the end of the list, and the print_list method prints all elements."
Technical,Data Structures & Algorithms,Write a function to find the nth Fibonacci number using recursion.,"The Fibonacci sequence is a series of numbers where each number is the sum of the two preceding ones, usually starting with 0 and 1. Heres a recursive function to find the nth Fibonacci number:def fibonacci(n):if n <= 0:return ""Invalid input""elif n == 1:return 0elif n == 2:return 1else:return fibonacci(n-1) + fibonacci(n-2)# Example usageprint(fibonacci(10)) # Output: 34This function uses recursion to compute the Fibonacci number. The base cases handle the first two Fibonacci numbers (0 and 1), and the recursive case sums the previous two Fibonacci numbers."
Technical,Python,"Given a dataset of retail transactions, write a Pandas script to perform the following tasks: Load the dataset from a CSV file named retail_data.csv. Display the first 5 rows of the dataset. Clean the data by removing any rows with missing values. Create a new column named TotalPrice that is the product of Quantity and UnitPrice. Group the data by Country and calculate the total TotalPrice for each country. Sort the resulting grouped data by TotalPrice in descending order and display the top 10 countries. Assume the dataset has the following columns: InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, Country","Heres how you can do it:import pandas as pd# Step 1: Load the dataset from a CSV file named 'retail_data.csv'df = pd.read_csv('retail_data.csv')# Step 2: Display the first 5 rows of the datasetprint(""First 5 rows of the dataset:"")print(df.head())# Step 3: Clean the data by removing any rows with missing valuesdf_cleaned = df.dropna()# Step 4: Create a new column named 'TotalPrice' that is the product of 'Quantity' and 'UnitPrice'df_cleaned['TotalPrice'] = df_cleaned['Quantity'] * df_cleaned['UnitPrice']# Step 5: Group the data by 'Country' and calculate the total 'TotalPrice' for each countrycountry_totals = df_cleaned.groupby('Country')['TotalPrice'].sum().reset_index()# Step 6: Sort the resulting grouped data by 'TotalPrice' in descending order and display the top 10 countriestop_countries = country_totals.sort_values(by='TotalPrice', ascending=False).head(10)print(""Top 10 countries by total sales:"")print(top_countries)"
Technical,Data Analysis,How do you read a CSV file into a DataFrame in Pandas?,"Reading a CSV file into a DataFrame is straightforward with Pandas. You use the read_csv function. Heres how you can do it:import pandas as pd# Reading a CSV file into a DataFramedf = pd.read_csv('path_to_file.csv')# Displaying the first few rows of the DataFrameprint(df.head())This function reads the CSV file from the specified path and loads it into a DataFrame, which is a powerful data structure for data manipulation and analysis."
Technical,Data Analysis,How do you select specific rows and columns in a DataFrame?,"Selecting specific rows and columns in a DataFrame can be done using various methods. Here are a few examples:1. Selecting columns:# Select a single columncolumn = df['column_name']# Select multiple columnscolumns = df[['column1', 'column2']]2. Selecting rows:# Select rows by indexrows = df[0:5] # First 5 rows3. Selecting rows and columns:# Select specific rows and columnssubset = df.loc[0:5, ['column1', 'column2']] # Using labelssubset_iloc = df.iloc[0:5, [0, 1]] # Using integer positions"
Technical,Python,What is the difference between loc and iloc in Pandas?,"The main difference between loc and iloc lies in how you select data from a DataFrame:loc: Uses labels or boolean arrays to select data. It is label-based.# Select rows and columns by labeldf.loc[0:5, ['column1', 'column2']]iloc: Uses integer positions to select data. It is position-based.# Select rows and columns by integer positiondf.iloc[0:5, [0, 1]]Essentially, loc is used when you know the labels of your data, and iloc is used when you know the index positions."
Technical,Data Analysis,How do you handle missing values in a DataFrame?,"Handling missing values is crucial for data analysis. Pandas provides several methods to deal with missing data.Detecting missing values:# Detect missing valuesmissing_values = df.isnull()Dropping missing values:# Drop rows with missing valuesdf_cleaned = df.dropna()# Drop columns with missing valuesdf_cleaned = df.dropna(axis=1)Filling missing values:# Fill missing values with a specific valuedf_filled = df.fillna(0)# Fill missing values with the mean of the columndf_filled = df.fillna(df.mean())These methods allow you to clean your data, making it ready for analysis."
Technical,Data Analysis,How do you merge two DataFrames in Pandas?,"To merge two DataFrames, you can use the merge function, which is similar to SQL joins. Heres an example:# Creating two DataFramesdf1 = pd.DataFrame({'key': ['A', 'B', 'C'], 'value1': [1, 2, 3]})df2 = pd.DataFrame({'key': ['A', 'B', 'D'], 'value2': [4, 5, 6]})# Merging DataFrames on the 'key' columnmerged_df = pd.merge(df1, df2, on='key', how='inner')# Displaying the merged DataFrameprint(merged_df)In this example, how=inner specifies an inner join. You can also use left, right, or outer for different types of joins."
Technical,Data Analysis,What is groupby in Pandas? Provide an example.,"The groupby function in Pandas is used to split the data into groups based on some criteria, apply a function to each group, and then combine the results. Heres a simple example:# Creating a DataFramedata = {'Category': ['A', 'B', 'A', 'B'], 'Values': [10, 20, 30, 40]}df = pd.DataFrame(data)# Grouping by 'Category' and calculating the sum of 'Values'grouped = df.groupby('Category').sum()# Displaying the grouped DataFrameprint(grouped)In this example, the DataFrame is grouped by the Category column, and the sum of the Values column is calculated for each group. Grouping data is very powerful for aggregation and summary statistics."
Technical,Data Analysis,"Given a 2D array, write a NumPy script to perform the following tasks: Create a 55 matrix with values ranging from 1 to 25. Reshape the matrix to 125 and then back to 55. Compute the sum of all elements in the matrix. Calculate the mean of each row. Replace all values greater than 10 with 10. Transpose of the matrix.","Heres how you can do it:import numpy as np# Step 1: Create a 5x5 matrix with values ranging from 1 to 25matrix = np.arange(1, 26).reshape(5, 5)print(""Original 5x5 matrix:"")print(matrix)# Step 2: Reshape the matrix to 1x25 and then back to 5x5matrix_reshaped = matrix.reshape(1, 25)print(""Reshaped to 1x25:"")print(matrix_reshaped)matrix_back_to_5x5 = matrix_reshaped.reshape(5, 5)print(""Reshaped back to 5x5:"")print(matrix_back_to_5x5)# Step 3: Compute the sum of all elements in the matrixsum_of_elements = np.sum(matrix)print(""Sum of all elements:"")print(sum_of_elements)# Step 4: Calculate the mean of each rowmean_of_rows = np.mean(matrix, axis=1)print(""Mean of each row:"")print(mean_of_rows)# Step 5: Replace all values greater than 10 with 10matrix_clipped = np.clip(matrix, None, 10)print(""Matrix with values greater than 10 replaced with 10:"")print(matrix_clipped)# Step 6: Transpose the matrixmatrix_transposed = np.transpose(matrix)print(""Transposed matrix:"")print(matrix_transposed)"
Technical,SQL,"Write a SQL query that finds all customers who placed an order with a total amount greater than $100 in the last month (from todays date). Assume the database has the following tables: customers: Contains customer information like customer_id, name, email orders: Contains order details like order_id, customer_id, order_date, total_amount","Heres how you write the query for it:SELECT customers.name, orders.order_date, orders.total_amountFROM customersINNER JOIN orders ON customers.customer_id = orders.customer_idWHERE orders.order_date >= DATE_SUB(CURDATE(), INTERVAL 1 MONTH) AND orders.total_amount > 100;"
Technical,SQL,Write an SQL query to select all records from a table.,"To select all records from a table, you use the SELECT statement with the asterisk (*) wildcard, which means all columns. Heres the syntax:SELECT * FROM table_name;For example, if you have a table named employees, the query would be:SELECT * FROM employees;This query retrieves all columns and rows from the employees table."
Technical,SQL,Explain the difference between GROUP BY and HAVING clauses in SQL.,"Both GROUP BY and HAVING are used in SQL to organize and filter data, but they serve different purposes:GROUP BY: This clause is used to group rows that have the same values in specified columns into aggregated data. It is often used with aggregate functions like COUNT, SUM, AVG, etc.SELECT department, COUNT(*)FROM employeesGROUP BY department;HAVING: This clause is used to filter groups created by the GROUP BY clause. It acts like a WHERE clause, but is used after the aggregation.SELECT department, COUNT(*)FROM employeesGROUP BY departmentHAVING COUNT(*) > 10;"
Technical,SQL,Write an SQL query to find the second-highest salary from an Employee table.,"To find the second-highest salary, you can use the LIMIT clause along with a subquery. Heres one way to do it:SELECT MAX(salary)FROM employeesWHERE salary < (SELECT MAX(salary) FROM employees);This query first finds the highest salary and then uses it to find the maximum salary that is less than this highest salary, effectively giving you the second-highest salary."
Technical,SQL,Write an SQL query to count the number of employees in each department.,"To count the number of employees in each department, you can use the GROUP BY clause along with the COUNT function. Heres how:SELECT department, COUNT(*) AS employee_countFROM employeesGROUP BY department;This query groups the employees by their department and counts the number of employees in each group."
Technical,SQL,What is a subquery in SQL? Provide an example.,"A subquery, or inner query, is a query nested within another query. It can be used in various places like the SELECT, INSERT, UPDATE, and DELETE statements, or inside other subqueries. Heres an example:SELECT name, salaryFROM employeesWHERE salary > (SELECT AVG(salary) FROM employees);In this example, the subquery (SELECT AVG(salary) FROM employees) calculates the average salary of all employees. The outer query then selects the names and salaries of employees who earn more than this average salary."
Technical,ML,What is overfitting? How do you prevent it?,"Overfitting occurs when a machine learning model learns not only the underlying patterns in the training data but also the noise and outliers. This results in excellent performance on the training data but poor generalization to new, unseen data. Here are a few strategies to prevent overfitting:Cross-Validation: Use techniques like k-fold cross-validation to ensure the model performs well on different subsets of the data.Regularization: Add a penalty for larger coefficients (L1 or L2 regularization) to simplify the model.from sklearn.linear_model import Ridgemodel = Ridge(alpha=1.0)Pruning (for decision trees): Trim the branches of a tree that have little importance.Early Stopping: Stop training when the model performance on a validation set starts to degrade.Dropout (for neural networks): Randomly drop neurons during training to prevent co-adaptation.from tensorflow.keras.layers import Dropoutmodel.add(Dropout(0.5))More Data: Increasing the size of the training dataset can help the model generalize better.Preventing overfitting is crucial for building robust models that perform well on new data."
Technical,Data Analysis,Write a Python script to perform Principal Component Analysis (PCA) on a dataset and plot the first two principal components.,"Heres how you can do it:import pandas as pdfrom sklearn.decomposition import PCAimport matplotlib.pyplot as plt# Example DataFramedf = pd.DataFrame({'feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],'feature2': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11],'feature3': [3, 4, 5, 6, 7, 8, 9, 10, 11, 12] })X = df[['feature1', 'feature2', 'feature3']]# Step 1: Apply PCApca = PCA(n_components=2)principal_components = pca.fit_transform(X)principal_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])# Step 2: Plot the first two principal componentsplt.scatter(principal_df['PC1'], principal_df['PC2'])plt.xlabel('Principal Component 1')plt.ylabel('Principal Component 2')plt.title('PCA of Dataset')plt.show()"
Technical,ML,How do you evaluate a machine learning model?,"Evaluating a machine learning model involves several metrics and techniques to ensure its performance. Here are some common methods:Train-Test Split: Divide the dataset into a training set and a test set to evaluate how well the model generalizes to unseen data.from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)Cross-Validation: Use k-fold cross-validation to assess the models performance on different subsets of the data.from sklearn.model_selection import cross_val_scorescores = cross_val_score(model, X, y, cv=5)Confusion Matrix: For classification problems, a confusion matrix helps visualize the performance by showing true vs. predicted values.from sklearn.metrics import confusion_matrixy_pred = model.predict(X_test)cm = confusion_matrix(y_test, y_pred)ROC-AUC Curve: For binary classification, the ROC-AUC curve helps evaluate the models ability to distinguish between classes.from sklearn.metrics import roc_auc_scoreauc = roc_auc_score(y_test, y_pred)Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE): For regression problems, these metrics help quantify the prediction errors.from sklearn.metrics import mean_absolute_error, mean_squared_errormae = mean_absolute_error(y_test, y_pred)rmse = mean_squared_error(y_test, y_pred, squared=False)Evaluating a model comprehensively ensures that it performs well not just on training data but also on new, unseen data, making it robust and reliable."
Technical,Statistics & Probability,How to perform univariate analysis for numerical and categorical variables?,"Univariate analysis is a statistical technique used to analyze and describe the characteristics of a single variable. It is a useful tool for understanding the distribution, central tendency, and dispersion of a variable, as well as identifying patterns and relationships within the data. Here are the steps for performing univariate analysis for numerical and categorical variables:For numerical variables:Calculate descriptive statistics such as the mean, median, mode, and standard deviation to summarize the distribution of the data.Visualize the distribution of the data using plots such as histograms, boxplots, or density plots.Check for outliers and anomalies in the data.Check for normality in the data using statistical tests or visualizations such as a Q-Q plot.For categorical variables.Calculate the frequency or count of each category in the data.Calculate the percentage or proportion of each category in the data.Visualize the distribution of the data using plots such as bar plots or pie charts.Check for imbalances or abnormalities in the distribution of the data."
Technical,Data Analysis,What are the different ways in which we can find outliers in the data?,"What are the different ways in which we can find outliers in the data?A. Outliers are data points that are significantly different from the majority of the data. They can be caused by errors, anomalies, or unusual circumstances, and they can have a significant impact on statistical analyses and machine learning models. Therefore, it is important to identify and handle outliers appropriately in order to obtain accurate and reliable results.Here are some common ways to find outliers in the data:Visual inspection: Outliers can often be identified by visually inspecting the data using plots such as histograms, scatterplots, or boxplots.Summary statistics: Outliers can sometimes be identified by calculating summary statistics such as the mean, median, or interquartile range, and comparing them to the data. For example, if the mean is significantly different from the median, it could indicate the presence of outliers."
Technical,Data Analysis,What are the different ways by which you can impute the missing values in the dataset?,"There are several ways that you can impute null values (i.e., missing values) in a dataset:Drop rows: One option is to simply drop rows with null values from the dataset. This is a simple and fast method, but it can be problematic if a large number of rows are dropped, as it can significantly reduce the sample size and impact the statistical power of the analysis.Drop columns: Another option is to drop columns with null values from the dataset. This can be a good option if the number of null values is large compared to the number of non-null values, or if the column is not relevant to the analysis.Imputation with mean or median: One common method of imputation is to replace null values with the mean or median of the non-null values in the column. This can be a good option if the data are missing at random and the mean or median is a reasonable representation of the data.Imputation with mode: Another option is to replace null values with the mode (i.e., the most common value) of the non-null values in the column. This can be a good option for categorical data where the mode is a meaningful representation of the data.Imputation with a predictive model: Another method of imputation is to use a predictive model to estimate the missing values based on the other available data. This can be a more complex and time-consuming method, but it can be more accurate if the data are not missing at random and there is a strong relationship between the missing values and the other data."
Technical,Data Analysis,What are the key elements of an EDA report and how do they contribute to understanding a dataset?,"The key elements of an EDA report include univariate analysis, bivariate analysis, missing data analysis, and basic data visualization. Univariate analysis helps in understanding the distribution of individual variables, bivariate analysis helps in understanding the relationship between variables, missing data analysis helps in understanding the quality of data, and data visualization provides a visual interpretation of the data."
Technical,Statistics & Probability,Why is hypothesis testing useful for a data scientist?,"Hypothesis testing is a statistical technique used in data science to evaluate the validity of a claim or hypothesis about a population. It is used to determine whether there is sufficient evidence to support a claim or hypothesis and to assess the statistical significance of the results.There are many situations in data science where hypothesis testing is useful. For example, it can be used to test the effectiveness of a new marketing campaign, to determine if there is a significant difference between the means of two groups, to evaluate the relationship between two variables, or to assess the accuracy of a predictive model.Hypothesis testing is an important tool in data science because it allows data scientists to make informed decisions based on data, rather than relying on assumptions or subjective opinions. It helps data scientists to draw conclusions about the data that are supported by statistical evidence, and to communicate their findings in a clear and reliable manner. Hypothesis testing is therefore a key component of the scientific method and a fundamental aspect of data science practice."
Technical,ML,How will you identify underfitting in a model?,"Underfitting occurs when a statistical model or machine learning algorithm is not able to capture the underlying trend of the data. This can happen for a variety of reasons, but one common cause is that the model is too simple and is not able to capture the complexity of the dataHere is how to identify underfitting in a model:The training error of an underfitting error will be high, i.e., the model will not be able to learn from the training data and will perform poorly on the training data.The validation error of an underfitting model will also be high as it will perform poorly on the new data as well."
Technical,ML,How do you decide whether a model is suffering from high bias or high variance?,"There are several techniques that can be used to balance the bias and variance in a model, including:Increasing the model complexity by adding more parameters or features: This can help the model capture more complex patterns in the data and reduce bias, but it can also increase variance if the model becomes too complex.Reducing the model complexity by removing parameters or features: This can help the model avoid overfitting and reduce variance, but it can also increase bias if the model becomes too simple.Using regularization techniques: These techniques constrain the model complexity by penalizing large weights, which can help the model avoid overfitting and reduce variance. Some examples of regularization techniques are L1 regularization, L2 regularization, and elastic net regularization.Splitting the data into a training set and a test set: This allows us to evaluate the models generalization ability and tune the model complexity to achieve a good balance between bias and variance.Using cross-validation: This is a technique for evaluating the models performance on different splits of the data and averaging the results to get a more accurate estimate of the models generalization ability."
Technical,ML,"How do you choose the appropriate evaluation metric for a classification problem, and how do you interpret the results of the evaluation?","There are many evaluation metrics that you can use for a classification problem, and the appropriate metric depends on the specific characteristics of the problem and the goals of the evaluation. Some common evaluation metrics for classification include:Accuracy: This is the most common evaluation metric for classification. It measures the percentage of correct predictions made by the model.Precision: This metric measures the proportion of true positive predictions among all positive predictions made by the model.Recall: This metric measures the proportion of true positive predictions among all actual positive cases in the test set.F1 Score: This is the harmonic mean of precision and recall. It is a good metric to use when you want to balance precision and recall.AUC-ROC: This metric measures the ability of the model to distinguish between positive and negative classes. It is commonly used for imbalanced classification problems.To interpret the results of the evaluation, you should consider the specific characteristics of the problem and the goals of the evaluation. For example, if you are trying to identify fraudulent transactions, you may be more interested in maximizing precision, because you want to minimize the number of false alarms. On the other hand, if you are trying to diagnose a disease, you may be more interested in maximizing recall, because you want to minimize the number of missed diagnoses."
Technical,ML,How can you handle imbalanced classes in a logistic regression model?,"There are several ways to handle imbalanced classes in a logistic regression model. Some approaches include:Undersampling the majority class: This involves randomly selecting a subset of the majority class samples to use in training the model. This can help to balance the class distribution, but it may also throw away valuable information.Oversampling the minority class: This involves generating synthetic samples of the minority class to add to the training set. One popular method for generating synthetic samples is called SMOTE (Synthetic Minority Oversampling Technique).Adjusting the class weights: Many machine learning algorithms allow you to adjust the weighting of each class. In logistic regression, you can do this by setting the class_weight parameter to balanced. This will automatically weight the classes inversely proportional to their frequency, so that the model pays more attention to the minority class.Using a different evaluation metric: In imbalanced classification tasks, it is often more informative to use evaluation metrics that are sensitive to class imbalance, such as precision, recall, and the F1 score.Using a different algorithm: Some algorithms, such as decision trees and Random Forests, are more robust to imbalanced classes and may perform better on imbalanced datasets."
Technical,ML,When not to use PCA for dimensionality reduction?,"There are several situations when you may not want to use Principal Component Analysis (PCA) for dimensionality reduction:When the data is not linearly separable: PCA is a linear technique, so it may not be effective at reducing the dimensionality of data that is not linearly separable.The data has categorical features: PCA is designed to work with continuous numerical data and may not be effective at reducing the dimensionality of data with categorical features.When the data has a large number of missing values: PCA is sensitive to missing values and may not work well with data sets that have a large number of missing values.The goal is to preserve the relationships between the original features: PCA is a technique that looks for patterns in the data and creates new features that are combinations of the original features. As a result, it may not be the best choice if the goal is to preserve the relationships between the original features.When the data is highly imbalanced: PCA is sensitive to class imbalances and may not produce good results on highly imbalanced data sets."
Technical,Data Analysis,How do you decide the size of your validation and test sets?,"You can validate the size of your test sets in the following ways:Size of the dataset: In general, the larger the dataset, the larger the validation and test sets can be. This is because there is more data to work with, so the validation and test sets can be more representative of the overall dataset.Complexity of the model: If the model is very simple, it may not require as much data to validate and test. On the other hand, if the model is very complex, it may require more data to ensure that it is robust and generalizes well to unseen data.Level of uncertainty: If the model is expected to perform very well on the task, the validation and test sets can be smaller. However, if the performance of the model is uncertain or the task is very challenging, it may be helpful to have larger validation and test sets to get a more accurate assessment of the models performance.Resources available: The size of the validation and test sets may also be limited by the computational resources available. It may not be practical to use very large validation and test sets if it takes a long time to train and evaluate the model."
Technical,ML,How do you evaluate a models performance for a multi-class classification problem?,"One approach for evaluating a multi-class classification model is to calculate a separate evaluation metric for each class, and then calculate a macro or micro average. The macro average gives equal weight to all the classes, while the micro average gives more weight to the classes with more observations. Additionally, some commonly used metrics for multi-class classification problems such as confusion matrix, precision, recall, F1 score, Accuracy and ROC-AUC can also be used."
Technical,Data Analysis,How is normalized data beneficial for making models in data science?,"Improved model performance: Normalizing the data can improve the performance of some machine learning models, particularly those that are sensitive to the scale of the input data. For example, normalizing the data can improve the performance of algorithms such as K-nearest neighbors and neural networks.Easier feature comparison: Normalizing the data can make it easier to compare the importance of different features. Without normalization, features with large scales can dominate the model, making it difficult to determine the relative importance of other features.Reduced impact of outliers: Normalizing the data can reduce the impact of outliers on the model, as they are scaled down along with the rest of the data. This can improve the robustness of the model and prevent it from being influenced by extreme values.Improved interpretability: Normalizing the data can make it easier to interpret the results of the model, as the coefficients and feature importances are all on the same scale.It is important to note that normalization is not always necessary or beneficial for all models. It is necessary to carefully evaluate the specific characteristics and needs of the data and the model in order to determine whether normalization is appropriate."
Technical,ML,What are some ways to select features?,"Here are some ways to select the features:Filter methods: These methods use statistical scores to select the most relevant features. Example:Correlation coefficient: Selects features that are highly correlated with the target variable.Chi-squared test: Selects features that are independent of the target variable.Wrapper methods: These methods use a learning algorithm to select the best features. For exampleForward selection: Begins with an empty set of features and adds one feature at a time until the performance of the model is optimal.Backward selection: Begins with the full set of features and removes one feature at a time until the performance of the model is optimal.Embedded methods: These methods learn which features are most important while the model is being trained.Example:Lasso regression: Regularizes the model by adding a penalty term to the loss function that shrinks the coefficients of the less important features to zero.Ridge regression: Regularizes the model by adding a penalty term to the loss function that shrinks the coefficients of all features towards zero, but does not set them to zero.Feature Importance: We can also use the feature importance parameter which gives us the most important features considered by the model"
Technical,ML,How can we use cross-validation to overcome overfitting?,"The cross-validation technique can be used to identify if the model is underfitting or overfitting but it cannot be used to overcome either of the problems. We can only compare the performance of the model on two different sets of data and find if the data is overfitting or underfitting, or generalized."
Technical,ML,How does the step size (or learning rate) of an optimization algorithm impact the convergence of the optimization process in logistic regression?,"The step size, or learning rate, determines the size of the steps taken by the optimization algorithm when moving towards the minimum of the objective function. In logistic regression, the objective function is the negative log-likelihood of the model, which we want to minimize in order to find the optimal coefficients.If the step size is too large, the optimization algorithm may overshoot the minimum and oscillate around it, possibly even diverging instead of converging. On the other hand, if the step size is too small, the optimization algorithm will make very slow progress and may take a long time to converge.Therefore, it is important to choose an appropriate step size in order to ensure the convergence of the optimization process. In general, a larger step size can lead to faster convergence, but it also increases the risk of overshooting the minimum. A smaller step size will be safer, but it will also be slower.There are several approaches for choosing an appropriate step size. One common approach is to use a fixed step size for all iterations. Another approach is to use a decreasing step size, which starts out large and decreases over time. This can help the optimization algorithm to make faster progress at the beginning and then fine-tune the coefficients as it gets closer to the minimum."
Technical,ML,What will happen if we increase the number of neighbors in KNN?,"If you increase the number of neighbors to a very large value in KNN, the classifier will become more and more conservative, and the decision boundary will become smoother and smoother. This can help to reduce overfitting, but it can also cause the classifier to be less sensitive to subtle patterns in the training data. A larger value of k will lead to a less complex model, which is less prone to overfitting but more prone to underfitting..Therefore, in order to avoid overfitting or underfitting, it is important to choose an appropriate value of k that strikes a balance between complexity and simplicity. It is usually better to try a range of values for the number of neighbors and see which one works best for a particular dataset."
Technical,ML,What will happen in the decision tree if the max depth is increased?,"Increasing the max depth of adecision treewill increase the complexity of the model and make it more prone to overfitting. If you increase the max depth of a decision tree, the tree will be able to make more complex and nuanced decisions, which can improve the models ability to fit the training data well. However, if the tree is too deep, it may become overly sensitive to the specific patterns in the training data and not generalize well to unseen data."
Technical,ML,What is the difference between extra trees and random forests?,"The main difference between the two algorithms is how the decision trees are constructed.In a Random Forest, the decision trees are constructed using bootstrapped samples of the training data and a random subset of the features. This results in each tree being trained on a slightly different set of data and features, leading to a greater diversity of trees and a lower variance.In an Extra Trees classifier, the decision trees are constructed in a similar way, but instead of selecting a random subset of the features at each split, the algorithm selects the best split among a random subset of the features. This results in a greater number of random splits and a higher degree of randomness, leading to a lower bias and a higher variance."
Technical,ML,What is the problem with using label encoding for nominal data?,"Label encoding is a method of encoding categorical variables as numerical values, which can be beneficial in certain situations. However, there are some potential problems that you should be aware of when using label encoding for nominal data.One problem with label encoding is that it can create an ordinal relationship between categories where none existsIf you have a categorical variable with three categories: red, green, and blue, and you apply label encoding to map these categories to numerical values 0, 1, and 2, the model may assume that the category green is somehow between the categories red and blue. This can be a problem if your model depends on the assumption that the categories are independent of one another.Another problem with label encoding is that it can lead to unexpected results if you have an imbalanced dataset. For example, if one category is much more common than the others, it will be assigned a much lower numerical value, which could lead the model to give it less importance than it deserves."
Technical,ML,When can one-hot encoding be a problem?,"One-hot encoding can be a problem in certain situations because it can create a large number of new columns in the dataset, which can make the data more difficult to work with and potentially lead to overfitting.One-hot encoding creates a new binary column for each category in a categorical variable. If you have a categorical variable with many categories, this can result in a very large number of new columns.Another problem with one-hot encoding is that it can lead to overfitting, especially if you have a small dataset and a large number of categories. When you create many new columns for each category, you are effectively increasing the number of features in the dataset. This can lead to overfitting, because the model may be able to memorize the training data, but it will not generalize well to new data.Finally, one-hot encoding can also be a problem if you need to add new categories to the dataset in the future. If you have already one-hot encoded the existing categories, you will need to be careful to ensure that the new categories are added in a way that does not create confusion or lead to unexpected results."
Technical,ML,What can be an appropriate encoding technique when you have hundreds of categorical values in a column?,A few techniques can be used when we have hundreds of columns in a categorical variable.Frequency encoding: This involves replacing each category with the frequency of that category in the dataset. This can work well if the categories have a natural ordinal relationship based on their frequency.Target encoding: This involves replacing each category with the mean of the target variable for that category. This can be effective if the categories have a clear relationship with the target variable.
Technical,ML,How do you decide which feature to split on at each node of the tree?,"When training a decision tree, the algorithm must choose the feature to split on at each node of the tree. There are several strategies that can be used to decide which feature to split on, including:Greedy search: The algorithm selects the feature that maximizes a splitting criterion (such as information gain or Gini impurity) at each step.Random Search: The algorithm selects the feature to split on at random at each step.Exhaustive search: The algorithm considers all possible splits and selects the one that maximizes the splitting criterion.Forward search: The algorithm starts with an empty tree and adds splits one by one, selecting the split that maximizes the splitting criterion at each step.Backward search: The algorithm starts with a fully grown tree and prunes split one by one, selecting the split to remove that results in the smallest decrease in the splitting criterion."
Technical,ML,How do you choose the number of models to use in a Boosting or Bagging ensemble?,"The number of models to use in an ensemble is usually determined by the trade-off between performance and computational cost. As a general rule of thumb, increasing the number of models will improve the performance of the ensemble, but at the cost of increasing the computational cost.In practice, the number of models is determined by Cross validation which is used to determine the optimal number of models based on the evaluation metric chosen."
Technical,ML,In which scenarios Boosting and Bagging are preferred over single models?,Both boosting and bagging are generally preferred in scenarios where the individual models have high variance or high bias and the goal is to improve the overall performance of the model. Bagging is generally used to reduce the variance of a model while boosting is used to reduce bias and improve the generalization error of the model. Both methods are also useful when working with models that are sensitive to the training data and have a high chance of overfitting.
Technical,ML,Can you explain the ROC curve and AUC score and how they are used to evaluate a models performance?,"A ROC (Receiver Operating Characteristic) curve is a graphical representation of the performance of a binary classification model. It plots the true positive rate (TPR) against the false positive rate (FPR) at different thresholds. AUC (Area Under the Curve) is the area under the ROC curve. It gives a single number that represents the models overall performance. AUC is useful because it considers all possible thresholds, not just a single point on the ROC curve."
Technical,ML,How do you approach setting the threshold in a binary classification problem when you want to adjust precision and recall by yourself?,"When setting the threshold in a binary classification problem, its important to consider the trade-off between precision and recall. Precision is the proportion of true positive predictions out of all positive predictions, while recall is the proportion of true positive predictions out of all actual positive cases.One approach to adjusting precision and recall is to first train a model and then evaluate its performance on a validation set. The validation set should have a similar distribution of positive and negative cases as the test set on the model will be deployed.Next, you can use a confusion matrix to visualize the models performance and identify the current threshold that is being used to make predictions. A confusion matrix shows the number of true positive, false positive, true negative, and false negative predictions the model is making.From there, you can adjust the threshold to change the balance between precision and recall. For example, increasing the threshold will increase precision, but decrease recall. On the other hand, decreasing the threshold will increase recall and decrease precision.It is also important to consider the specific use case and the cost of false negatives and false positives. In certain applications, such as medical diagnosis, it may be more important to have a high recall (i.e., not to miss any actual positive cases) even if that means accepting a lower precision. In other cases, such as fraud detection, it may be more important to have high precision (i.e., not to flag any legitimate transactions as fraudulent) even if that means accepting a lower recall."
Technical,ML,Can you explain the concept of the kernel trick and its application in Support Vector Machines (SVMs)?,"The kernel trick is a technique used to transform the input data in SVMs to a higher-dimensional feature space, where it becomes linearly separable. The kernel trick works by replacing the standard inner product in the input space with a kernel function, which computes the inner product in a higher-dimensional space without actually having to compute the coordinates of the data in that space. This allows SVMs to handle non-linearly separable data by mapping it to a higher-dimensional space where it becomes linearly separable. Common kernel functions used in SVMs include the polynomial kernel, radial basis function (RBF) kernel, and sigmoid kernel"
Technical,ML,Explain logistic Regression and it's assumptions,"Logistic Regression is a go-to method for classification. Logistic regression models the probability of the default class (e.g. the first class). It employs the use of the sigmoid function that can take any real-valued number and map it into a probability value between 0 and 1 to predict the output class. There are two types of logistic regression: Binary and Multinomial. Binary Logistic Regression deals with two categories whereas multinomial deals with three or more categories.Assumptions:Binary logistic regression requires the dependent variable to be binary. The independent variables should be independent of each other. That is, the model should have little or no multicollinearity, The independent variables should be linearly related to the log odds."
Technical,ML,Explain Linear Regression and its assumptions,"Linear regression is useful for finding the relationship between two continuous variables. One is the predictor or independent variable and the other is the response or dependent variable.AssumptionsThere are 5 basic assumptions of linear regression:Linear relationship: Between the dependent and independent variable,Multivariate normality: Multiple regression assumes that the residuals are normally distributed.No or little multicollinearity between the independent variablesNo autocorrelation: It's a characteristic of data in which the correlation between the values of the same variables is based on related objects. It violates the assumption of instance independence, which underlies most of the conventional models.Homoscedasticity: This assumption means that the variance around the regression line is the same for all values of the predictor variable."
Technical,ML,How do you split your data between training and validation?,"Training and validation set from data can be split on the following 2 principles. First, ensure the validation set is large enough to yield statistically meaningful results. Second, the validation set should be representative of the data set as a whole. In other words, don't pick a validation set with different characteristics than the training set. An optimal way would be to use k-folds validation. This method makes multiple splits of the dataset into train and validation sets. This offer various samples of data and ultimately reduces the chances of overfitting."
Technical,ML,Describe Binary Classification.,"Binary classification is the process of predicting the class of a given set of data points. These classes are also known as targets/ labels. This method of predictive modeling approximates a mapping function (f) from input variables (X) to discrete output variables (y). For example, spam detection in email service providers can be identified as a classification problem. This is binary classification since there are only 2 classes:spamandnot spam."
Technical,ML,Explain the working of decision trees,"A decision tree classification algorithm uses a training dataset to stratify or segment the predictor space into multiple regions. Each such region has only a subset of the training dataset. To predict the outcome for a given (test) observation, first, we determine which of these regions it belongs to. Once its region is identified, its outcome class is predicted as being the same as the mode (most common) of the outcome classes of all the training observations that are included in that region. The rules used to stratify the predictor space can be graphically described in a tree-like flow-chart, hence the name of the algorithm. The only difference being that these decision trees are drawn upside down.Decision tree classification models can easily handle qualitative predictors without the need to create dummy variables. Missing values are not a problem either. Interestingly, decision tree algorithms are used for regression models as well. The same library that you would use to build a classification model can also be used to build a regression model after changing some of the parameters. However, one major problem with decision trees is their high variance."
Technical,ML,What are different metrics to classify a dataset?,"The performance metrics for classification problems are as follows:Confusion MatrixAccuracyPrecision and RecallF1 ScoreAUC-ROC Curve.The choice of selecting a performance metric depends on the type of question and the dataset. For instance, if the dataset is balanced then accuracy would be a good measure to evaluate the model performance.Confusion matrix would be a good alternative if you want to know the cost of False Positives and False Negatives."
Technical,ML,What's the role of a cost function?,Cost function is used to learn the parameters in the machine learning model such that the total error is as minimal as possible. A cost function is a measure of how wrong the model is in terms of its ability to estimate the relationship between the dependent and independent variable. This function is typically expressed as a difference between the predicted value and the actual value. Every algorithm can have it's own cost function depending on the problem.
Technical,ML,Why is dimensionality reduction important,"Datasets with large number of feature sets (specifically images, sound, and/or textual contents) increase space, add overfitting and slow down the time to train the models.Dimensionality reduction is the process of reducing the dimensionality of the feature space with consideration by obtaining a set of principal features. This way, it can assist in better performance of the learning algorithm resulting in less computational cost with simplification of models.It also eliminates redundant features and features with strong correlation between them, therefore, reducing overfitting. Moreover, projection into two or three dimensions is often used to facilitate the visualization of high dimensional data sets, leading to better human interpretations."
Technical,ML,"What are hyperparameters, how to tune them, how to test and know if they worked for the particular problem?","While a machine learning model tries to learn parameters from the training data, hyperparameters are those values that are set before the training process begins. They are properties used to describe how a model is supposed to function, for example: the number of trees in a decision tree or the learning rate of a regression model. Hyperparameters directly control the behavior of the training algorithm and have a significant impact on the performance of the model being trained.To choose the optimal set of hyperparameters for providing the best results, they can be tuned by running multiple trials. By setting different values for those hyperparameters, training different models, and deciding which ones work best by testing them, we can figure out those values. Each trial is a complete execution of our training application with a broad set of values we specify. Some common techniques are: Grid search, Random Search, Bayesian Optimization.To test and find out if the specified hyperparameters work for our model, we need to test them against a specific evaluation metric based on the nature of our problem. For example, we can choose a set of hyperparameters that give us the best accuracy or the best F1 score."
Technical,ML,"How will you decide whether a customer will buy a product today or not given the income of the customer, location where the customer lives, profession, and gender? Define a machine learning algorithm for this.","This is a classification algorithm. Therefore, to solve this problem, the dataset will be collected and stored. It will be cleaned and pre-processed for any abnormalities or missing values. After this it will be subject to feature engineering. Some of these steps may include dealing with missing values, encoding categorical values, and normalizing numerical values if required.The dataset would then be divided into train and test sets, using K-folds validation with k set to an appropriate number of folds. The train set would be used to fit a classification model such as a Logistic Regression, Decision Tree Classifier or Support Vector Classifier with appropriate parameters. The fitted model will then be evaluated against the test set to check how good the model is using an appropriate evaluation metric such as accuracy or F1 score."
Technical,Python,"Write a Python function to calculate the mean, median, and mode of a dataset.","def calc_mean_median(data): import statistics; return statistics.mean(data), statistics.median(data)"
Technical,SQL,Write a query to find customers who made more than 5 purchases in the last month.,"SELECT customer_id FROM transactions WHERE purchase_date >= DATEADD(MONTH, -1, GETDATE()) GROUP BY customer_id HAVING COUNT(transaction_id) > 5;"
Technical,Statistics & Probability,Explain p-values and their significance in hypothesis testing.,"A p-value represents the probability of obtaining results at least as extreme as the observed results, assuming the null hypothesis is true. A small p-value (e.g., < 0.05) indicates strong evidence against the null hypothesis, suggesting that the alternative hypothesis might be true. However, it does not measure the magnitude of an effect or the importance of a result"
Technical,Case Study,Imagine you're given a dataset with customer reviews and star ratings. How would you build a model to predict review sentiments?,"First, I would preprocess the text data by removing stop words, stemming, and vectorizing it using TF-IDF or word embeddings. Next, I would split the data into training and testing sets and build a classification model like logistic regression or a more advanced model like BERT. Finally, I would evaluate the model using metrics such as accuracy, precision, recall, and F1-score, iterating as needed to optimize performance."
Behavioral,Soft Skills,Tell me about a time you had to collaborate with a non-technical team to solve a problem.,"In a previous project, I worked with the marketing team to analyze customer retention rates. They were unfamiliar with technical jargon, so I explained the insights using simple visualizations like bar charts and pie charts. By aligning on the key business metrics they cared about, we created a successful retention strategy that improved customer loyalty by 15%."
Behavioral,Soft Skills,Describe a time when you faced a significant data quality issue and how you addressed it.,"While analyzing sales data, I noticed missing values and inconsistent formats across regions. I collaborated with the data engineering team to establish standardized data pipelines. Additionally, I implemented imputation techniques like mean substitution for missing numerical values and added validation checks. This ensured cleaner data for analysis and increased model accuracy by 10%"
Behavioral,Soft Skills,Give an example of how you resolved a conflict while working in a team.,"In one project, there was a disagreement about the feature selection for a predictive model. I suggested we test both approaches using cross-validation and present the results to the team. The results showed one approach had a 5% higher accuracy, and the entire team agreed to proceed with the better-performing solution. This data-driven approach minimized conflict and kept the project on track."
Behavioral,Soft Skills,Tell me about a time when you identified an opportunity to improve a process.,"While working on weekly sales reports, I noticed that manual data processing was time-consuming. I automated the process using Python scripts and scheduling tools, reducing the time spent from 8 hours to 1 hour per week. This allowed the team to focus on more strategic tasks and saved approximately 40 hours per month."
Behavioral,Soft Skills,How do you handle situations where the project requirements change mid-way?,"In a project to predict customer churn, halfway through, the client requested additional features based on new data. I quickly reprioritized the work, integrated the new features using exploratory data analysis, and retrained the model. Although it required extra effort, we delivered the updated model on time, meeting the clients needs"
Behavioral,Soft Skills,Share an experience where you led a team to achieve a challenging goal.,"I led a team to develop a recommendation system for a retail client. The timeline was tight, and the requirements were complex. I broke the project into manageable tasks, assigned roles based on team strengths, and conducted daily stand-ups. Despite the challenges, we delivered the system ahead of schedule, improving product recommendations by 20%."""
Behavioral,Soft Skills,Describe a situation where you had to make a difficult decision with limited data.,"In a supply chain optimization project, we lacked detailed data for certain regions. I decided to use historical trends as a proxy and built sensitivity analyses to assess the potential risks. This allowed the team to proceed with a robust strategy while monitoring for updates. The decision minimized delays and kept the project on schedule"
Behavioral,Soft Skills,Tell me about a time you had to come up with an innovative solution to a problem.,"In a fraud detection project, standard methods like thresholding werent effective. I proposed using unsupervised learning with clustering algorithms to identify anomalies. This novel approach flagged suspicious transactions with 90% precision, exceeding the clients expectations."
Behavioral,Soft Skills,Describe a time you had to deliver results under tight deadlines.,"During a quarterly business review, I was asked to provide insights from sales data within 24 hours. I quickly prioritized tasks, automated data cleaning with Python, and focused on key metrics. I delivered a concise report with actionable insights, which was well-received by stakeholders."
Behavioral,Soft Skills,Tell me about a project that didnt go as planned and what you learned from it.,"In a sentiment analysis project, we initially overlooked the importance of preprocessing emojis and slang, which affected accuracy. After identifying the issue, I updated the preprocessing pipeline and retrained the model, improving accuracy by 15%. This taught me to thoroughly analyze data before modeling."
Behavioral,Soft Skills,How do you handle feedback or criticism from clients?,"In a project for building a dashboard, the client felt the design didn't align with their vision. I actively listened to their feedback, clarified their requirements, and iterated on the design. By incorporating their suggestions, the final product exceeded their expectations and strengthened our relationship."""
Technical,SQL,Write a SQL query to find the second highest salary in a table,SELECT MAX(salary) FROM employees WHERE salary < (SELECT MAX(salary) FROM employees)
Technical,Data Analysis,You are working on a dataset with a significant amount of missing values. How would you handle this situation?,"To handle missing data, I would first assess the extent and pattern of the missing values. Depending on the nature and distribution of the missing data, I might take several approaches: Deletion:If the percentage of missing values is minimal and appears random, I might remove the affected rows or columns. For example, if less than 5% of the data is missing in a column that isn't crucial, I could drop those rows. Imputation:For more extensive missing data, I could use imputation techniques. This might involve filling in missing values with the mean, median, or mode for numerical data, or using the most frequent category for categorical data. For instance, if the average age is 35 and a small percentage of ages are missing, I might replace the missing values with 35. Predictive Modelling:If the data is more complex, I might use predictive modeling techniques to estimate the missing values. For example, using regression or K-nearest neighbours (KNN) to predict the missing values based on other variables in the dataset."
Technical,Data Analysis,You notice several outliers in your dataset. How would you handle them?,"Outliers can skew the results of the analysis, so it's essential to address them properly: Investigation:First, I would investigate the outliers to understand if they are genuine data points or errors. This involves looking into the context and source of the data. For instance, if sales data shows a sudden spike, I would check if it corresponds to a special event or promotion. Removal:If the outliers are determined to be errors or irrelevant, I might remove them. For example, if an entry for age is 150, its likely an error and should be removed. Transformation:If the outliers are valid but disproportionately affect the analysis, I might transform the data. Log transformation or normalisation can reduce the impact of outliers. Model Robustness:Using robust statistical methods and models less sensitive to outliers, such as median-based measures or decision trees, can also help."
Technical,ML,Your initial predictive models performance is below expectations. What steps would you take to improve it?,"To improve a predictive model, I would follow these steps: Feature Engineering:Enhance the dataset by creating new features or transforming existing ones. For instance, combining date and time data into a single timestamp feature or creating interaction terms. Algorithm Tuning:Optimise the model by tuning hyperparameters using techniques like grid search or random search. For example, adjusting the learning rate and number of trees in a random forest model. Data Cleaning:Re-examine the data for any additional cleaning or preprocessing that might improve model performance, such as dealing with class imbalances using techniques like SMOTE (Synthetic Minority Over-sampling Technique). Model Selection:Experiment with different algorithms. If a linear regression model underperforms, I might try more complex models like gradient boosting machines or neural networks. Cross-Validation:Ensure robust model validation using cross-validation techniques to get a better estimate of model performance and reduce overfitting."
Behavioral,Soft Skills,You have completed an analysis and need to present your findings to a non-technical audience. How would you approach this?,"To effectively communicate findings to a non-technical audience: Simplify the Language:Avoid technical jargon and use simple, clear language. For example, instead of saying ""regression analysis,"" I might say ""we looked at how different factors influence sales."" Visualisations:Use charts, graphs, and other visual aids to illustrate key points. Tools like Power BI or Tableau can help create engaging visualisations. For instance, a bar chart to show sales trends over time or a pie chart to depict market share distribution. Storytelling:Frame the findings in a story format that highlights the problem, the analysis, and the actionable insights. For example, ""Our analysis revealed that customer satisfaction significantly impacts repeat purchases. Improving service response times could increase customer loyalty by 20%."" Focus on Impact:Emphasise the business impact of the findings. Explain how the insights can lead to better decision-making or improved business outcomes. For instance, ""Implementing these changes could lead to a 10% increase in quarterly revenue."""
Behavioral,Soft Skills,You are assigned multiple data analysis projects with overlapping deadlines. How would you manage your time and resources?,"To manage multiple projects effectively: Prioritisation:Determine the priority of each project based on factors such as urgency, business impact, and resource requirements. For example, a project with a tight deadline and high business impact would be prioritised higher. Task Breakdown:Break down each project into smaller tasks and create a timeline or Gantt chart to track progress. This helps in identifying dependencies and managing time effectively. Time Management:Allocate specific blocks of time to each project daily or weekly. Using time management techniques like the Pomodoro Technique can help maintain focus and productivity. Communication:Regularly update stakeholders on progress and any potential delays. Clear communication ensures that expectations are managed, and any issues can be addressed promptly. Resource Allocation:Delegate tasks where possible and utilise available tools and technologies to streamline workflows. For instance, using automation scripts to handle repetitive tasks."
Technical,ML,What are L1 and L2 regularization? What are the differences between the two?,Regularization is a technique used to avoid overfitting by trying to make the model more simple. One way to apply regularization is by adding the weights to the loss function. This is done in order to consider minimizing unimportant weights. In L1 regularization we add the sum of the absolute of the weights to the loss function. In L2 regularization we add the sum of the squares of the weights to the loss function.
Technical,ML,What are the differences between a model that minimizes squared error and one that minimizes the absolute error? and in which cases each error metric would be more appropriate?,"The main difference between them is that in MSE the errors are squared before being averaged while in MAE they are not. This means that a large weight will be given to large errors. MSE is useful when large errors in the model are trying to be avoided. This means that outliers affect MSE more than MAE, which is whyMAE is more robust to outliers. Computation-wise MSE is easier to use as the gradient calculation will be more straightforward than MAE, which requires linear programming to calculate it."
Technical,ML,Define and compare parametric and non-parametric models and give two examples for each of them.,"Parametric models:assume that the dataset comes from a certain function with some set of parameters that should be tuned to reach the optimal performance. Nonparametric models: dont assume anything about the function from which the dataset was sampled. For these models, the number of parameters is not determined prior to training, thus they are free to generalize the model based on the data."
Technical,ML,What are Loss Functions and Cost Functions? Explain the key Difference Between them.,"The loss function is the measure of the performance of the model on a single training example, whereas the cost function is the average loss function over all training examples or across the batch in the case of mini-batch gradient descent. Some examples of loss functions are Mean Squared Error, Binary Cross Entropy, etc. Whereas, the cost function is the average of the above loss functions over training examples."
Technical,ML,What is the importance of batch in machine learning and explain some batch-depend gradient descent algorithms?,"In the memory, the dataset can load either completely at once or in the form of a set. If we have a huge size of the dataset, then loading the whole data into memory will reduce the training speed, hence batch term is introduced. Example: image data contains 1,00,000 images, we can load this into 3125 batches where 1 batch = 32 images. So instead of loading the whole 1,00,000 images in memory, we can load 32 images 3125 times which requires less memory. In summary, a batch is important in two ways:1. Efficient memory consumption.2. Improve training speed."
Technical,ML,What is active learning and discuss one strategy of it?,"Active learning is a special case of machine learning in which a learning algorithm can interactively query a user (or some other information source) to label new data points with the desired outputs. In statistics literature, it is sometimes referred to as optimal experimental design. Stream-based sampling: 1. In stream-based selective sampling, unlabelled data is continuously fed to an active learning system, where the learner decides whether to send the same to a human oracle or not based on a predefined learning strategy. 2. Pool-based sampling: In this case, the data samples are chosen from a pool of unlabelled data based on the informative value scores and sent for manual labeling."
Technical,ML,Explain briefly the logistic regression model and state an example of when you have used it recently.,"Logistic regression is used to calculate the probability of occurrence of an event in the form of a dependent output variable based on independent input variables. Logistic regression is commonly used to estimate the probability that an instance belongs to a particular class. If the probability is bigger than 0.5 then it will belong to that class (positive) and if it is below 0.5 it will belong to the other class. This will make it a binary classifier. It is important to remember that Logistic regression isnt a classification model, its an ordinary type of regression algorithm, and it was developed and used before machine learning, but it can be used in classification when we put a threshold to determine specific categories"
Technical,ML,Explain the linear regression model anddiscuss its assumption.,Linear regression is a supervised statistical model to predict dependent variable quantity based on independent variables. Linear regression is a parametric model and the objective of linear regression is that it has to learn coefficients using the training data and predict the target value given only independent values.
Technical,ML,Explain the kernel trick in SVM why we use it and how to choose what kernel to use.,"Kernels are used in SVM to map the original input data into a particular higher dimensional space where it will be easier to find patterns in the data and train the model with better performance. For eg.: If we have binary class data which form a ring-like pattern (inner and outer rings representing two different class instances) when plotted in 2D space, a linear SVM kernel will not be able to differentiate the two classes well when compared to an RBF (radial basis function) kernel, mapping the data into a particular higher dimensional space where the two classes are clearly separable."
Technical,ML,Do you need to scale your data if you will be using the SVM classifier and discuss your answer,"Yes, feature scaling is required for SVM and all margin-based classifiers since the optimal hyperplane (the decision boundary) is dependent on the scale of the input features. In other words, the distance between two observations will differ for scaled and non-scaled cases, leading to different models being generated. This can be seen in the figure below, when the features have different scales, we can see that the decision boundary and the support vectors are only classifying the X1 features without taking into consideration the X0 feature, however after scaling the data to the same scale the decision boundaries and support vectors are looking much better and the model is taking into account both features.To scale the data, normalization and standardization are the most popular approaches."
Technical,ML,"You are working on a clustering problem, what are different evaluation metrics that can be used, and how to choose between them?","Clusters are evaluated based on some similarity or dissimilarity measure such as the distance between cluster points. If the clustering algorithm separates dissimilar observations and similar observations together, then it has performed well. The two most popular metrics evaluation metrics for clustering algorithms are the and ."
Technical,ML,How can you evaluate the performance of a dimensionality reduction algorithm on your dataset?,"Intuitively, a dimensionality reduction algorithm performs well if it eliminates a lot of dimensions from the dataset without losing too much information. One way to measure this is to apply the reverse transformation and measure the reconstruction error. However, not all dimensionality reduction algorithms provide a reverse transformation. Alternatively, if you are using dimensionality reduction as a preprocessing step before another Machine Learning algorithm (e.g., a Random Forest classifier), then you can simply measure the performance of that second algorithm; if dimensionality reduction did not lose too much information, then the algorithm should perform just as well as when using the original dataset."
Technical,ML,Define the curse of dimensionality and how to solve it.,"Curse of dimensionality represents the situation when the amount of data is too few to be represented in a high-dimensional space, as it will be highly scattered in that high-dimensional space and it becomes more probable that we overfit this data. If we increase the number of features, we are implicitly increasing model complexity and if we increase model complexity we need more data. Possible solutions are: Remove irrelevant features not discriminating classes correlated or features not resulting in much improvement, we can use: Feature selection(select the most important ones).Feature extraction(transform current feature dimensionality into a lower dimension preserving the most possible amount of information like PCA )."
Technical,ML,"In what cases would you use vanilla PCA, Incremental PCA, Randomized PCA, or Kernel PCA?","Regular PCA is the default, but it works only if the dataset fits in memory. Incremental PCA is useful for large datasets that dont fit in memory, but it is slower than regular PCA, so if the dataset fits in memory you should prefer regular PCA. Incremental PCA is also useful for online tasks when you need to apply PCA on the fly, every time a new instance arrives. Randomized PCA is useful when you want to considerably reduce dimensionality and the dataset fits in memory; in this case, it is much faster than regular PCA. Finally, Kernel PCA is useful for nonlinear datasets."
Technical,ML,Discuss two clustering algorithms that can scale to large datasets,"Minibatch Kmeans: Instead of using the full dataset at each iteration, the algorithm is capable of using mini-batches, moving the centroids just slightly at each iteration. This speeds up the algorithm typically by a factor of 3 or 4 and makes it possible to cluster huge datasets that do not fit in memory. Scikit-Learn implements this algorithm in the MiniBatchKMeans class. Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH) is a clustering algorithm that can cluster large datasets by first generating a small and compact summary of the large dataset that retains as much information as possible. This smaller summary is then clustered instead of clustering the larger dataset."
Technical,ML,Explain what is information gain and entropy in the context of decision trees.,"Entropy and Information Gain are two key metrics used in determining the relevance of decision making when constructing a decision tree model and to determine the nodes and the best way to split. The idea of a decision tree is to divide the data set into smaller data sets based on the descriptive features until we reach a small enough set that contains data points that fall under one label. Entropy is the measure of impurity, disorder, or uncertainty in a bunch of examples. Entropy controls how a Decision Tree decides to split the data. Information gain calculates the reduction in entropy or surprise from transforming a dataset in some way. It is commonly used in the construction of decision trees from a training dataset, by evaluating the information gain for each variable and selecting the variable that maximizes the information gain, which in turn minimizes the entropy and best splits the dataset into groups for effective classification."
Technical,ML,What are the different methods to split a tree in a decision tree algorithm?,"Decision trees can be of two types regression and classification. Decision trees can be of two types regression and classification.For classification, classification accuracy created a lot of instability. So the following loss functions are used: Ginis Index: Gini impurity is used to predict the likelihood of a randomly chosen example being incorrectly classified by a particular node. Its referred to as an impurity measure because it demonstrates how the model departs from a simple division. Cross-entropy or Information Gain: Information gain refers to the process of identifying the most important features/attributes that convey the most information about a class. The entropy principle is followed with the goal of reducing entropy from the root node to the leaf nodes. Information gain is the difference in entropy before and after splitting, which describes the impurity of in-class items."
Technical,ML,Describe the motivation behind random forests and mention two reasons why they are better than individual decision trees,"The motivation behind random forest or ensemble models in general in laymans terms, Lets say we have a question/problem to solve we bring 100 people and ask each of them the question/problem and record their solution. Next, we prepare a solution which is a combination/ a mixture of all the solutions provided by these 100 people. We will find that the aggregated solution will be close to the actual solution. This is known as the Wisdom of the crowd and this is the motivation behind Random Forests. We take weak learners (ML models) specifically, Decision Trees in the case of Random Forest & aggregate their results to get good predictions by removing dependency on a particular set of features. In regression, we take the mean and for Classification, we take the majority vote of the classifiers. Reasons why random forests allow for stronger prediction than individual decision trees:Decision trees are prone to overfit whereas random forest generalizes better on unseen data as it uses randomness in feature selection as well as during sampling of the data. Therefore, random forests have lower variance compared to that of the decision tree without substantially increasing the error due to bias. Generally, ensemble models like Random Forest perform better as they are aggregations of various models (Decision Trees in the case of Random Forest), using the concept of the Wisdom of the crowd."
Technical,ML,What is the difference between hard and soft voting classifiers in the context of ensemble learners?,In Hard votingWe take into account the class predictions for each classifier and then classify an input based on the maximum votes to a particular class. Soft Voting: We take into account the probability predictions for each class by each classifier and then classify an input to the class with maximum probability based on the average probability (averaged over the classifiers probabilities) for that class.
Technical,ML,Why boosting is a more stable algorithm as compared to other ensemble algorithms?,Boosting algorithms focus on errors found in previous iterations until they become obsolete. Whereas in bagging there is no corrective loop. Thats why boosting is a more stable algorithm compared to other ensemble algorithms.
Technical,ML,Mention three ways to make your model robust to outliers.,"Investigating the outliers is always the first step in understanding how to treat them. After you understand the nature of why the outliers occurred you can apply one of the several methods mentioned below.Add regularization that will reduce variance, for example, L1 or L2 regularization.Use tree-based models (random forest, gradient boosting ) that are generally less affected by outliers.Winsorize the data. Winsorizing or winsorization is the transformation of statistics by limiting extreme values in the statistical data to reduce the effect of possibly spurious outliers. In numerical data, if the distribution is almost normal using the Z-score we can detect the outliers and treat them by either removing or capping them with some value. If the distribution is skewed using IQR we can detect and treat it by again either removing or capping it with some value. In categorical data check for value_count in the percentage if we have very few records from some category, either we can remove it or cap it with some categorical value like others.Transform the data, for example, you do a log transformation when the response variable follows an exponential distribution or is right-skewed.Use more robust error metrics such as MAE or Huber loss instead of MSE.Remove the outliers, only do this if you are certain that the outliers are true anomalies that are not worth adding to your model. This should be your last consideration since dropping them means losing information."
Technical,Data Analysis,Mention three ways to handle missing or corrupted data in a dataset.,"In general, real-world data often has a lot of missing values. The cause of missing values can be data corruption or failure to record data. The handling of missing data is very important during the preprocessing of the dataset as many machine learning algorithms do not support missing values. However, you should start by asking the data owner/stakeholder about the missing or corrupted data. It might be at the data entry level, because of file encoding, etc. which if aligned, can be handled without the need to use advanced techniques.There are different ways to handle missing data, and we will discuss only three of them:1. Deleting the row with missing valuesThe first method to handle missing values is to delete the rows or columns that have null values. This is an easy and fast method and leads to a robust model, however, it will lead to the loss of a lot of information depending on the amount of missing data and can only be applied if the missing data represent a small percentage of the whole dataset.2. Using learning algorithms that support missing valuesSome machine learning algorithms are robust to missing values in the dataset. The K-NN algorithm can ignore a column from a distance measure when there are missing values. Naive Bayes can also support missing values when making a prediction. Another algorithm that can handle a dataset with missing values or null values is the random forest model and Xgboost (check the post in the first comment), as it can work on non-linear and categorical data. The problem with this method is that these models implementation in the scikit-learn library does not support handling missing values, so you will have to implement it yourself.3. Missing value imputationData imputation means the substitution of estimated values for missing or inconsistent data in your dataset. There are different ways to estimate the values that will replace the missing value. The simplest one is to replace the missing value with the most repeated value in the row or the column. Another simple way is to replace it with the mean, median, or mode of the rest of the row or column.The advantage of this is that it is an easy and fast way to handle the missing data, but it might lead to data leakage and does not factor in the covariance between features. A better way is to use a machine learning model to learn the pattern between the data and predict the missing values, this is a very good method to estimate the missing values that will not lead to data leakage and will factor the covariance between the feature, the drawback of this method is the computational complexity especially if your dataset is large."
Technical,ML,"You are building a binary classifier and found that the data is imbalanced, what should you do to handle this situation?","If there is a data imbalance there are several measures we can take to train a fairer binary classifier:1. Pre-Processing:Check whether you can get more data or not.Use sampling techniques (Up Sample minority class, and Downsample majority class, can take the hybrid approach as well). We can also use data augmentation to add more data points for the minority class but with little deviations/changes leading to new data points that are similar to the ones they are derived from. The most common/popular technique is SMOTE (Synthetic Minority Oversampling technique)Suppression: Though not recommended, we can drop off some features directly responsible for the imbalance.Learning Fair Representation: Projecting the training examples to a subspace or plane minimizes the data imbalance.Re-Weighting: We can assign some weights to each training example to reduce the imbalance in the data.2. In-Processing:Regularisation: We can add score terms that measure the data imbalance in the loss function and therefore minimizing the loss function will also minimize the degree of imbalance with respect to the score chosen which also indirectly minimizes other metrics that measure the degree of data imbalance.Adversarial Debiasing: Here we use the adversarial notion to train the model where the discriminator tries to detect if there are signs of data imbalance in the predicted data by the generator and hence the generator learns to generate data that is less prone to imbalance.3. Post-Processing:Odds-Equalization: Here we try to equalize the odds for the classes wrt the data is imbalanced for correct imbalance in the trained model. Usually, the F1 score is a good choice, if both precision and recall scores are importantChoose appropriate performance metrics: For example, accuracy is not a correct metric to use when classes are imbalanced. Instead, use precision, recall, F1 score, and ROC curve."
Technical,ML,What is the difference between concept and data drift and how to overcome each of them?,"Concept drift and data drift are two different types of problems that can occur in machine learning systems.Concept drift refers to changes in the underlying relationships between the input data and the target variable over time. This means that the distribution of the data that the model was trained on no longer matches the distribution of the data it is being tested on. For example, a spam filter model that was trained on emails from several years ago may not be as effective at identifying spam emails from today because the language and tactics used in spam emails may have changed.Data drift, on the other hand, refers to changes in the input data itself over time. This means that the values of the input feature that the model was trained on no longer match the values of the input features in the data it is being tested on. For example, a model that was trained on data from a particular geographical region may not be as effective at predicting outcomes for data from a different region.To overcome concept drift, one approach is to use online learning methods that allow the model to adapt to new data as it arrives. This involves continually training the model on the most recent data while using historical data to maintain context. Another approach is to periodically retrain the model using a representative sample of the most recent data.To overcome data drift, one approach is to monitor the input data for changes and retrain the model when significant changes are detected. This may involve setting up a monitoring system that alerts the user when the data distribution changes beyond a certain threshold."
Behavioral,Soft Skills,Tell me about a time when you had to quickly learn a new tool or technology to complete a project.,"In one project, I was asked to analyze time-series data, but the team was using R, which I wasnt familiar with. I dedicated a weekend to learning the basics of R, created simple visualizations, and by the next week, I was able to contribute fully to the analysis."
Behavioral,Soft Skills,How do you handle disagreements with team members during a project?,"In a recent project, a teammate and I disagreed on the feature selection method for a predictive model. I proposed we test both approaches and evaluate them using cross-validation metrics. This not only resolved the conflict but also led us to select the best-performing model"
Behavioral,Soft Skills,Describe a time when you led a project under tight deadlines.,"I was leading a project to automate financial reporting. The deadline was three weeks, so I divided the tasks into smaller milestones, assigned roles, and ensured daily progress updates. We completed the project on time, saving 20 hours of manual work per week."
Technical,Data Analysis,How would you visualize categorical data to show trends over time?,"I would use a stacked bar chart or line chart with different colors representing each category. For example, to show monthly sales by region, Id plot months on the x-axis and sales volumes on the y-axis, segmented by region."
Technical,Python,Write a Python function to count the frequency of each word in a text file,"from collections import Counter def word_frequency(file_path): with open(file_path, 'r') as file: text = file.read().lower() words = text.split() return Counter(words)"
Technical,SQL,Write a SQL query to retrieve the top 3 employees with the highest salaries in each department.,"SELECT department_id, employee_id, salary FROM ( SELECT department_id, employee_id, salary, RANK() OVER (PARTITION BY department_id ORDER BY salary DESC) AS rank FROM employees ) ranked WHERE rank <= 3;"
Behavioral,Soft Skills,How do you explain the trade-offs of accuracy versus interpretability in machine learning models?,"When discussing with stakeholders, I explain that highly accurate models like ensembles (e.g., Random Forest) are often harder to interpret but provide better performance. Conversely, simpler models like linear regression are interpretable but may sacrifice some accuracy. I suggest balancing both based on project goals."
Behavioral,Soft Skills,"You're given multiple datasets for analysis, but the deadline is tight. How do you decide which dataset to focus on first?","I prioritize datasets based on their quality, completeness, and relevance to the project's goals. If a dataset requires significant preprocessing but is critical, I'd start there while assigning simpler tasks to teammates to optimize time."
Technical,ML,What criteria do decision trees use to select the best feature for splitting?,"The most common criteria are Information Gain (used in ID3/C4.5), Gini Impurity (used in CART), and Chi-Square. Each criterion measures the reduction in uncertainty or impurity after the split."
Technical,ML,Explain how Gini Impurity or Information Gain influences feature splitting in a decision tree.,"Gini Impurity measures the probability of misclassifying a randomly chosen element, while Information Gain measures the reduction in entropy (disorder). A feature with a higher reduction in impurity or higher information gain is chosen as the splitting criterion."
Technical,ML,How would you tune the decision threshold for a model to favor precision over recall?,"To favor precision, I would increase the decision threshold to ensure the model predicts a positive label only when it is very confident. I would use a precision-recall curve to determine the point where precision is maximized without significant loss in recall."
Technical,ML,Explain the trade-offs between precision and recall when setting the threshold in a classification task.,"Precision focuses on reducing false positives, while recall minimizes false negatives. Adjusting the threshold shifts the balance between these metrics. A high threshold may lower false positives (high precision) but increase false negatives (low recall), and vice versa."
Technical,ML,How does the kernel trick help SVMs handle non-linearly separable data?,The kernel trick enables SVMs to separate non-linear data by transforming it into a higher-dimensional space where a linear hyperplane can effectively classify the data. This is achieved using kernel functions without the computational overhead of explicitly transforming the data.
Technical,ML,Compare linear kernels with RBF kernels in SVMs. When would you use each?,"Linear kernels are used when the data is linearly separable, while RBF kernels are suitable for non-linear datasets as they create complex decision boundaries. RBF kernels are generally preferred for high-dimensional or highly complex data."
Behavioral,Soft Skills,Tell me about a time you had to collaborate with someone whose working style was different from yours.,"In my previous role, I worked with a colleague who preferred a more detail-oriented approach, while I tend to focus on big-picture strategies. We were assigned to create a project roadmap for a client in three days. I suggested a middle ground where we divided the tasks: they handled detailed data analysis, while I focused on outlining the project phases. This collaboration resulted in a well-rounded plan that impressed the client and improved our working relationship."
Behavioral,Soft Skills,Can you describe a situation where your team faced a setback? How did you respond?,Our team faced unexpected data loss during a client project. I needed to keep the team motivated and find a way to recover the lost data quickly. I assigned urgent data recovery tasks to team members and scheduled brainstorming sessions to identify alternative solutions. We managed to retrieve 80% of the data and delivered the project on time.
Behavioral,Soft Skills,Can you describe a situation where your team faced a setback? How did you respond?,Our team faced unexpected data loss during a client project. I needed to keep the team motivated and find a way to recover the lost data quickly. I assigned urgent data recovery tasks to team members and scheduled brainstorming sessions to identify alternative solutions. We managed to retrieve 80% of the data and delivered the project on time.
Behavioral,Soft Skills,Describe a workplace conflict and how you resolved it.,"Two team members had differing opinions on the direction of a project. I had to mediate and ensure the project moved forward without delays. I organized a meeting, heard both perspectives, and proposed a compromise that integrated elements from both approaches. The solution satisfied both members and strengthened team cohesion."
Behavioral,Soft Skills,How do you handle disagreements with colleagues?,"While working on a marketing strategy, a colleague disagreed with my proposed timeline."" It was critical to resolve the disagreement to meet the project deadline."" I explained my reasoning for the timeline and actively listened to their concerns. Together, we adjusted the plan to address both perspectives."" The project was completed ahead of schedule, and the team appreciated the compromise."""
Behavioral,Soft Skills,Tell me about a time you led a team to success,"I was assigned as the leader of a cross-functional team working on a new product launch. I needed to ensure the project was delivered on time while managing a diverse team with different priorities. I organized regular check-ins, set clear deadlines, and encouraged open communication to address issues promptly. The launch was a success, with a 20% increase in sales during the first quarter."
Behavioral,Soft Skills,How do you motivate a team during challenging times?,"Our team faced budget cuts, which demotivated several members. My goal was to maintain morale and productivity despite the constraints. I acknowledged the challenges openly, celebrated small wins, and encouraged creative solutions to work within the budget. The team maintained high performance, and we exceeded our project goals by 15%."
Behavioral,Soft Skills,Can you give an example of a problem you solved at work?,"A key vendor suddenly delayed delivery of critical supplies. I needed to find an alternative supplier to avoid halting production. I researched local vendors, negotiated expedited delivery, and adjusted production schedules to minimize delays. The project was completed on time, and the new vendor became a reliable partner."
Behavioral,Soft Skills,How do you approach solving complex problems?,"I was tasked with optimizing a slow internal process that affected productivity. My goal was to identify bottlenecks and implement a more efficient solution. I conducted a root cause analysis, gathered input from the team, and proposed an automation tool. The new process reduced task completion time by 40%."
Behavioral,Soft Skills,How do you prioritize tasks during tight deadlines?,"I had to manage three overlapping client projects with tight deadlines. I needed to ensure all projects were delivered without compromising quality. I used a task management tool to prioritize critical deliverables, delegated tasks where possible, and set clear expectations with clients. All three projects were delivered on time with positive client feedback."
Behavioral,Soft Skills,Describe a time when you missed a deadline. How did you handle it?,"I underestimated the time required to complete a data analysis report. I had to inform the client about the delay and deliver the report quickly. I communicated transparently with the client, worked overtime, and delivered the report the next day. The client appreciated my honesty and was satisfied with the quality of the report."
Behavioral,Soft Skills,Tell me about a time when you had to support a struggling team member to meet a goal.,A new hire was struggling to adapt to our project management software. I needed to ensure they could use the tool effectively to meet a critical deadline. I set up one-on-one training sessions and provided a quick reference guide for them. They gained confidence with the software and met their deliverables on time.
Behavioral,Soft Skills,Describe a time when you had to adapt your communication style for a team member.,"I worked with a colleague who preferred detailed written instructions over verbal ones. To ensure smooth collaboration, I needed to adapt how I communicated tasks to them. I started sending detailed emails summarizing our verbal discussions and tasks. This approach improved their productivity and reduced misunderstandings."
Behavioral,Soft Skills,How have you handled working with a remote team to ensure collaboration?,"During a project, our team was split across three time zones."" We needed to stay aligned on deliverables despite limited overlap in working hours. I implemented asynchronous updates via Slack, recorded meetings, and maintained a shared task board on Trello. The project was completed ahead of schedule, with consistent communication."
Behavioral,Soft Skills,Describe a time you had to step up and lead without being asked.,"Our team leader unexpectedly went on leave during a high-priority project. I needed to ensure the project stayed on track without formal authority. I organized team meetings, delegated tasks, and maintained regular updates with stakeholders. The project was delivered successfully, and the team appreciated my initiative."
Behavioral,Soft Skills,How have you dealt with a team member who was not pulling their weight?,"During a group project, one member consistently missed deadlines. I had to address the issue to prevent delays. I scheduled a private conversation to understand their challenges and offered support while setting clear expectations. The member improved their performance, and the project was completed on time."
Behavioral,Soft Skills,Can you share an example of how you developed someone's skills at work?,A junior team member was struggling with data visualization. I needed to help them gain confidence and improve their skills in this area. I provided hands-on training sessions and recommended online tutorials. They became proficient in creating visualizations and contributed significantly to our next report.
Behavioral,Soft Skills,Describe a time when you identified a problem no one else had noticed.,"During a data analysis review, I noticed a misalignment in the dataset that could have skewed results. I needed to address the issue before it affected the project. I raised the concern, cross-checked the data, and corrected the errors. This prevented incorrect conclusions and ensured the accuracy of our analysis."
Behavioral,Soft Skills,Tell me about a time you used creativity to solve a problem.,"A client requested a feature that exceeded the project's budget. I had to find a cost-effective way to meet their needs. I proposed using an open-source tool instead of custom development. The client was satisfied, and we delivered within budget."
Behavioral,Soft Skills,How have you handled unexpected challenges during a project?,"Halfway through a project, a key stakeholder changed the requirements. I had to manage the changes without delaying the project. I reassessed the timeline, reprioritized tasks, and updated the team on the changes. The project was completed successfully, meeting the new requirements."
Behavioral,Soft Skills,How do you handle competing priorities?,"I was assigned two high-priority tasks due on the same day. I needed to ensure both tasks were completed without compromising quality. I assessed the effort required for each task, delegated part of one task to a colleague, and focused on the more critical task first. Both tasks were delivered on time and met expectations."
Behavioral,Soft Skills,Describe a time when you had to adjust your schedule to meet an urgent deadline.,A last-minute request came in for a client report that required immediate attention. I had to reorganize my priorities to accommodate this request. I postponed non-urgent tasks and worked late to complete the report. The client received the report on time and appreciated the quick turnaround
Behavioral,Soft Skills,How do you manage stress during busy periods at work?,"Our team faced tight deadlines and multiple deliverables. I needed to maintain productivity while managing stress. I created a detailed schedule, took regular short breaks, and encouraged my team to do the same. We met all deadlines without burnout."
Behavioral,Soft Skills,Tell me about a time you worked on a team project that failed. What did you learn?,Situation: Our team failed to meet a client's deadline due to poor communication. Task: We needed to identify what went wrong and ensure it wouldn't happen again. Action: We implemented weekly check-ins and used project management software to track tasks. Result: Future projects were completed on time with improved communication.
Behavioral,Soft Skills,Describe a time you had to mediate a conflict between team members.,"Situation: Two colleagues disagreed on the approach to a project. Task: I needed to help them resolve their differences to keep the project on track. Action: I organized a meeting, listened to both sides, and proposed a compromise. Result: The conflict was resolved, and the project proceeded smoothly."
Behavioral,Soft Skills,How have you contributed to creating an inclusive work environment?,"Situation: A colleague felt excluded from team discussions. Task: I ensured everyones ideas were heard by introducing a round-robin format for meetings. Action: I facilitated discussions and encouraged quieter team members to participate. Result: The colleague felt more involved, and team morale improved."
Behavioral,Soft Skills,Describe a time you had to lead a project under a tight deadline.,"Situation: A key deliverable was due in one week, and I was assigned as the project lead. Task: To deliver quality work on time. Action: I delegated tasks efficiently, held daily check-ins, and ensured clear communication. Result: The project was delivered on time and received positive client feedback."
Behavioral,Soft Skills,Tell me about a difficult problem you solved in the workplace.,"Situation: A data pipeline issue caused delays in analysis. Task: To identify the root cause and fix it quickly. Action: I collaborated with the IT team, analyzed logs, and implemented a fix. Result: The issue was resolved, and we delivered the analysis within 24 hours."
Behavioral,Soft Skills,How did you handle a situation where you had to learn a new skill quickly?,Situation: A project required me to use a tool I was unfamiliar with. Task: To learn the tool and complete the project on time. Action: I attended a crash course and practiced intensively. Result: I mastered the tool and delivered the project successfully.
Behavioral,Soft Skills,Can you describe a time when you had to explain a technical concept to a non-technical audience?,"Situation: I had to present a machine learning model to executives. Task: To explain the model in simple terms. Action: I used analogies, visualizations, and avoided jargon. Result: The executives understood and approved the model for deployment."
Behavioral,Soft Skills,Tell me about a time you disagreed with a colleague. How did you resolve it?,"Situation: A colleague and I had different opinions on a project strategy. Task: To find common ground without compromising project goals. Action: I initiated a one-on-one discussion and proposed a hybrid approach. Result: We implemented the approach successfully, and the project progressed smoothly."
Behavioral,Soft Skills,How do you prioritize tasks when you have multiple deadlines to meet?,"Situation: I had to handle three competing deadlines at once. Task: To ensure all tasks were completed on time. Action: I assessed urgency and impact, created a schedule, and focused on one task at a time. Result: All tasks were completed ahead of schedule."
Behavioral,Soft Skills,Describe a time you faced a significant setback at work. How did you recover?,"Situation: A major client rejected our proposal. Task: To regroup and create a new proposal. Action: I analyzed feedback, collaborated with my team, and revised the proposal. Result: The client accepted the revised proposal."
Behavioral,Soft Skills,Tell me about a time you worked on a team project that failed. What did you learn?,Situation: Our team failed to meet a client's deadline due to poor communication. Task: We needed to identify what went wrong and ensure it wouldnt happen again. Action: We implemented weekly check-ins and used project management software to track tasks. Result: Future projects were completed on time with improved communication.
Behavioral,Soft Skills,Describe a situation where you motivated a team to achieve a challenging goal.,"Situation: Our sales team was behind on quarterly targets. Task: Motivate the team to surpass the target within one month. Action: I set smaller weekly milestones, provided incentives, and celebrated every win. Result: The team exceeded the target by 15%."
Behavioral,Soft Skills,Give an example of a time you resolved a dispute among team members.,"Situation: Two colleagues disagreed on resource allocation. Task: I needed to mediate and find a resolution. Action: I facilitated an open discussion, ensured everyone felt heard, and proposed a compromise. Result: Both parties were satisfied, and the project moved forward smoothly."
Behavioral,Soft Skills,How do you manage your workload when priorities change quickly?,"Situation: I was juggling multiple projects when a high-priority task emerged. Task: Reorganize my workload to accommodate the new priority. Action: I paused lower-priority tasks, delegated where possible, and focused on the critical task. Result: The urgent task was completed without affecting overall productivity."
Behavioral,Soft Skills,Describe a time you had to adapt to a significant change at work.,"Situation: My company adopted a new project management tool. Task: Learn the tool and train the team within two weeks. Action: I attended training sessions, created guides, and organized workshops. Result: The team quickly adapted, and productivity increased."
Behavioral,Soft Skills,Tell me about a time you identified a problem and took proactive steps to resolve it.,"Situation: A recurring software bug was delaying reports. Task: Identify and fix the root cause. Action: I conducted a code review, identified the issue, and implemented a permanent fix. Result: Reports were delivered on time, and client satisfaction improved."
Behavioral,Soft Skills,Can you provide an example of how you ensured everyones voice was heard in a team setting?,"Situation: Some team members felt hesitant to share ideas. Task: Create an inclusive environment where everyone felt comfortable contributing. Action: I implemented brainstorming sessions with anonymous input. Result: The quality of ideas improved, and team collaboration increased."
Behavioral,Soft Skills,How have you handled a difficult client situation?,"Situation: A client was dissatisfied with project delays. Task: Address their concerns and regain their trust. Action: I communicated transparently, provided regular updates, and expedited deliverables. Result: The clients trust was restored, and the project was completed successfully."
Behavioral,Soft Skills,Describe a time you had to make a difficult decision with limited information.,"Situation: A supplier issue threatened project deadlines. Task: Choose between waiting for resolution or switching suppliers. Action: I analyzed risks, consulted stakeholders, and chose the latter. Result: The project was delivered on time."
Behavioral,Soft Skills,Give an example of a time you provided constructive feedback to a colleague.,"Situation: A colleague's work quality was impacting the team. Task: Provide feedback to help them improve. Action: I scheduled a one-on-one, shared specific examples, and offered support. Result: Their performance improved, and the team's productivity increased."
Behavioral,Soft Skills,How have you used creativity to solve a workplace problem?,Situation: A limited budget was impacting marketing efforts. Task: Generate leads without increasing costs. Action: I proposed using social media and partnerships to promote our brand. Result: Lead generation increased by 30% with minimal expenses.
Behavioral,Soft Skills,Can you describe a time when you faced an ethical dilemma at work?,"Situation: A colleague was falsifying data to meet targets. Task: Decide whether to report the issue. Action: I gathered evidence and reported the matter confidentially. Result: The issue was addressed, and integrity was upheld."
Behavioral,Soft Skills,Describe a time you had to convey bad news to a team or client.,"Situation: A project deadline was delayed due to resource issues. Task: Communicate the delay effectively. Action: I explained the situation honestly, proposed a revised timeline, and provided reassurance. Result: The client appreciated the transparency and accepted the new timeline."
Behavioral,Soft Skills,Tell me about a time you identified a risk and took steps to mitigate it.,"Situation: A new vendor had inconsistent delivery records. Task: Ensure project timelines weren't affected. Action: I sourced a backup vendor and negotiated contracts. Result: Deliveries were on time, avoiding potential delays."
Behavioral,Soft Skills,Describe a time when you had to address a complaint from a coworker.,Situation: A coworker was unhappy about workload distribution. Task: Address the concern fairly. Action: I reviewed assignments and adjusted workloads where needed. Result: Team satisfaction improved.
Technical,ML,What Are Some of the Steps for Data Wrangling and Data Cleaning Before Applying Machine Learning Algorithms?,"There are many steps that can be taken whendata wrangling and data cleaning. Some of the most common steps include: Data profiling:Almost everyone starts off by getting an understanding of their data set. More specifically, you can look at the shape of the data set with .shape and a description of your numerical variables with.describe().Data visualizations:Sometimes, it's useful to visualize your data withhistograms,boxplotsand scatterplots to better understand the relationships between variables and also to identify potentialoutliers.Syntax error: This includes making sure there's no white space, making sure letter casing is consistent, and checking for typos. You can check for typos by using.unique()or by using bar graphs.Standardization or normalization: Depending on the data set you're working with and the machine learning method you decide to use, it may be useful to standardize or normalize your data so that different scales of different variables dont negatively impact the performance of your model.Handling null values:There are a number of ways tohandle null valuesincluding deleting rows with null values altogether, replacing null values with the mean/median/mode, replacing null values with a new category (i.e., unknown), predicting the values, or using machine learning models that can deal with null values.Other things to consider:Removing irrelevant data,removing duplicatesand type conversion."
Technical,ML,How Do You Deal With Unbalanced Binary Classification?,"There are a number of ways to handle unbalanced binary classification, assuming that you want to identify the minority class: First, you want to reconsider the metrics that youd use to evaluate your model. The accuracy of your model might not be the best metric to look at,and Ill use an example to explain why. Lets say 99 bank withdrawals were not fraudulent and one withdrawal was. If your model simply classified every instance as not fraudulent, it would have an accuracy of 99 percent. Therefore, you may want to consider using metrics likeprecision and recall.Another method to improve unbalanced binary classification is to increase the cost of misclassifying the minority class. By increasing the penalty of such, the model should classify the minority class more accurately.Finally, you can improve the balance of classes by oversampling the minority class or by under-sampling the majority class."
Technical,ML,What Is the Difference Between a Boxplot and a Histogram?,"While boxplots and histograms are visualizations used to show the distribution of the data, they communicate information differently. Histograms are bar charts that show the frequency of a numerical variables values and are used to approximate the probability distribution of the given variable. It allows you to quickly understand the shape of the distribution, the variation andpotential outliers. Boxplots communicate different aspects of the distribution of data. While you cant see the shape of the distribution through a boxplot, you can gather other information like the quartiles, the range and outliers. Boxplots are especially useful when you want to compare multiple charts at the same time because they take up less space than histograms."
Technical,ML,"Describe Different Regularization Methods, Such as L1 and L2 Regularization?","BothL1 and L2 regularizationare methods used to reduce the overfitting of training data.Least squaresminimizes the sum of the squared residuals, which can result in low bias but high variance. L2 regularization, also calledridge regression, minimizes the sum of the squared residuals plus lambda times the slope squared. This additional term is called the ridge regression penalty. This increases thebias of the model, making the fit worse on the training data but also decreases the variance. If you take the ridge regression penalty and replace it with the absolutevalue of the slope, then you get Lasso regression or L1 regularization. L2 is less robust but has a stable solution and always one solution. L1 is more robust but has an unstable solution and can possibly have multiple solutions."
Technical,ML,What Is Cross-Validation?,"Cross-validationis essentially a technique used to assess how well a model performs on a new, independentdata set. The simplest example of cross-validation is when you split your data into two groups:training data and testing data. You use the training data to build the model and the testing data to test the model."
Technical,ML,How Do You Define/Select Metrics?,"There isn't a one-size-fits-all metric. The metric(s) chosen to evaluate a machine learning model depends on various factors: Is it a regression or classification task?What is the business objective? For example, precision versus recall.What is the distribution of the target variable? There are a number of metrics that can be used, including adjusted r-squared, mean absolute error (MAE), mean squared error (MSE), accuracy, recall, precision, f1 score and the list goes on."
Technical,ML,"Explain What a False Positive and a False Negative Are. Why Is It Important to Distinguish One From the Other? Provide Examples When False Positives Are More Important Than False Negatives, False Negatives Are More Important Than False Positives and When They Are Equally Important.","False positive:A false positive is an incorrect identification of the presence of a condition when it's absent.False negative:A false negative is an incorrect identification of the absence of a condition when it's actually present. An example of when false negatives are more important than false positives is when screening for cancer. It's much worse to say that someone doesn't have cancer when they do, instead of saying that someone does and later realizing that they don't. This is a subjective argument, but false positives can be worse than false negatives from a psychological point of view. For example, a false positive for winning the lottery could be a worse outcome than a false negative because people normally don't expect to win the lottery anyways."
Technical,ML,What Is the Difference Between Supervised Learning and Unsupervised Learning? Give Concrete Examples.,"Supervised learninginvolves learning a function that maps an input to an output based on example input-output pairs. For example, if I had a data set with two variables, age (input) and height (output), I could implement a supervised learning model to predict the height of a person based on their age. Unsupervised learning is used to draw inferences and find patterns from input data without references to labeled outcomes. A common use of unsupervised learning is grouping customers by purchasing behavior to find target markets."
Technical,ML,When Would You Use Random Forests vs. Support Vector Machines and Why?,"There are a couple of reasons why arandom forestis a better choice of model than asupport vector machine (SVM): Random forests allow you to determine the feature importance. SVMs cant do this.Random forests are much quicker and simpler to build than an SVM.For multi-class classification problems, SVMs require a one-vs-rest method, which is less scalable and more memory intensive."
Technical,ML,Why Is Dimension Reduction Important?,Dimensionality reductionis the process of reducing the number of features in a data set. This is important mainly in the case when you want to reduce variance in your model (overfitting).
Technical,ML,What Is Principal Component Analysis? Explain the Sort of Problems You Would Use PCA for.,"In its simplest sense,PCAinvolves projecting higher dimensional data, such as three dimensions, to a smaller space, like two dimensions. This results in a lower dimension of data, two dimensions instead of three, while keeping all original variables in the model. PCA is commonly used for compression purposes, to reduce required memory and to speed up the algorithm, as well as for visualization purposes, making it easier to summarize data."
Technical,ML,Why Is Naive Bayes Bad? How Would You Improve a Spam Detection Algorithm That Uses Naive Bayes?,"One major drawback ofnaive Bayesis that it holds a strong assumption in that the features are assumed to be uncorrelated with one another, which typically is never the case. One way to improve such an algorithm that uses naive Bayes is to decorrelate the features so that the assumption holds true."
Technical,ML,What Are the Drawbacks of a Linear Model?,"There are a couple of drawbacks to alinear model: A linear model holds some strong assumptions that may not be true in application. It assumes a linear relationship, multivariate normality, no or little multicollinearity, no auto-correlation and homoscedasticityA linear model cant be used for discrete or binary outcomes.You cant vary the model flexibility of a linear model."
Technical,ML,Do You Think 50 Small Decision Trees Are Better Than a Large One? Why?,"Another way of asking this question is a random forest a better model than adecision tree? And the answer is yes, because a random forest is an ensemble method that takes many weak decision trees to make a strong learner. Random forests are more accurate, more robust and less prone to overfitting."
Technical,ML,Why Is Mean Square Error a Bad Measure of Model Performance? What Would You Suggest Instead?,"Mean squared error (MSE) gives a relatively high weight to large errors. Therefore, MSE tends to put too much emphasis on large deviations. A more robust alternative is mean absolute deviation (MAD)."
Technical,ML,What Are the Assumptions Required for Linear Regression? What If Some of These Assumptions Are Violated?,"The assumptions are as follows: The sample data used to fit the model is representative of the population.The relationship between X and the mean of Y is linear.The variance of the residual is the same for any value of X (homoscedasticity).Observations are independent of each otherFor any value of X, Y is normally distributed.Extreme violations of these assumptions will make the results redundant. Small violations of these assumptions will result in a greater bias or variance of the estimate."
Technical,ML,What Is Collinearity and What to Do With It? How to Remove Multicollinearity?,"Multicollinearity exists when an independent variable is highly correlated with another independent variable in a multiple regression equation. This can be problematic because it undermines the statistical significance of an independent variable. You could use the variance inflation factors (VIF) to determine if there is any multicollinearity between independent variables, a standard benchmark is that if the VIF is greater than five then multicollinearity exists."
Technical,ML,How to Check If the Regression Model Fits the Data Well?,There are a couple of metrics that you can use: R-squared/Adjusted R-squared: Relative measure of fit.F1 score: Evaluates the null hypothesis that all regression coefficients are equal to zero vs the alternative hypothesis that at least one doesn't equal zeroRoot mean-squared error (RMSE):Absolute measure of fit.
Technical,ML,What Is a Decision Tree?,"Decision trees are a popular model, used in operations research, strategic planning and machine learning. Each square above is called a node, and the more nodes you have, the more accurate your decision tree will be (generally). The last nodes of the decision tree, where a decision is made, are called the leaves of the tree. Decision trees are intuitive and easy to build but fall short when it comes to accuracy."
Technical,ML,What Is a Random Forest? Why Is It Good?,"Random forests are an ensemble learning technique that builds off of decision trees. Random forests involve creating multiple decision trees usingbootstrappeddata sets of the original data and randomly selecting a subset of variables at each step of the decision tree. The model then selects the mode of all of the predictions of each decision tree. By relying on a majority wins model, it reduces the risk of error from an individual tree. For example, if we created one decision tree, the third one, it would predict 0. But if we relied on the mode of all four decision trees, the predicted value would be one. This is the power of random forests. Random forests offer several other benefits including strong performance, can model non-linear boundaries, no cross-validation needed and gives feature importance."
Technical,ML,What Is a Kernel? Explain the Kernel Trick.,"A kernel is a way of computing the dot product of two vectorsx andy in some, possibly very high-dimensional, feature space, which is why kernel functions are sometimes called generalized dot product. The kernel trick is a method of using a linear classifier to solve a nonlinear problem by transforming linearly inseparable data to linearly separable ones in a higher dimension."
Technical,ML,Is It Beneficial to Perform Dimensionality Reduction Before Fitting an SVM? Why or Why Not?,"When the number of features is greater than the number of observations, then performing dimensionality reduction will generally improve the SVM."
Technical,ML,What Is Overfitting?,"Overfitting is an error where the model fits the data too well, resulting in a model with high variance and low bias. As a consequence, an overfit model will inaccurately predict new data points even though it has a high accuracy on the training data."
Technical,ML,What Is Boosting?,"Boosting is an ensemble method to improve a model by reducing its bias and variance, ultimately converting weak learners to strong learners. The general idea is to train a weak learner and sequentially iterate and improve the model by learning from the previous learner."
Technical,Statistics & Probability,"If the Probability That an Item at Location A is 0.6 and 0.8 at location B, What Is the Probability That the Item Would be Found on Amazon?","We need to make some assumptions about this question before we can answer it. Lets assume that there are two possible places to purchase a particular item on Amazon and the probability of finding it at location A is 0.6 and B is 0.8. The probability of finding the item on Amazon can be explained as so: We can reword the above asP(A) = 0.6 and P(B) = 0.8. Furthermore, lets assume that these are independent events, meaning that the probability of one event is not impacted by the other. We can then use the formula: P(A or B) = P(A) + P(B) - P(A and B)P(A or B) = 0.6 + 0.8 - (0.6*0.8) P(A or B) = 0.92"
Technical,Statistics & Probability,Explain the Difference Between Convex and Non-Convex Cost Function. What Does It Mean When a Cost Function Is Non-Convex?,"A convex function is one where a line drawn between any two points on the graph lies on or above the graph. It has one minimum. A non-convex function is one where a line drawn between any two points on the graph may intersect other points on the graph. Its characterized as wavy. When a cost function is non-convex, it means that theres a likelihood that the function may find local minima instead of the global minimum, which is typically undesired in machine learning models from an optimization perspective."
Technical,Statistics & Probability,Describe Markov Chains.,"A Markov chainis a mathematical model that predicts the probability that a sequence of events will occur based on a previous event. For example, predicting the next word to appear in a search query. It utilizes a transition matrix and initial state vector. The actual math behind Markov chains requires knowledge onlinear algebra and matrices."
Technical,Statistics & Probability,"A Box has 12 Red Cards and 12 Black Cards. Another Box has 24 Red Cards and 24 Black Cards. You Want to Draw Two Cards at Random From One of the Two Boxes, One Card at a Time. Which Box has a Higher Probability of Getting Cards of the Same Color and Why?","The box with 24 red cards and 24 black cards has a higher probability of getting two cards of the same color.Lets say the first card you draw from each deck is a red Ace. This means that in the deck with 12 reds and 12 blacks, theres now 11 reds and 12 blacks. Therefore your odds of drawing another red are equal to11/(11+12) or 11/23. In the deck with 24 reds and 24 blacks, there would then be 23 reds and 24 blacks. Therefore your odds of drawing another red are equal to23/(23+24) or 23/47. Since23/47 > 11/23, the second deck with more cards has a higher probability of getting the same two cards."
Technical,Statistics & Probability,"You are at a Casino and Have Two Dice to Play With. You win $10 Every Time You Roll a 5. If You Play Until You Win and Then Stop, What Is the Expected Payout?","Let's assume that it costs $5 every time you want to play.There are 36 possible combinations with two dice. Of the 36 combinations, there are four combinations that result in rolling a five. This means that there is a4/36or1/9chance of rolling a five.A1/9chance of winning means youll lose eight times and win once (theoretically). Therefore, your expected payout is equal to$10.00 * 1 -$5.00 * 9= -$35.00."
Technical,Statistics & Probability,How Can You Tell If a Given Coin Is Biased?,"This isn't a trick question. The answer is simply to perform a hypothesis test: The null hypothesis is that the coin is not biased and the probability of flipping heads should equal 50 percent (p=0.5). The alternative hypothesis is that the coin is biased and p != 0.5.Flip the coin 500 times.Calculate the Z-score. If the sample is less than 30, you would calculate the t-statistics.Compare against the alpha, Two-tailed test, so 0.05/2 = 0.025.If p-value > alpha, the null is not rejected and the coin is not biased. If p-value < alpha, the null is rejected and the coin is biased."
Technical,Statistics & Probability,Make an Unfair Coin Fair.,"Since a coin flip is a binary outcome, you can make an unfair coin fair by flipping it twice. If you flip it twice, there are two outcomes that you can bet on: heads followed by tails or tails followed by heads. P(heads) * P(tails) = P(tails) * P(heads) This makes sense since each coin toss is an independent event.This means that if you getheads headsortails tails, you would need to reflip the coin."
Technical,Statistics & Probability,"You're given 40 Cards With Four Different Colors 10 Green Cards, 10 Red Cards, 10 Blue Cards and 10 Yellow Cards. The Cards of Each Color Are Numbered From One to 10. Two Cards Are Picked at Random. Find Out the Probability That the Cards Picked Arent the Same Number and Same Color.","Since these events aren't independent, we can use the rule: P(A and B) = P(A) * P(B|A), which is also equal toP(not A and not B) = P(not A) * P(not B | not A). For example: P(not 4 and not yellow) = P(not 4) * P(not yellow | not 4)P(not 4 and not yellow) = (36/39) * (27/36)P(not 4 and not yellow) = 0.692 Therefore, the probability that the cards picked are not the same number and the same color is 69.2 percent."
Technical,Statistics & Probability,How Do You Assess the Statistical Significance of an Insight?,"You would perform hypothesis testing to determine statistical significance. First, you would state the null hypothesis and alternative hypothesis. Second, you would calculate the p-value, the probability of obtaining the observed results of a test assuming that the null hypothesis is true. Last, you would set the level of the significance (alpha), and if the p-value is less than the alpha, you would reject the null. In other words, the result is statistically significant."
Technical,Statistics & Probability,Explain What a Long-Tailed Distribution Is and Provide Three Examples of Relevant Phenomena That Have Long Tails. Why Are They Important in Classification and Regression Problems?,"A long-tailed distribution is a type of heavy-tailed distribution that has a tail (or tails) that drops off gradually and asymptotically. Three practical examples include the power law, thePareto principle, more commonly known as the 8020 rule and product sales (i.e., best selling products versus others). Its important to be mindful of long-tailed distributions in classification and regression problems because the least frequently occurring values make up the majority of the population. This can ultimately change the way that you deal with outliers, and it also conflicts with some machine learning techniques with the assumption that the data isnormally distributed."
Technical,Statistics & Probability,What Is the Central Limit Theorem? Why Is It Important?,"Central limit theoremis the statistical concept that as a sample size expands, the mean of all samples will equal the mean of the population, and the distribution of means will follow a normal distribution. The central limit theorem is important because it is used in hypothesis testing and also to calculate confidence intervals."
Technical,Statistics & Probability,"Explain Selection Bias With Regard to a Data Set, Not Variable Selection. Why Is It Important? How Can Data Management Procedures, Such as Missing Data Handling, Make It Worse?","Selection biasis the phenomenon of selecting individuals, groups or data for analysis in such a way that proper randomization is not achieved, ultimately resulting in a sample that is not representative of the population. Understanding and identifying selection bias is important because it can significantly skew results and provide false insights about a particular population group. Types of selection biasinclude: Sampling bias: A biased sample caused by non-random sampling.Time interval: Selecting a specific time frame that supports the desired conclusion. For example, conducting a sales analysis near Christmas.Exposure: This includes clinical susceptibility bias, protopathic bias, indication bias.Data: Includes cherry-picking, suppressing evidence and the fallacy of incomplete evidence.Attrition: Attrition bias is similar to survivorship bias, where only those that ""survived"" a long process are included in an analysis, or failure bias, where those that failed are only included. Observer selection: Related to the Anthropic principle, which is a philosophical consideration that any data we collect about the universe is filtered by the fact that, in order for it to be observable, it must be compatible with the conscious and sapient life that observes it. Handling missing data can make selection bias worse because different methods impact the data in different ways. For example, if you replace null values with the mean of the data, you're adding bias in the sense that you're assuming that the data is not as spread out as it might actually be."
Technical,Statistics & Probability,Provide a Simple Example of How an Experimental Design Can Help Answer a Question About Behavior. How Does Experimental Data Contrast With Observational Data?,"Observational data comes from observational studies, which are when you observe certain variables and try to determine if there is any correlation. Experimental data comes from experimental studies, which are when you control certain variables and hold them constant to determine if there is any causality. An example of experimental design is the following: split a group up into two. The control group lives their lives normally. The test group is told to drink a glass of wine every night for 30 days. Then research can be conducted to see how wine affects sleep."
Technical,Statistics & Probability,Is Mean Imputation of Missing Data Acceptable Practice? Why or Why Not?,"Mean imputationis the practice of replacing null values in a data set with the mean of the data. Mean imputation is generally bad practice because it doesn't take into account featurecorrelation. For example, imagine we have a table showing age and fitness score and imagine that an 80-year-old has a missing fitness score. If we took the average fitness score from an age range of 15-to-80 years old, then the 80-year-old will appear to have a much higher fitness score than they actually should. Second, mean imputation reduces the variance of the data and increases bias in our data. This leads to a less accurate model and a narrower confidence interval due to a smaller variance."
Technical,Statistics & Probability,"What Is an Outlier? Explain How You'd Screen for Outliers and What Youd Do If You Found Them in Your Data Set. Explain What an Inlier Is, How Youd Screen for Them and What Youd Do If You Found Them in Your Data Set.","An outlier is a data point that differs significantly from other observations. Depending on the cause of the outlier, they can be bad from a machine learning perspective because they can worsen the accuracy of a model. If a measurement error caused the outlier, its important to remove them from the data set. There are a couple of ways toidentify outliers. Z-Score/Standard Deviations If we know that 99.7 percent of data in a data set lies within three standard deviations, then we can calculate the size of one standard deviation, multiply it by 3, and identify the data points that are outside of this range. Likewise, we can calculate the z-score of a given point, and if its equal to plus-or-minus three, then its an outlier. There are a few contingencies that need to be considered when using this method. The data must be normally distributed, this is not applicable for small data sets, and the presence of too many outliers can throw off z-score. Interquartile Range (IQR) IQR, the concept used to build boxplots, can also be used to identify outliers. The IQR is equal to the difference between the third quartile and the first quartile. You can then identify if a point is an outlier if it is less than Q11.5*IRQ or greater than Q3 + 1.5*IQR. This comes to approximately 2.698 standard deviations.Other methods include DBScan clustering, isolation forests and robust random cut forests. InlierAn inlier is a data observation that lies within the rest of the data set and is unusual or an error. Since it lies in the data set, it is typically harder to identify than an outlier and requires external data to identify them. Should you identify any inliers, you can simply remove them from the data set to address them."
Technical,Statistics & Probability,How Do You Handle Missing Data? What Imputation Techniques Do You Recommend?,"There are several ways to handlemissing data: Delete rows with missing data. Mean/Median/Mode imputation. Assigning a unique value.Predicting the missing values. Using an algorithm which supports missing values, like random forests. The best method is to delete rows with missing data as it ensures that no bias or variance is added or removed, and ultimately results in a robust and accurate model. However, this is only recommended if there's a lot of data to start with and the percentage of missing values is low."
Technical,Statistics & Probability,"You Have Data on the Duration of Calls to a Call Center. Generate a Plan for How You Would Code and Analyze the Data. Explain a Plausible Scenario for What the Distribution of These Durations Might Look Like. How Could You Test, Even Graphically, Whether Your Expectations Are Accurate?","First I would conduct anexploratory data analysis(EDA) to clean, explore, and understand my data. As part of my EDA, I could compose a histogram of the duration of calls to see the underlying distribution. My guess is that the duration of calls would follow a lognormal distribution. The reason that I believe its positively skewed is because the lower end is limited to zero, since a call cant be negative seconds. However, on the upper end, it's likely for there to be a small proportion of calls that are extremely long relatively. You could use a Q-Q plot to confirm whether the duration of calls follows a lognormal distribution or not."
Technical,Statistics & Probability,Explain Likely Differences Between Administrative Data Sets and Data Sets Gathered From Experimental Studies. What Are Likely Problems Encountered With Administrative Data? How Do Experimental Methods Help Alleviate These Problems? What Problem Do They Bring?,"Administrative data sets are typically data sets used by governments or other organizations for non-statistical reasons. Administrative data sets are usually larger and more cost-efficient than experimental studies. They are also regularly updated assuming that the organization associated with the administrative data set is active and functioning. At the same time, administrative data sets may not capture all of the data that one may want and may not be in the desired format either. It is also prone to quality issues and missing entries."
Technical,Statistics & Probability,"You Are Compiling a Report for User Content Uploaded Every Month and Notice a Spike in Picture Uploads in October. What Might You Think Is the Cause of This, and How Would You Test It?","There are a number of potential reasons for a spike in photo uploads: A new feature may have been implemented in October which involves uploading photos and gained a lot of traction by users. For example, a feature that gives the ability to create photo albums.Similarly, its possible that the process of uploading photos before was not intuitive and was improved in the month of October.There may have been a viral social media movement that involved uploading photos that lasted for all of October. For example, Movember but something more scalable.Its possible that the spike is due to people posting pictures of themselves in costumes for Halloween.The method of testing depends on the cause of the spike, but you would conduct hypothesis testing to determine if the inferred cause is the actual cause."
Technical,Statistics & Probability,"Give Examples of Data That Doesn't Have a Gaussian Distribution, Nor Log-Normal","Any type of categorical data won't have a gaussian distribution or lognormal distribution.Exponential distributions:For example, the amount of time that a car battery lasts or the amount of time until an earthquake occurs."
Technical,Statistics & Probability,What Is Root Cause Analysis? How to Identify a Cause vs. a Correlation? Give Examples,"Root cause analysis isa method of problem-solving used for identifying the root cause(s) of a problem. Correlationmeasures the relationship between two variables, ranging from -1 to 1. Causationis when a first event appears to have caused a second event. Causation essentially looks at direct relationships while correlation can look at both direct and indirect relationships. For example, a higher crime rate is associated with higher sales in ice cream in Canada, in other words, they are positively correlated. However, this doesnt mean that one causes another. Instead, its because both occur more when its warmer outside. You can test for causation using hypothesis testing or A/B testing."
Technical,Statistics & Probability,Give an example where the median is a better measure than the mean,When there are a number of outliers that positively or negatively skew the data.
Technical,Statistics & Probability,"Given Two Fair Dice, What Is the Probability of Getting Scores That Sum to Four? To Eight?","There are four combinations of rolling a four(1+3, 3+1, 2+2): P(rolling a 4) = 3/36 = 1/12. There are combinations of rolling an eight(2+6, 6+2, 3+5, 5+3, 4+4): P(rolling an 8) = 5/36."
Technical,Statistics & Probability,What Is the Law of Large Numbers?,"The law of large numbers is a theory that states that as the number of trials increases, the average of the result will become closer to the expected value. For example, flipping heads using a fair coin 100,000 times should be closer to 0.5 than 100 times."
Technical,Statistics & Probability,"When You Sample, What Bias Are You Inflicting?",Potential biases include the following: Sampling bias:A biased sample caused by non-random sampling.Under coverage bias:Sampling too few observations.Survivorship bias:Error of overlooking observations that did not make it past a form of selection process.
Technical,Statistics & Probability,How Do You Control for Biases?,"There are many things that you can do to control and minimize bias. Two common things include randomization, where participants are assigned by chance, and random sampling, sampling in which each member has an equal probability of being chosen."
Technical,Statistics & Probability,What Are Confounding Variables?,"A confounding variable, or a confounder, is a variable that influences both the dependent variable and the independent variable, causing a spurious association, a mathematical relationship in which two or more variables are associated but not causally related."
Technical,Statistics & Probability,What Is A/B Testing?,"A/B testingis a form of hypothesis testing and two-sample hypothesis testing to compare two versions, the control and variant, of a single variable. It is commonly used to improve and optimizeuser experienceandmarketing."
Technical,Statistics & Probability,How Do You Prove That Males Are on Average Taller Than Females?,"You can use hypothesis testing to prove that males are taller on average than females. The null hypothesis would state that males and females are the same height on average, while the alternative hypothesis would state that the average height of males is greater than the average height of females. Then you would collect a random sample of heights of males and females and use a t-test to determine if you reject the null or not."
Technical,Statistics & Probability,A Random Variable X Is Normal With Mean 1020 and a Standard Deviation 50. Calculate P(X>1200).,"Using Excel: p = 1-norm.dist(1200, 1020, 50, true)p = 0.000159"
Technical,Statistics & Probability,Consider the Number of People Who Show Up at a Bus Station Is Poisson with Mean 2.5/h. What Is the Probability That at Most Three People Show Up in a Four Hour Period?,"x = 3mean = 2.5*4 = 10 Using Excel to solve: p = poisson.dist(3,10,true)p = 0.010336"
Technical,Statistics & Probability,You Are Running for Office and Your Pollster Polled 100 People. Sixty of Them Claimed Theyll Vote for You. Can You Relax?,"First we need to make a few assumptions: Assume that theres only you and one other opponent.Also, assume that we want a 95 percent confidence interval. This gives us a z-score of 1.96."
Technical,Statistics & Probability,A Geiger Counter Records 100 Radioactive Decays in 5 Minutes. Find an Approximate 95 Percent Interval for the Number of Decays Per Hour.,"Since this is a Poisson distribution question,mean = lambda = variance, which also means that standard deviation = square root of the mean.A 95 percent confidence interval implies a z score of 1.96.One standard deviation=10. Therefore, the confidence interval= 100 +/- 19.6 = [964.8, 1435.2]."
Technical,Statistics & Probability,The Homicide Rate in Scotland Fell Last Year to 99 From 115 the year before. Is This Reported Change Noteworthy?,"Since this is a Poisson distribution question, mean = lambda = variance, which also means that standard deviation = square root of the meanA 95 percent confidence interval implies a z score of 1.96.One standard deviation= sqrt(115) = 10.724 Therefore the confidence interval= 115+/- 21.45 = [93.55, 136.45]. Since 99 is within this confidence interval, we can assume that this change is not very noteworthy."
Technical,Statistics & Probability,Suppose That Diastolic Blood Pressures (DBPs) for Men Aged 35-to-44 Years Old Are Normally Distributed With a Mean of 80 mm Hg and a Standard Deviation of 10. Whats the Probability That a Random 35-to-44 Year Old has a DBP Less Than 70?,"Since 70 is one standard deviation below the mean, take the area of the Gaussian distribution to the left of one standard deviation, whichequals2.3 + 13.6 = 15.9 percent"
Technical,Statistics & Probability,A Diet Pill Is Given to 9 Subjects Over Six Weeks. The Average Difference in Weight (Follow Up Baseline) Is -2 Pounds. What Would the Standard Deviation of the Difference in Weight Have to Be for the Upper Endpoint of the 95 Percent T-Confidence Interval to Touch 0?,"Upper bound= mean + t-score*(standard deviation/sqrt(sample size))0 = -2 + 2.306*(s/3)2 = 2.306 * s / 3 s = 2.601903 Therefore, the standard deviation would have to be at least approximately 2.60 for the upper bound of the 95 percent T-confidence interval to touch 0."
Technical,Case Study,How Would a Change of Amazon Prime's Membership Fee Affect the Market?,"Lets take the instance where theres an increase in the prime membership fee. There are two parties involved, the buyers and the sellers. For the buyers, the impact of an increase in a Prime membership fee ultimately depends on the price elasticity of demand for the buyers. If the price elasticity is high, then a given price increase will result in a large drop in demand and vice versa. Buyers that continue to purchase a membership fee are likely Amazons most loyal and active customers. Theyre also likely to place a higher emphasis on products with Prime. Sellers will take a hit, as there is now a higher cost of purchasing Amazons basket of products. That being said, some products will take a harder hit while others may not be impacted. Its likely that premium products that Amazons most loyal customers purchase would not be affected as much, like electronics."
Technical,Case Study,"If 70 Percent of Facebook Users on iOS Use Instagram, But Only 35 Percent of Users on Android Use Instagram, How Would You Investigate the Discrepancy?","There are a number of possible variables that can cause such a discrepancy that I would check to see: The demographics of iOS and Android users might differ significantly. For example, according toHootsuite, 43 percent of females use Instagram as opposed to 31 percent of men. If the proportion of female users for iOS is significantly larger than for Android, then this can explain the discrepancy, or at least a part of it. This can also be said for age, race, ethnicity and location, etc.Behavioral factors can also have an impact on the discrepancy. If iOS users use their phones more heavily than Android users, its more likely that theyll indulge in Instagram and other apps than someone who spent significantly less time on their phones.Another possible factor to consider is how Google Play and the App Store differ. For example, if Android users have significantly more apps (and social media apps) to choose from, that may cause greater dilution of users.Lastly, any differences in the user experience can deter Android users from using Instagram compared to iOS users. If the app is more buggy for Android users than iOS users, they'll be less likely to be active on the app."
Technical,Case Study,Likes Per User and Minutes Spent on a Platform Are Increasing but Total Number of Users Are Decreasing. What Could Be the Root Cause of It?,"Generally, you would want to probe the interviewer for more information but lets assume that this is the only information that they are willing to give. Focusing on likes per user, there are two reasons why this would have gone up. The first reason is that the engagement of users has generally increased on average over time this makes sense because as time passes, active users are more likely to be loyal users as using the platform becomes a habitual practice. The other reason why likes per user would increase is that the denominator, the total number of users, is decreasing. Assuming that users that stop using the platform are inactive users, or users with little engagement and fewer likes than average, this would increase the average number of likes per user. The explanation above can also be applied to minutes spent on the platform. Active users are becoming more engaged over time, while users with little usage are becoming inactive. Overall the increase in engagement outweighs the users with little engagement. To take it a step further, its possible that the users with little engagement are bots that Facebook has been able to detect. But over time, Facebook has been able to develop algorithms to spot and remove bots. If there were a significant number of bots before, this can potentially be the root cause of this phenomenon."
Technical,Case Study,Facebook Likes Are Up 10 Percent Year-Over-Year. Why Could This Be?,"The total number of likes in a given year is a function of the total number of users and the average number of likes per user, which Ill refer to as engagement. Some potential reasons for an increase in the total number of users are the following: Uusers acquired due to international expansion and younger age groups signing up for Facebook as they get older. Some potential reasons for an increase in engagement are an increase in usage of the app from users that are becoming more and more loyal, new features and functionality and an improved user experience."
Technical,Case Study,"If a PM Says That They Want to Double the Number of Ads in NewsFeed, How Would You Figure Out If This Is a Good Idea or Not?","You can perform an A/B test splitting the users into two groups: a control group with the normal number of ads and a test group with double the number of ads. Then you would choose the metric to define what a good idea is. For example, we can say that the null hypothesis is that doubling the number of ads will reduce the time spent on Facebook and the alternative hypothesis is that doubling the number of ads wont have any impact on the time spent on Facebook. However, you can choose a different metric like the number of active users or the churn rate. Then you would conduct the test and determine the statistical significance of the test to reject or not reject the null."
Technical,Case Study,"Define Lift, KPI, Robustness, Model Fitting, Design of Experiments and the 80/20 Rule?","Lift lift is a measure of the performance of a targeting model measured against a random choice targeting model. In other words, lift tells you how much better your model is at predicting things than if you had no model.KPIKPI stands for key performance indicator, which is a measurable metric used to determine how well a company is achieving its business objectives. For example, error rate.Robustness Generally, robustness refers to a systems ability to handle variability and remain effective.Model FittingModel fitting refers to how well a model fits a set of observations.Design of Experiments Also known as DOE, design of experiments is the design of any task that aims to describe and explain the variation of information under conditions that are hypothesized to reflect the variable. In essence, an experiment aims to predict an outcome based on a change in one or more inputs (independent variables.80/20 Rule Also known as the Pareto principle, the 80/20 rule states that 80 percent of the effects come from 20 percent of the causes. For example, 80 percent of sales come from 20 percent of the customers."
Technical,Case Study,Define Quality Assurance and Six Sigma.,Quality assurance:An activity or set of activities focused on maintaining a desired level of quality by minimizing mistakes and defects.Six sigma:Six sigma is a specific type of quality assurance methodology composed of a set of techniques and tools for process improvement. A six sigma process is one in which 99.99966 percent of all outcomes are free of defects.
Technical,ML,What is the Purpose of Feature Selection?,"Feature Selection identifies the most relevant predictors to:Improve model performance.Reduce overfitting.Lower computational cost. Techniques include:Filter Methods: Correlation, Chi-Square.Wrapper Methods: Recursive Feature Elimination (RFE).Embedded Methods: Feature importance from models like Random Forest."
Technical,ML,"What are Decision Trees, and How Do They Work?","Decision Trees are supervised learning algorithms used for classification and regression. They split data recursively based on feature thresholds to minimize impurity (e.g., Gini Index, Entropy). Pros:Easy to interpret.Handles non-linear relationships. Cons:Prone to overfitting (addressed by pruning or using ensemble methods)."
Technical,ML,What is Batch Normalization?,"Batch Normalization is a technique to improve deep learning model training by normalizing the inputs of each layer:It standardizes activations to have zero mean and unit variance within each mini-batch.It reduces internal covariate shifts, stabilizes training, and allows higher learning rates."
Technical,Statistics & Probability,How do you treat outliers ?,"Outliers are extreme values that differ significantly from the majority of data points. They can distort statistical analyses and machine learning models. Treatment methods include:Detection Methods:Z-Score: If a data points Z-score (standardized score) is above 3 or below -3, it may be an outlier.IQR (Interquartile Range): Outliers are data points outside [Q1 - 1.5 * IQR, Q3 + 1.5 * IQR].Boxplots & Scatterplots: Useful for visual detection.Handling Methods:Remove the outlier (if it's a data entry error).Cap and Floor the values (Winsorization): Replace extreme values with upper/lower percentiles.Transform the data using log/square root transformation to reduce impact.Use robust models like decision trees that are less sensitive to outliers."
Technical,ML,Explain confusion matrix ?,"A confusion matrix is a table used to evaluate the performance of a classification model.It contains True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).Metrics Derived from Confusion Matrix:Accuracy = (TP + TN) / (TP + TN + FP + FN)Precision = TP / (TP + FP)Recall = TP / (TP + FN)F1-score = 2 * (Precision * Recall) / (Precision + Recall)"
Technical,ML,Explain PCA (Wanted me to explain the co-variance matrix and eigen vectors and values and the mathematical expression and mathematical derivation for co-variance matrix),"PCA is a dimensionality reduction technique that transforms data into a set of uncorrelated principal components.Mathematical Steps:Compute the Covariance Matrix: Measures how different variables are related.Find Eigenvalues & Eigenvectors: Eigenvectors define the new axes (principal components), and eigenvalues indicate variance explained.Sort Eigenvalues: Choose top k components.Project Data onto New Subspace.PCA helps remove redundancy and reduce computational cost."
Technical,Data Structures & Algorithms,How do you cut a cake into 8 equal parts using only 3 straight cuts ?,First cut: Vertically down the middle (divides into 2).Second cut: Another vertical cut perpendicular to the first (creates 4 pieces).Third cut: A horizontal slice through the middle (creates 8 pieces).
Technical,ML,Explain kmeans clustering,"K-Means is an unsupervised clustering algorithm that groups data into K clusters.Steps:Choose K random centroids.Assign each point to the nearest centroid.Compute new centroids as the mean of each cluster.Repeat until convergence.Use Cases: Image segmentation, anomaly detection, customer segmentation."
Technical,ML,How is KNN different from k-means clustering?,"KNN (K-Nearest Neighbors): Supervised learning algorithm used for classification.K-Means: Unsupervised clustering algorithm.KNN finds the most similar points for classification, while K-Means identifies patterns in unlabeled data."
Technical,ML,What would be your strategy to handle a situation indicating an imbalanced dataset?,"Resampling techniques:Oversampling (SMOTE): Synthesize new minority class samples.Undersampling: Remove samples from the majority class.Class weight adjustments: Increase weight for the minority class.Use different evaluation metrics: Prefer F1-score, ROC-AUC, Precision-Recall Curve instead of accuracy."
Technical,ML,Stock market prediction: You would like to predict whether or not a certain company will declare bankruptcy within the next 7 days (by training on data of similar companies that had previously been at risk of bankruptcy). Would you treat this as a classification or a regression problem?,"Since we are predicting bankruptcy (Yes/No), it is a classification problem.If we were predicting a continuous value (e.g., stock price), it would be regression."
Technical,Statistics & Probability,How to handle missing data? What imputation techniques can be used?,Delete missing values (if a small proportion).Mean/Median/Mode imputation.KNN imputation (predict missing values using similar data points).Interpolation (for time-series data).
Technical,ML,Explain topic modelling in NLP and various methods in performing topic modeling.,"Latent Dirichlet Allocation (LDA): Assumes documents are mixtures of topics, and topics are distributions of words.Non-Negative Matrix Factorization (NMF): Factorizes document-term matrix to extract topics."
Technical,Statistics & Probability,Explain how you would find and tackle an outlier in the dataset.,Z-score & IQR for detection.Cap extreme values or use transformations.
Technical,ML,Explain back propagation in few words and its variants?,"Backpropagation is a key algorithm for training neural networks. It works by calculating the error (loss) between predicted and actual outputs and then adjusting the network's weights using gradient descent.Steps in Backpropagation:Forward Pass: Compute output based on current weights.Compute Error: Compare predicted output with actual values using a loss function.Backward Pass: Compute gradients of the loss function with respect to each weight.Weight Update: Adjust weights using an optimizer (like SGD, Adam, or RMSprop).Variants of Backpropagation:Stochastic Gradient Descent (SGD): Updates weights after each training example.Mini-Batch Gradient Descent: Updates weights after a batch of examples.Adam Optimizer: Combines momentum and adaptive learning rates for efficient training."
Technical,ML,"Is interpretability important for machine learning model? If so, ways to achieve interpretability for a machine learning models?","Yes, interpretability is crucial, especially in domains like healthcare, finance, and law, where models must be explainable.Ways to Improve Interpretability:Feature Importance Analysis: Identify which features contribute most to predictions.SHAP (Shapley Additive Explanations): Assigns an impact value to each feature.LIME (Local Interpretable Model-agnostic Explanations): Explains individual predictions by approximating the model locally.Decision Trees & Linear Models: These are inherently interpretable compared to deep learning models.Partial Dependence Plots (PDPs): Show how individual features affect predictions."
Technical,ML,How would you design a data science pipeline?,"A data science pipeline consists of sequential steps to process and analyze data efficiently.Pipeline Steps:Data Collection: Gather raw data from various sources (databases, APIs, CSV files).Data Preprocessing: Handle missing values, remove duplicates, and normalize/scale features.Feature Engineering: Create meaningful features, one-hot encoding categorical variables.Model Selection & Training: Choose an appropriate ML model and train it.Model Evaluation: Assess performance using metrics like accuracy, F1-score, RMSE.Model Deployment: Deploy the trained model via an API, cloud service, or edge device.Monitoring & Maintenance: Continuously track model performance and update when needed."
Technical,ML,Explain bias - variance trade off. How does this affect the model?,"The bias-variance tradeoff refers to the balance between model complexity and generalization ability.High Bias (Underfitting): The model is too simple and fails to capture data patterns (e.g., Linear Regression).High Variance (Overfitting): The model learns noise instead of patterns, performing poorly on new data (e.g., Deep Neural Networks trained without regularization).How to Balance Bias and Variance?Use cross-validation to find an optimal model.Apply regularization techniques (L1, L2).Choose an appropriate model complexity (not too simple, not too complex)."
Technical,Statistics & Probability,How to determine if a coin is biased?,"We can check if a coin is biased using statistical hypothesis testing.Methods:Chi-Square Test: Compares observed vs. expected outcomes.Binomial Test: Tests if heads and tails occur with a 50% probability.Frequentist Approach:Flip the coin n times and count the proportion of heads (p).If p significantly deviates from 0.5, the coin may be biased."
Technical,Statistics & Probability,How would you check if the model is suffering from multi Collinearity?,"Multicollinearity occurs when independent variables are highly correlated, leading to unstable model coefficients.Detection Methods:Variance Inflation Factor (VIF): If VIF > 10, multicollinearity exists.Correlation Matrix: Check high correlation (|r|?> 0.9) between independent variables.Condition Number: If its high, the dataset may have multicollinearity.Solutions:Remove redundant features.Use Principal Component Analysis (PCA) to reduce dimensionality.Use regularization techniques (Ridge Regression)."
Technical,ML,What is transfer learning? Steps you would take to perform transfer learning.,"Transfer learning involves reusing a pre-trained model (e.g., ResNet, BERT) on a new task to save training time and improve performance.Steps for Transfer Learning:Load a pre-trained model (e.g., ResNet for images, BERT for text).Remove the last layer and replace it with a new task-specific layer.Freeze initial layers to retain learned features.Fine-tune the last few layers on new data.Train & Evaluate the model."
Technical,ML,What are the approaches for solving class imbalance problem?,"Class imbalance occurs when one class significantly outnumbers another.Solutions:Oversampling: Generate synthetic samples (e.g., SMOTE - Synthetic Minority Over-sampling Technique).Undersampling: Reduce the majority class samples.Use Different Metrics: Prefer F1-score, Precision-Recall Curve, ROC-AUC instead of accuracy.Adjust Class Weights: Give more weight to the minority class."
Technical,Statistics & Probability,When sampling what types of biases can be inflected? How to control the biases?,Bias in sampling leads to unrepresentative data.Types of Sampling Bias:Selection Bias: Certain groups are over/underrepresented.Survivorship Bias: Only successful samples are analyzed.Recall Bias: Respondents fail to recall past events accurately.Confirmation Bias: Selecting samples that support a hypothesis.How to Control Bias?Ensure random sampling.Use stratified sampling if population groups differ significantly.Increase sample size for better representation.
Technical,ML,"Explain concepts of epoch, batch, iteration in machine learning.","Epoch: One complete pass through the entire dataset.Batch: A subset of the dataset used for one forward and backward pass.Iteration: The number of batches processed before an epoch is completed.Example: If a dataset has 10,000 samples and a batch size of 100, it takes 100 iterations to complete one epoch."
Technical,ML,What type of performance metrics would you choose to evaluate the different classification models and why?,"The choice of metric depends on the problem type:Accuracy: Only useful when classes are balanced.Precision: Used when false positives are costly (e.g., spam detection).Recall: Used when false negatives are costly (e.g., medical diagnosis).F1-Score: Harmonic mean of Precision and Recall (good for imbalanced data).ROC-AUC: Measures overall model performance across thresholds.Precision-Recall Curve: Preferred for imbalanced classification problems."
Technical,ML,What are some of the types of activation functions and specifically when to use them?,"ReLU (Rectified Linear Unit):Used in deep learning for faster training.Works well in hidden layers.Sigmoid:Used for binary classification in the output layer.Maps input to (0,1).Tanh (Hyperbolic Tangent):Similar to sigmoid but centered at 0 (-1 to 1).Works well in hidden layers when data is centered.Softmax:Used in multi-class classification.Outputs probabilities that sum to 1."
Technical,Statistics & Probability,What are the conditions that should be satisfied for a time series to be stationary?,"For a time series to be stationary, it must satisfy the following conditions:Constant Mean (?):The average value of the series should remain the same over time.No upward or downward trend.Constant Variance (? ^2 ):The fluctuations (spread of values) should remain stable over time.No increasing or decreasing volatility.Constant Autocovariance:The correlation between values at different time points should depend only on the lag (difference in time) and not on the actual time at which it is measured.How to Check Stationarity?Visual Inspection: Plot the time series and check for trends and variance changes.Augmented Dickey-Fuller (ADF) Test: A statistical test where a low p-value (< 0.05) suggests stationarity.KPSS Test: Tests whether the time series has a unit root (non-stationary).How to Make a Time Series Stationary?Differencing: Subtract current value from the previous value to remove trends.Log Transformation: Reduces variance if data has exponential growth.Moving Average Smoothing: Averages out fluctuations over time."
Technical,ML,"Suppose you had bank transaction data, and wanted to separate out likely fraudulent transactions. How would you approach it? Why might accuracy be a bad metric for evaluating success?","In Machine Learning, problems like fraud detection are usually framed as classification problems. In order to solve this problem we may use different features like amount, merchant, location, time etc associated with each transaction.One of the biggest challenge with fraud transaction detection is- majority of transactions are not fraud, so we have inbalance data!First step will be to do EDA and understand our data and intesity of class inbalance.In order to handle inbalance data problem we can use one of the following methodOversampling SMOTE (Synthetic Minority Over-sampling Technique)Undersampling One simple way of undersampling is randomly selecting a handful of samples from the class that is overrepresented.Combined Class Methods Use SMOTE together with edited nearest-neighbours (ENN). Here, ENN is used as the cleaning method after SMOTE over-sampling to obtain a cleaner space.Developed by Wilson (1972), the ENN method works by finding the K-nearest neighbor of each observation first, then check whether the majority class from the observations k-nearest neighbor is the same as the observations class or not.If the majority class of the observations K-nearest neighbor and the observations class is different, then the observation and its K-nearest neighbor are deleted from the dataset. In default, the number of nearest-neighbor used in ENN is K=3.As ENN removes the observation and its K-nearest neighbor instead of just removing observation and its 1-nearest neighbor that are having different classes. Thus, ENN can be expected to give more in-depth data cleaning.Test model performane for each of above technique and choose best performing model.Why might accuracy be a bad metric for evaluating success?In case of inbalance data accuracy metric is not usefull. Accuracy tells us how close a measured value is to the actual (true) value. But here we are more interested in fraud transactions.We dont mind declaring few good transactions as fruad but failing to identify fraud transaction is not acceptable. In such cases classification of true positives is a priority, hence precision metric make more sense."
Technical,ML,What are the assumptions for linear regression,"Linear regression assumptions are as below:Data should have linear relationship between X and Y (actually mean of Y)Data should be normally distributedNo or little multicollinearity (observations should be independent of each other)Assumption of additivity: This means that each feature (X) should affect the target (Y) independently. In other words, the influence of one feature on the target does not change because of another feature.Example: Let's say you're predicting house prices based on square footage and number of bedrooms. Additivity assumes that adding an extra bedroom increases the price by a certain amount, regardless of whether the house is large or small.If the impact of the bedroom varied based on house size (e.g., extra bedroom matters more in small houses), then the additivity assumption would be violated.Homoscedasticity: This means that the spread of the errors (the differences between your model's predictions and the actual values) should be roughly the same for all the predicted values of Y.Example: If you're predicting a person's weight based on their height, the spread of actual weights around the predicted weight should be similar whether the person is tall or short. If the spread varies widely (e.g., much larger for taller people), this assumption is violated."
Technical,ML,Why do we need confusion matrix,"We can not rely on a single value of accuracy in classification when the classes are imbalanced.For example, we have a dataset of 100 patients in which 5 have diabetes and 95 are healthy. However, if our model only predicts the majority class i.e. all 100 people are healthy then also we will have a classification accuracy of 95%.Confusion matrices are used to visualize important predictive analytics like recall, specificity, accuracy, and precision.Confusion matrices are useful because they give direct comparisons of values like True Positives, False Positives, True Negatives and False Negatives."
Technical,Statistics & Probability,Explain collinearity and technique to reduce it,"In statistics collinearity or multicollinearity is the phenomenon where one or more predictive variables(features) in multiple regression models are highly linearly related to each other.Technique to reduce multicollearityRemove highly correlated predictors from the model. If you have two or more factors with a high collinearity, remove one from the model. Because they supply redundant information, removing one of the correlated factors usually doesn't drastically reduce the R-squared. Consider using stepwise regression, best subsets regression, or specialized knowledge of the data set to remove these variables. Select the model that has the highest R-squared value.Principal Components Analysis(PCA) regression methods that cut the number of predictors to a smaller set of uncorrelated components."
Technical,ML,What is more important model accuracy or model performance?,"Model accuracy matters the most! inaccurate information is not usefull.Model performance can be improved by increasing the compute resources.Model accuracy and performance can be subjective to the problem in hand. For example, in analysis of medical images to determine if there is a disease (such as cancer), the accuracy extremely critical, even if the models would take minutes or hours to make a prediction.Some applications require real time performance, even if this comes at a cost of accuracy. For example, imagine a machine that views a fast conveyor belt carrying tomatoes, where it must separate the green from the red ones. Though an occasional error is undesired, the success of this machine is more determined by its ability to withstand its throughput.A more common example is face detection for recreational applications. People would expect a fast response from the app, though the occasional missed face would not render it useless"
Technical,ML,Explain confusion matrix,"What is a Confusion Matrix?Imagine a table that helps you see how well your machine learning model is performing, especially when it comes to classification tasks (like deciding if an email is spam or not).It compares the model's predictions to the actual, true labels of your data.Why is it useful?Beyond Accuracy: A confusion matrix gives you a more detailed look at your model's performance than just overall accuracy.Spotting Weaknesses: You can see which classes your model struggles with.Choosing the Right Metric: Depending on your problem, you might care more about minimizing false positives or false negatives. The confusion matrix helps you calculate metrics like precision, recall, and F1-score which focus on these aspects.In essence:A confusion matrix is like a scorecard for your classification model, showing you not just how many it got right, but also the specific types of errors it's making. This helps you understand its strengths and weaknesses, and make informed decisions about how to improve it.From our confusion matrix, we can calculate five different metrics measuring the validity of our model.ACCURACYAccuracy is the ratio of correctly identified subjects in a pool of subjects.Accuracy = (all correct / all) = (TP+TN)/(TP+FP+FN+TN)Accuracy answers the question: How many patients did we correctly identify out of all patients?PRECISIONPrecision is the ratio of correctly identified +ve subjects by test, against all +ve subjects identified by test.Precision = (true positives / predicted positives) = TP/(TP+FP)Precision answers the question: How many patients tested +ve are actually +ve?This metric is often used in cases where classification of true positives is a priority. For example, a spam email classifier would rather classify some spam emails as regular emails rather than classify some regular emails as spam. Thats why some spam emails end up in your main inbox, just to be safe. (Here true positives are the spam emails)SENSITIVITY (RECALL)Sensitivity is the ratio of correctly identified +ve subjects by test against all +ve subjects in reality.Sensitivity = (true positives / all actual positives)= TP/(TP+FN)Sensitivity answers the question: Of all the patients that are +ve, how many did the test correctly predict?This metric is often used in cases where classification of false negatives is a priority. A good example is the medical test that we used for illustration above. The government would rather have some healthy people labeled +ve than have an infected individual labeled -ve and spread the disease. We would rather be overly cautious and have false positives than risk wrongly identifying false negatives.SPECIFICITYSpecificity is the ratio of correctly identified -ve subjects by test against all -ve subjects in reality.Specificity = (true negatives / all actual negatives) = TN/(TN+FP)Specificity answers the question: Of all the patients that are -ve, how many did the test correctly predict?This metric is often used in cases where classification of true negatives is a priority. For example, a doping test will immediately ban an athlete if they are tested positive. We would not want to any drug-free athlete to be wrongly classified and banned.F1 SCOREF1 Score accounts for both precision and sensitivity.F1 Score = 2 (Recall Precision)/(Recall + Precision)It is often considered a better indicator of a classifiers performance than a regular accuracy measure as it compensates for uneven class distribution in the training dataset. For example, an uneven class distribution is likely to occur in insurance fraud detection, where a large majority of claims are legitimate and only a very small minority are fraudulent.Which metric to use is depends on the problem in hand"
Technical,Statistics & Probability,"In a test, students in section A scored with a mean of 75 and standard deviation of 10, while students in section B scored with a mean of 80 and standard deviation of 12? Melissa from section A and Ryan from section B both have scored 90 in this test. Who had a better performance in this test as compared to their classmates","To compare the two scores we need to standardize them to the same scale. We do that by calculating the Z score, which allows us to compare the 2 scores in units of standard deviations.Z score= (X- mean)/Standard DeviationMelissa's Z score = (90-75)/10 = 1.5Ryan's Z score = (90-80)/12 = 0.83Melissa has performed better."
Technical,Statistics & Probability,What is power of hypothesis test? Why is it important,"Remember that if actual value is positive and our model predicts it as negative then Type II error occuras (False negative). e.g. Calling a guilty person innocent, diaognosing cancer infected person as healthy etc.The probability of not commiting Type II error is called as power of hypothesis test. The higher probability we have of not commiting a type 2 error, the better our hypothesis test is."
Technical,ML,Explain Random forest algorithm,"Random forest is supervised learning algorithm and can be used to solve classification and regression problems.Since decision-tree create only one tree to fit the dataset, it may cause overfitting and model may not generalize well. Unlike decision tree random forest fits multiple decision trees on various sub samples of dataset and make the predictions by averaging the predictions from each tree.Averaging the results from multiple decision trees help to control the overfitting and results in much better prediction accuracy. As you may have noticed, since this algorithm uses multiple trees hence the name Random ForestThis algorithm is heavily used in various industries such as Banking and e-commerce to predict behavior and outcomes."
Technical,ML,Can Random Forest Algorithm be used both for Continuous and Categorical Target Variables,"Yes, Random Forest can be used for both continuous and categorical target (dependent) variables.In a random forest the classification model refers to the categorical dependent variable, and the regression model refers to the numeric or continuous dependent variable."
Technical,ML,What is Out-of-Bag Error in Random Forests,"Out-of-Bag is equivalent to validation or test data but it is calculated internally by Random Forest algorithm. In case of Sklearn if we set hyperparameter 'oob_score = True' then Out-of-Bag score will be calculated for every decision tree.Finally, we aggregate all the errors from all the decision trees and we will determine the overall OOB error rate for the classification."
Technical,ML,What is the use of proximity matrix in the random forest algorithm,A proximity matrix is used for the following cases :Missing value imputationDetection of outliers
Technical,ML,What is K Fold cross validation? Why do you use it?,"In case of K Fold cross validation input data is divided into a number of folds, hence the name K Fold. Suppose we have divided data into 5 folds i.e. K=5. Now we have 5 sets of data to train and test our model. So the model will get trained and tested 5 times, but for every iteration we will use one fold as test data and rest all as training data. Note that for every iteration, data in training and test fold changes which adds to the effectiveness of this method.This significantly reduces underfitting as we are using most of the data for training(fitting), and also significantly reduces overfitting as most of the data is also being used in validation set.K Fold cross validation helps to generalize the machine learning model, which results in better predictions on unknown data."
Technical,ML,How to handle missing data?,"Data can be missing because of mannual error or can be gennualy missing.Delete low quality records completely which have too much missing dataImpute the values by educated guess, taking average or regressionUse domain knwledge to impute values"
Technical,Statistics & Probability,What is outlier? How to handle them,"An outlier is an observation that lies an abnormal distance from other values in a random sample from a population.Data points above and below 1.5*IQR, are most commonly outliers.Outliers can drastically change the results of the data analysis and statistical modeling.Types of the outliersData entry errorsMeasuremental errorsIntentional outliers. This is commonly found in self-reported measures that involves sensitive data. For example: Teens would typically under report the amount of alcohol that they consume.Data processing erros. Whenever we perform data mining, we extract data from multiple sources. It is possible that some manipulation or extraction errors may lead to outliers in the dataset.Sampling error. For instance, we have to measure the height of athletes. By mistake, we include a few basketball players in the sample. This inclusion is likely to cause outliers in the dataset.Natutal oulier. When an outlier is not artificial (due to error), it is a natural outlier. For instance: In my problem assignment with one of the renowned insurance company, I noticed that the performance of top 50 financial advisors was far higher than rest of the population. Surprisingly, it was not due to any error. Hence, whenever we perform any data mining activity with advisors, we used to treat this segment separately.How to detect Outliers?Most commonly used method to detect outliers is visualization.We use various visualization methods, like Box-plot, Histogram, Scatter PlotUse capping methods. Any value which out of range of 5th and 95th percentile can be considered as outlierData points, three or more standard deviation away from mean are considered outlierApart from visualization we can also use Z-Score or Extreme Value Analysis (parametric) to detect outliers.How to remove outliers?Most of the methods used to handle missing values are aslo application in case ot outliersDeleting observationsWe delete outlier values if it is due to data entry error, data processing error or outlier observations are very small in numbers. We can also use trimming at both ends to remove outliers.Transforming and binning valuesTransforming variables can also eliminate outliers. Natural log of a value reduces the variation caused by extreme values. Binning is also a form of variable transformation. Decision Tree algorithm allows to deal with outliers well due to binning of variable.ImputingWe can use mean, median, mode imputation methods.Treat separatelyIf there are significant number of outliers, we should treat them separately in the statistical model. One of the approach is to treat both groups as two different groups and build individual model for both groups and then combine the output."
Technical,ML,"If deleting outliers is not an option, how will you handle them","I will try differen models. Data detected as outliers by linear model, can be fit by non-linear model.Try normalizing the data, this way the extreame datapoints are pulled to the similar range.We can use algorithms which are less affected by outliers.We can also create separate model to handle the outlier data points."
Technical,ML,You fit two linear models on a dataset. Model 1 has 25 predictors and model 2 has 10 predictors. What performance metric would you use to select the best model based on training dataset?,"First of all model performace is not directly proportional to the number of predictors, so we cant say that model with 25 predictors is better than the model with 10 predictorsHere important thing is to understand different evaluation metric for linear regresion and which one of them can help us identify the impact of number of predictors on model performance.Evaluation metric used for linear regression are MSE, MAE, R-squared, Adjusted R-squared, and RMSE.MSE penalizes large errors, MAE does not penalize large errors, RMSE penalizes large errors and R-squared or Coefficient of Determination represent the strength of the relationship between your model and the dependent variable.Though R-squared represent the strength of relationship between model and the dependent variables, it is never used for comparing the models as the value of R increases with the increase in the number of predictors (even if these predictors do not add any value to the model)Now only remaining metric is Adjusted R-squared. Unlike R-squared, Adjusted R-squared measures variation explained by only the independent variables that actually affect the dependent variable.So the Adjusted R-squre score will increase only if addition of predictors improve the models performance significantly or else it will decrease. Hence correct answer is Adjusted R-squared"
Technical,ML,How do Random Forest handle missing data,"Random Forests inherently have two primary ways of handling missing data:During Training (Building the Trees):For Numerical Features: Missing values can be imputed using simple strategies like mean or median.For Categorical Features: A new ""missing"" category is often created to handle missing values. This ensures that data points with missing categorical values are still considered during the tree building process.During Prediction (Making New Predictions):""Surrogate"" Splits: Each tree in the forest stores ""surrogate"" splits along with the primary split at each node. Surrogate splits are based on other features that are highly correlated with the primary split feature. If a new data point encounters a missing value during prediction, the tree will use the surrogate split to guide the data point down the appropriate branch.Proximity Measures: Random Forests also calculate ""proximity measures"" between data points based on how often they end up in the same leaf nodes across all the trees. These proximities can be used to impute missing values by taking a weighted average of the values from similar data points."
Technical,ML,What is model overfitting? How can you avoid it,"Overfitting occurs when your model learns too much from training data and isn't able to generalize the underlying information. When this happens, the model is able to describe training data very accurately but loses precision on every dataset it has not been trained on. Below images represent the overfitting linear and logistic regression models. How To Avoid Overfitting?Since overfitting algorithm captures the noise in data, reducing the number of features will help. We can manually select only important features or can use model selection algorithm for sameWe can also use the Regularization technique. It works well when we have lots of slightly useful features. Sklearn linear model(Ridge and LASSO) uses regularization parameter alpha to control the size of the coefficients by imposing a penalty.K-fold cross validation. In this technique we divide the training data in multiple batches and use each batch for training and testing the model.Increasing the training data also helps to avoid overfitting."
Technical,Data Structures & Algorithms,There are 9 balls out of which one ball is heavy in weight and rest are of the same weight. In how many minimum weightings will you find the heavier bal,"To find the heavier ball among 9 balls using a balance scale, you can determine the minimum number of weighings required by strategically dividing the balls and comparing their weights.Step-by-Step Solution:First Weighing:Divide the 9 balls into three groups of 3 balls each: Group A, Group B, and Group C.Weigh Group A against Group B.Analyzing the First Weighing:Case 1: If the scales balance (i.e., Group A = Group B), it means the heavier ball is in Group C.Case 2: If the scales do not balance (i.e., Group A ? Group B), the heavier ball is in the group that tips the scale.Second Weighing:You now have 3 balls (either all from Group C in Case 1, or from the heavier group in Case 2).Take 2 of these 3 balls and weigh them against each other.Analyzing the Second Weighing:Case 1: If the scales balance, the heavier ball is the one that was not weighed.Case 2: If the scales do not balance, the heavier ball is the one that tips the scale.Conclusion:The minimum number of weighings required to find the heavier ball among the 9 balls is 2.By dividing the balls into three groups and strategically using the balance scale, you can ensure that you find the heavier ball in just two weighings."
Technical,ML,What are feature selection methods to select right variables?,"Feature selection is the process of reducing the number of input variables when developing a predictive model. There are two methods for feature selection. Filter method and wrapper methods. Best analogy for selecting features is bad data in bad answers out.Filter MethodsFilter feature selection methods use statistical techniques to evaluate the relationship between each input variable and the target variable, and these scores are used as the basis to choose (filter) those input variables that will be used in the model.These methods are faster and less computationally expensive than wrapper methods.Information GainInformation gain calculates the reduction in entropy from the transformation of a dataset. It can be used for feature selection by evaluating the Information gain of each variable in the context of the target variable.Chi-square TestThe Chi-square test is used for categorical features in a dataset. We calculate Chi-square between each feature and the target and select the desired number of features with the best Chi-square scores.Correlation CoefficientCorrelation is a measure of the linear relationship of 2 or more variables. Through correlation, we can predict one variable from the other. The logic behind using correlation for feature selection is that the good variables are highly correlated with the target. Furthermore, variables should be correlated with the target but should be uncorrelated among themselves.Wrapper MethodsWrapper feature selection methods create many models with different subsets of input features and select those features that result in the best performing model according to a performance metric.These methods are unconcerned with the variable types, although they can be computationally expensive.The wrapper methods usually result in better predictive accuracy than filter methods.Forward Feature SelectionThis is an iterative method wherein we start with the best performing variable against the target. Next, we select another variable that gives the best performance in combination with the first selected variable. This process continues until the preset criterion is achieved.Backward Feature EliminationThis method works exactly opposite to the Forward Feature Selection method. Here, we start with all the features available and build a model. Next, we remove the variable from the model which gives the best evaluation measure value. This process is continued until the preset criterion is achieved.Exhaustive Feature SelectionThis is the most robust feature selection method covered so far. This is a brute-force evaluation of each feature subset. This means that it tries every possible combination of the variables and returns the best performing subset."
Technical,Data Structures & Algorithms,"Write a program that prints the numbers from 1 to 50. But for multiples of three print ""Fizz"" instaed of the number and for the multiples of five print ""Buzz"". For the numbers which are multiples of both three and five print ""FizzBuzz""","# The continue statement rejects all the remaining statements in the current iteration of the loop and # moves the control back to the top of the loop.for i in range(1, 51): if (i%3 == 0 and i%5 == 0): print(""FizzBuzz"") continue if i%3 == 0: print(""Fizz"") continue if i%5 == 0: print(""Buzz"") continue print(i)"
Technical,ML,You are given a dataset consisting of variables having more than 30% missing values? How will you deal with them?,"There are multiple ways to handle missing values in the dataIf dataset is huge we can simply remove the rows containing the missing dataIf dataset is small then we have to impute the missing values. There are multiple ways to impute the missing values. In case of categorical data we may use the most common values and in case numerical data we can use mean, median etc."
Technical,ML,How should you maintain your deployed model?,MonitorConstant monitoring of all the models is needed to determine the performance accuracy of the modelsEvaluateEvaluation metric of the current model is calculated to determine if new algorithm is needed.CompareThe new models are compared against each other to determine which model performs the best.RebuildThe best performing model is re-built on the current set of data.
Technical,ML,How can you select K for K-Means,"There are two ways to select the number of clusters in case K-Means clustering algorithmVisualizationTo find the number of clusters manually by data visualization is one of the most common method.Domain knowledge and proper understanding of given data also help to make more informed decisions.Since its manual exercise there is always a scope for ambiguous observations, in such cases we can also use Elbow MethodElbow MethodIn Elbow method we run the K-Means algorithm multiple times over a loop, with an increasing number of cluster choice(say from 1 to 10) and then plotting a clustering score as a function of the number of clusters.Clustering score is nothing but sum of squared distances of samples to their closest cluster center.Elbow is the point on the plot where clustering score (distortion) slows down, and the value of cluster at that point gives us the optimum number of clusters to have.But sometimes we dont get clear elbow point on the plot, in such cases its very hard to finalize the number of clusters."
Technical,ML,"Explain dimensionality reduction, and its benefits",Dimensionality reduction referes to the process of converting a set of data having vast dimensions into data with lesser dimensions(features) to convey similar information concisely.It helps in data compressing and reducing the storage spaceIt reduces computation time as less dimensions lead to less computingIt removes redundant features. E.g. There is no point in storing value in two different units
Technical,Statistics & Probability,How can you say that the time series data is stationary,"For accurate analysis and forecasting, trend and seasonality is removed from the time series and converted it into stationary series. Time series data is said to be stationary when statistical properties like mean, standard deviation are constant and there is no seasonality. In other words statistical properties of the time series data should not be a function of time."
Technical,SQL,"Write a SQL query to list all orders with customer information having Order table containing orderId, Customerid, orderNumber, TotalAmount and Customer table having id, FirstNname, LastName, City, Country","SELECT orderNumber, TotalAmount, FirstName, LastName, City, Country FROM Order JOIN customer ON orde.CustomerID = Customer.id"
Technical,ML,"After studying the behaviour of population, you have identified four specific individual types who are valueable to your study. You would like find all users who are most similar to each indivdual type. Which algorithm is most approprate for this study?","The most appropriate algorithm for this study is K-Nearest Neighbors (KNN).Here's why KNN is well-suited for this task:Similarity-Based: KNN explicitly focuses on finding the most similar data points (users in this case) to a given point (your identified individual types) based on their features or attributes.No Assumption about Data Distribution: KNN is a non-parametric algorithm, meaning it doesn't make any assumptions about the underlying distribution of your data. This is beneficial when you're unsure how the data is distributed or if it doesn't fit a specific statistical model.Easy to Interpret: KNN is relatively easy to understand and interpret. You simply define the number of neighbors (k) and a distance metric (e.g., Euclidean distance), and the algorithm finds the k closest data points to each of your individual types.Flexible: KNN can handle both numerical and categorical data, making it adaptable to different types of features that might be relevant in your study."
Technical,Statistics & Probability,Your organization has a website where visitors randomly receive one of the two coupons. It is also possible that visitors to the website will not receive the coupon. You have been asked to determine if offering a coupon to the visitors to your website has any impact on their purchase decision. Which analysis method should you use,"In this scenario, the most appropriate analysis method would be A/B Testing (or Split Testing).Here's why A/B Testing is the best fit:Controlled Experiment: A/B Testing allows you to randomly assign visitors to different groups (control group with no coupon, group A with coupon type 1, group B with coupon type 2). This controlled experiment ensures that any differences in purchase behavior can be directly attributed to the presence and type of coupon.Measures Impact: You can directly compare the conversion rates (percentage of visitors who make a purchase) between the groups to see if offering a coupon has a statistically significant impact on purchase decisions.Compares Coupon Types: Additionally, you can analyze if one coupon type is more effective than the other in driving purchases.How to Implement A/B Testing:Define the Hypothesis: State clearly what you want to test. For example:Null Hypothesis: Offering a coupon has no impact on the purchase decision.Alternative Hypothesis: Offering a coupon increases the likelihood of a purchase.Set up the Experiment:Randomly assign website visitors to one of the three groups (control, coupon A, coupon B).Ensure the experiment runs for a sufficient duration to collect enough data for statistically significant results.Collect Data:Track the conversion rates for each group.Gather additional data like coupon usage, time spent on site, etc., for further analysis.Analyze the Results:Use statistical tests (e.g., Chi-squared test, t-test) to compare the conversion rates between the groups and determine if the differences are statistically significant.Calculate metrics like uplift (increase in conversion rate due to the coupon) and confidence intervals to understand the impact of the coupons.Draw Conclusions and Take Action:Based on the analysis, decide if offering coupons is beneficial.If so, identify the most effective coupon type and implement it in your marketing strategy.Important Considerations:Sample Size: Ensure you have a large enough sample size in each group to achieve statistically significant results.External Factors: Be mindful of external factors (e.g., seasonality, promotions by competitors) that could influence purchase behavior during the experiment.Ethical Considerations: Be transparent with visitors about the experiment and obtain necessary consent if required.A/B Testing provides a robust and data-driven way to understand the impact of coupons on purchase decisions, enabling you to make informed marketing decisions and optimize your website's conversion rate."
Technical,ML,Explain Principal Componenet Analysis,"Principal Component Analysis (PCA) is dimensionality reduction method, that is used to reduce dimensionality of large data sets, by transforming large set of variables into a smaller one that still contains most of the information in large set.Principal component analysis is a technique for feature extraction so it combines our input variables in a specific way, then we can drop the least important variables while still retaining the most valuable parts of all of the variables! As an added benefit, each of the new variables after PCA are all independent of one anotherReducing the number of the variables of the datset naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity.By reducing the dimension of your feature space, you have fewer relationships between variables to consider and you are less likely to overfit your modelWhen should I use PCA?Do you want to reduce the number of variables, but arent able to identify variables to completely remove from consideration?Do you want to ensure your variables are independent of one another?Are you comfortable making your independent variables less interpretable?"
Technical,ML,Explain feature scaling,"Feature scaling is one of the most important data preprocessing step in machine learningIf we are changing the range of the features then its called 'scaling' and if we are changing the distribution of the features then its called 'normalization/standardization'ScalingThis means that you're transforming your data so that it fits within a specific scale, like 0-100 or 0-1. By scaling your variables, you can help compare different variables on equal footing.Scaling is required in case of distance based algorithms like support vector machines (SVM) or k-nearest neighbors (KNN).For example, you might be looking at the prices of some products in both Yen and US Dollars. One US Dollar is worth about 100 Yen, but if you don't scale your prices, methods like SVM or KNN will consider a difference in price of 1 Yen as important as a difference of 1 US Dollar! This clearly doesn't fit with our intuitions of the world. With currency, you can convert between currencies.But what about if you're looking at something like height and weight? It's not entirely clear how many pounds should equal one inch (or how many kilograms should equal one meter). For example, if weight ranges from 50 to 100 kilograms and height ranges from 150 to 200 centimeters, the model might give more importance to the weight feature simply because its range is wider. Without scaling, a model might consider a difference of 10 kilograms as equally significant as a difference of 10 centimeters, even though these units measure very different things. This could lead to misleading results because the model's decisions are based on the scale rather than the underlying relationships."
Technical,ML,What is meant by Data Leakage,"Data Leakage is the scenario where the Machine Learning Model is already aware of some part of test data after training.This causes the problem of overfitting.In Machine learning, Data Leakage refers to a mistake that is made by the creator of a machine learning model in which they accidentally share the information between the test and training data sets.Data leakage is a serious and widespread problem in data mining and machine learning which needs to be handled well to obtain a robust and generalized predictive model.Examples of data leakageThe most obvious and easy-to-understand cause of data leakage is to include the target variable as a feature. What happens is that after including the target variable as a feature, our purpose of prediction got destroyed. This is likely to be done by mistake but while modelling any ML model, you have to make sure that the target variable is differentiated from the set of features.Another common cause of data leakage is to include test data with training data.Above two cases are not very likely to occur because they can easily be spotted while doing the modelling. Below are few data leakage examples that are hard to troubleshoot.Presence of Giveaway featuresLets we are working on a problem statement in which we have to build a model that predicts a certain medical condition. If we have a feature that indicates whether a patient had a surgery related to that medical condition, then it causes data leakage and we should never be included that as a feature in the training data. The indication of surgery is highly predictive of the medical condition and would probably not be available in all cases. If we already know that a patient had a surgery related to a medical condition, then we may not even require a predictive model to start with.Lets we are working on a problem statement in which we have to build a model that predicts if a user will stay on a website. Including features that expose the information about future visits will cause the problem of data leakage. So, we have to use only features about the current session because information about the future sessions is not generally available after we deployed our model.Leakage during Data preprocessingWhile solving a Machine learning problem statement, firstly we do the data cleaning and preprocessing which involves the following steps:Evaluating the parameters for normalizing or rescaling featuresFinding the minimum and maximum values of a particular featureNormalize the particular feature in our datasetRemoving the outliersFill or completely remove the missing data in our datasetThe above-described steps should be done using only the training set. If we use the entire dataset to perform these operations, data leakage may occur.Applying preprocessing techniques to the entire dataset will cause the model to learn not only the training set but also the test set. As we all know that the test set should be new and previously unseen for any model."
Technical,ML,How to detect Data Leakage,"Results are too good too trueIn general, if we see that the model which we build is too good to be true (i.,e gives predicted and actual output the same), then we should get suspicious and data leakage cannot be ruled out.At that time, the model might be somehow memorizing the relations between feature and target instead of learning and generalizing it for the unseen data.So, it is advised that before the testing, the prior documented results are weighed against the expected results.Using EDAWhile doing the Exploratory Data Analysis (EDA), we may detect features that are very highly correlated with the target variable. Of course, some features are more correlated than others but a surprisingly high correlation needs to be checked and handled carefully.We should pay close attention to those features. So, with the help of EDA, we can examine the raw data through statistical and visualization tools.High weight featuresAfter the completion of the model training, if features are having very high weights, then we should pay close attention. Those features might be leaky."
Technical,ML,How to fix the problem of Data Leakage,"The main culprit behind this is the way we split our dataset and when. The following steps can prove to be very crucial in preventing data leakage:Select the features such a way that they do not contain information about the target variable, which is not naturally available at the time of prediction.Create a Separate Validation SetTo minimize or avoid the problem of data leakage, we should try to set aside a validation set in addition to training and test sets if possible.The purpose of the validation set is to mimic the real-life scenario and can be used as a final step.By doing this type of activity, we will identify if there is any possible case of overfitting which in turn can act as a caution warning against deploying models that are expected to underperform in the production environment.Apply Data preprocessing Separately to both Train and Test subsetsWhile dealing with neural networks, it is a common practice that we normalize our input data firstly before feeding it into the model.Generally, data normalization is done by dividing the data by its mean value. More often than not, this normalization is applied to the overall data set, which influences the training set from the information of the test set and eventually it results in data leakage.Hence, to avoid data leakage, we have to apply any normalization technique separately to both training and test subsets.Problem with the Time-Series Type of dataWhen dealing with time-series data, we should pay more attention to data leakage. For example, if we somehow use data from the future when doing computations for current features or predictions, it is highly likely to end up with a leaked model.It generally happens when the data is randomly split into train and test subsets.So, when working with time-series data, we put a cutoff value on time which might be very useful, as it prevents us from getting any information after the time of prediction.Target Leakage: This happens when predictors (features) include information that's only available after the target variable is known.Example: Predicting customer churn based on whether they canceled their subscription. The act of canceling is only known after they've churned, making it a leaky predictor."
Technical,Statistics & Probability,What is selection bias,"Selection bias is the bias introduced by the selection of individuals, groups, or data for analysis in such a way that proper randomization is not achieved, thereby ensuring that the sample obtained is not representative of the population intended to be analyzed. It is sometimes referred to as the selection effect.Sampling bias is usually classified as a subtype of selection bias, sampling bias is a bias in which a sample is collected in such a way that some members of the intended population have a lower or higher sampling probability than others.Due to sampling bias, the probability distribution in the collected dataset deviates from its true natural distribution, which may affect ML models performance."
Technical,Statistics & Probability,What does it mean when distribution is left skew or right skew,"In a right-skewed distribution, the tail on the right side is longer. This means most of the data is clustered on the left, with a few unusually large values pulling the average higher. Think of income distribution - most people earn less, but a few very high earners skew the average upwards.In a left-skewed distribution, the tail on the left side is longer. This means most of the data is clustered on the right, with a few unusually small values pulling the average lower. An example could be exam scores where most students do well, but a few low scores bring down the average."
Technical,ML,What is regularization. Why it is usefull,Regularization is the process of adding tunning parameter(penalty term) to a model to induce smoothness in order to prevent overfitting.The tunning parameter controls the excessively fluctuating function in such a way that coefficients dont take extreame values.There are two types of regularization as follows:L1 Regularization or Lasso Regularization. L1 Regularization or Lasso Regularization adds a penalty to the error function. The penalty is the sum of the absolute values of weights.L2 Regularization or Ridge Regularization. L2 Regularization or Ridge Regularization also adds a penalty to the error function. But the penalty here is the sum of the squared values of weights.
Technical,ML,Explain the scenario where both false positive and false negative are equally important,"Medical Diagnosis (e.g., Cancer Screening)False Positive: A patient is told they have cancer when they don't. This leads to unnecessary anxiety, invasive procedures, and potential side effects from treatment.False Negative: A patient with cancer is told they are healthy. This delays crucial treatment, potentially allowing the disease to progress and worsen the prognosis.Fraud DetectionFalse Positive: A legitimate transaction is flagged as fraudulent. This inconveniences the customer, potentially disrupting their business or causing reputational damage.False Negative: A fraudulent transaction goes undetected. This results in financial loss for the business or individual, and can enable further fraudulent activity."
Technical,ML,Why feature scalling is required in Gradient Descent Based Algorithms,"Machine learning algorithms like linear regression, logistic regression, neural network, etc. that use gradient descent as an optimization technique require data to be scaled. The presence of feature value X in the formula will affect the step size of the gradient descent.The difference in ranges of features will cause different step sizes for each feature.To ensure that the gradient descent moves smoothly towards the minima and that the steps for gradient descent are updated at the same rate for all the features, we scale the data before feeding it to the model.Having features on a similar scale can help the gradient descent converge more quickly towards the minima."
Technical,ML,Why feature scaling not required in tree based algorithms,"Imagine you're sorting a pile of apples and oranges into two baskets. You could sort them by color (red vs. not red) or by weight (heavy vs. light).Tree-based algorithms work like this: They make decisions based on thresholds or cut-offs for each feature (like color or weight). They ask questions like: ""Is this fruit red?"" or ""Is this fruit heavier than 1 pound?"".Feature scaling doesn't matter here:Color: It doesn't matter if we represent ""red"" as the number 1 and ""not red"" as 0, or if we use some other scale. The decision (is it red or not?) remains the same.Weight: Even if we change the units from pounds to kilograms, the relative heaviness of the fruits doesn't change. A heavy apple will still be heavier than a light orange, regardless of the units.In simpler terms:Tree-based algorithms care about the order or ranking of the data, not the exact numerical values.Scaling the features changes the numbers but doesn't change their order. A heavy fruit stays heavy, and a light fruit stays light, no matter what units we use.This makes tree-based algorithms invariant to monotonic transformations of the features (transformations that preserve the order).Therefore, feature scaling is generally not required for tree-based algorithms.Exceptions:Some implementations: Certain libraries or specific algorithms within the tree-based family might be sensitive to the scale of the features due to implementation details. It's always good to check the documentation or experiment to be sure.Distance-based calculations: If your tree-based algorithm involves any calculations based on distances between data points, then scaling might be necessary to ensure all features contribute equally to the distance calculation."
Technical,ML,What are the risks associated with Data Science & how MLOps can overcome the same?,"In Data Science, several risks can impact projects, such as data quality issues, model deployment challenges, model performance degradation, lack of reproducibility, security concerns, and scalability issues.MLOps (Machine Learning Operations) helps mitigate these risks by:Automating Data Quality Checks and Monitoring: Ensures consistent data quality and detects data drift, which helps maintain model accuracy over time.Streamlining Model Deployment and Environment Consistency: Uses automated pipelines and containerization to deploy models smoothly across different environments, reducing errors and ensuring reliability.Continuous Monitoring and Automated Retraining: Tracks model performance in real-time and triggers retraining when necessary, preventing performance degradation and adapting to new data patterns.Improving Reproducibility and Collaboration: Incorporates version control for code, data, and models, making it easier to reproduce results and collaborate across teams.Enhancing Security and Compliance: Implements strict access controls, auditing, and encryption to protect data and models, ensuring compliance with regulations.Optimizing Scalability and Resource Management: Supports scalable infrastructure and provides insights into cost and resource use, ensuring models can handle large volumes and operate efficiently.By implementing MLOps, we can address these risks effectively, leading to more robust, reliable, and scalable machine-learning models."
Technical,Statistics & Probability,Please explain p-value to someone non-technical,"A p-value is a number that helps us understand if the results we see in an experiment or study are meaningful or if they might have happened just by chance.Imagine you're playing a game of chance, like flipping a coin. You suspect the coin might be rigged to land on heads more often, so you decide to test it.The ""normal"" assumption (null hypothesis): The coin is fair, and there's a 50/50 chance of getting heads or tails.Your experiment: You flip the coin 100 times and get 60 heads. Hmm, that seems a bit high...The p-value comes in: It tells you, ""If the coin was fair, how likely is it that you'd get a result as extreme (or more extreme) as 60 heads out of 100 flips, just by random chance?""A small p-value (e.g., 0.01) means it's very unlikely to get such a result with a fair coin. This makes you suspicious maybe the coin is rigged!A large p-value (e.g., 0.50) means it's quite likely to get such a result even with a fair coin. So, you don't have strong evidence to say the coin is rigged.In simpler terms:The p-value is like a ""surprise meter"". The lower the p-value, the more surprised you'd be to see your results if the ""normal"" assumption were true.It helps you decide whether your results are strong enough to challenge the ""normal"" assumption or if they could just be due to random luck.Important Note:The p-value doesn't prove anything. It just gives you a measure of how surprising your results are.It's up to you to decide what level of ""surprise"" is enough to make you doubt the ""normal"" assumption.If the p-value is less than or equal to alpha(0.05), it means your results are statistically significant, and you have enough evidence to reject the null hypothesis. In other words, your data suggests that the ""normal"" assumption is probably not true."
Technical,Case Study,"Average comments per month has dropped over three-month period, despite consistent growth after a new launch. What metric would u investigate?","To investigate the drop in average comments per month despite consistent growth after a new launch, I would examine:Engagement per User: Assess if the average number of comments per active user has decreased. Even with user growth, fewer comments per user could explain the overall decline.Content Type Analysis: Identify if the types of content driving growth are different from those that typically generate comments. Growth could be from content that attracts more views or reads, but not comments.New vs. Returning User Behavior: Check if new users, who might contribute to growth, are engaging less in commenting than returning users. This could indicate that while more people are joining, they aren't commenting as much.User Experience and UI Changes: Review if changes in the user interface or user experience since the launch might have unintentionally made it harder or less appealing to comment.Sentiment and Feedback: Analyze user feedback for any negative sentiment or issues related to commenting since the new growth phase.By focusing on these areas, we can identify the specific reasons behind the decline in comments despite overall platform growth."
Technical,Case Study,A PM tells you that a weekly active user metric is up by 5% but email notification open rate is down by 2%. WHat would you investigate to dignose this problem?,"Email open rate is calculated by dividing the number of emails opened by the number of emails sent minus any bounces. A good open rate is between 17-28%2. Email notification open rate is a type of email open rate that measures how many users open an email that notifies them about something.Weekly active user metric (WAU) is a measure of how many users are active on a website or app in a given week. It can be influenced by many factors, such as user acquisition, retention, engagement and churn.To diagnose the problem of WAU being up but email notification open rate being down, you might want to investigate:How are you defining active users? Are they performing meaningful actions on your website or app that indicate engagement and loyalty?How are you segmenting your users based on their behavior, preferences and needs? Are you sending relevant and personalized email notifications to each segment?How are you optimizing your email subject lines, preheaders, sender names and content to capture attention and interest? Are you using clear and compelling calls to action?How are you testing and measuring your email performance? Are you using tools like A/B testing, analytics and feedback surveys to improve your email strategy?"
Technical,ML,Explain data drift problem in machine learning,"Data drift is a common problem in machine learning that occurs when the statistical properties of the input data change over time. This change can lead to a decrease in the performance of machine learning models because the model is no longer receiving the same kind of data it was trained on.Key Points to Explain Data Drift:Definition of Data Drift:Data drift refers to any change in the distribution of data that a machine learning model was trained on compared to the data it encounters in production. This can cause the model to make inaccurate predictions or classifications because it relies on patterns that may no longer be relevant.Types of Data Drift:Covariate Drift: This occurs when the distribution of the input features (independent variables) changes. For example, if a model is predicting sales based on weather data, and there is a shift in climate patterns, the relationship between weather and sales could change.Prior Probability Shift: This type of drift happens when the distribution of the target variable (dependent variable) changes. For instance, in a spam detection model, if the proportion of spam emails decreases over time, the model might need adjustment.Concept Drift: This happens when the relationship between the input features and the target variable changes. For example, a model predicting credit card fraud may experience concept drift if fraudsters change their tactics over time.Why Data Drift is a Problem:Decreased Model Accuracy: Models trained on historical data may perform poorly on new data that has drifted. This can lead to incorrect predictions and reduced effectiveness of the model.Impact on Business Decisions: In many applications, such as finance, healthcare, and marketing, decisions based on outdated models can lead to significant financial loss, poor customer experiences, or even safety issues.Detecting Data Drift:Statistical Tests: Techniques such as Kolmogorov-Smirnov test, Chi-square test, or Jensen-Shannon divergence can be used to detect changes in data distributions.Monitoring Performance Metrics: Regularly tracking model performance metrics (like accuracy, precision, recall) over time can indicate data drift if theres a consistent decline.Data Visualization: Visual methods such as histograms, scatter plots, or density plots can help visualize changes in data distributions over time.Mitigating Data Drift:Regular Model Retraining: Regularly retraining models with new data helps to ensure they stay up-to-date with the latest trends and patterns.Continuous Monitoring: Implementing systems to continuously monitor incoming data and model performance allows for early detection of drift.Adaptive Learning: Techniques like online learning or incremental learning can help models adapt to changes by learning from new data as it becomes available.Feature Engineering Updates: Revisiting and potentially redefining features to align with the new data distributions can help in maintaining model accuracy.Real-World Example of Data Drift:Imagine a retail company that uses a machine learning model to predict customer preferences and recommend products. The model was trained using data collected over the past year. However, due to a sudden change in customer behavior (perhaps driven by a new trend or economic conditions), the products customers are interested in change. This change leads to data drift because the model's predictions are based on outdated information about customer preferences, causing it to recommend less relevant products and reducing the effectiveness of marketing campaigns.Conclusion:Data drift is a critical issue in maintaining the accuracy and reliability of machine learning models. Regular monitoring, retraining, and adaptation strategies are essential to ensure models remain effective as real-world conditions change."
Technical,ML,You are given a train data set having 1000 columns and 1 million rows. The data set is based on a classification problem. Your manager has asked you to reduce the dimension of this data so that model computation time can be reduced. Your machine has memory constraints. What would you do? (You are free to make practical assumptions.),"Processing a high dimensional data on a limited memory machine is a strenuous task, your interviewer would be fully aware of that. Following are the methods you can use to tackle such situation: Since we have lower RAM, we should close all other applications in our machine, including the web browser, so that most of the memory can be put to use.We can randomly sample the data set. This means, we can create a smaller data set, lets say, having 1000 variables and 300000 rows and do the computations.To reduce dimensionality, we can separate the numerical and categorical variables and remove the correlated variables. For numerical variables, well use correlation. For categorical variables, well use chi-square test.Also, we can use PCA and pick the components which can explain the maximum variance in the data set.Using online learning algorithms like Vowpal Wabbit (available in Python) is a possible option.Building a linear model using Stochastic Gradient Descent is also helpful.We can also apply our business understanding to estimate which all predictors can impact the response variable. But, this is an intuitive approach, failing to identify useful predictors might result in significant loss of information."
Technical,ML,"Is rotation necessary in PCA? If yes, Why? What will happen if you dont rotate the components?","Yes, rotation (orthogonal) is necessary because it maximizes the difference between variance captured by the component. This makes the components easier to interpret. Not to forget, thats the motive of doing PCA where, we aim to select fewer components (than features) which can explain the maximum variance in the data set. By doing rotation, the relative location of the components doesnt change, it only changes the actual coordinates of the points.If we dont rotate the components, the effect of PCA will diminish and well have to select more number of components to explain variance in the data set."
Technical,Statistics & Probability,You are given a data set. The data set has missing values which spread along 1 standard deviation from the median. What percentage of data would remain unaffected? Why?,"This question has enough hints for you to start thinking! Since, the data is spread across median, lets assume its a normal distribution. We know, in a normal distribution, ~68% of the data lies in 1 standard deviation from mean (or mode, median), which leaves ~32% of the data unaffected. Therefore, ~32% of the data would remain unaffected by missing values."
Technical,ML,You are given a data set on cancer detection. Youve build a classification model and achieved an accuracy of 96%. Why shouldnt you be happy with your model performance? What can you do about it?,"If you have worked on enough data sets, you should deduce that cancer detection results in imbalanced data. In an imbalanced data set, accuracy should not be used as a measure of performance because 96% (as given) might only be predicting majority class correctly, but our class of interest is minority class (4%) which is the people who actually got diagnosed with cancer. Hence, in order to evaluate model performance, we should use Sensitivity (True Positive Rate), Specificity (True Negative Rate), F measure to determine class wise performance of the classifier. If the minority class performance is found to to be poor, we can undertake the following steps:We can use undersampling, oversampling or SMOTE to make the data balanced.We can alter the prediction threshold value by doing probability caliberation and finding a optimal threshold using AUC-ROC curve.We can assign weight to classes such that the minority classes gets larger weight.We can also use anomaly detection."
Technical,Statistics & Probability,"Explain prior probability, likelihood and marginal likelihood in context of naiveBayes algorithm?","Prior probability is nothing but, the proportion of dependent (binary) variable in the data set. It is the closest guess you can make about a class, without any further information. For example: In a data set, the dependent variable is binary (1 and 0). The proportion of 1 (spam) is 70% and 0 (not spam) is 30%. Hence, we can estimate that there are 70% chances that any new email would be classified as spam.Likelihood is the probability of classifying a given observation as 1 in presence of some other variable. For example: The probability that the word FREE is used in previous spam message is likelihood. Marginal likelihood is, the probability that the word FREE is used in any message."
Technical,ML,"You are working on a time series data set. You manager has asked you to build a high accuracy model. You start with the decision tree algorithm, since you know it works fairly well on all kinds of data. Later, you tried a time series regression model and got higher accuracy than decision tree model. Can this happen? Why?","Time series data is known to posses linearity. On the other hand, a decision tree algorithm is known to work best to detect non linear interactions. The reason why decision tree failed to provide robust predictions because it couldnt map the linear relationship as good as a regression model did. Therefore, we learned that, a linear regression model can provide robust prediction given the data set satisfies its linearity assumptions."
Technical,Case Study,"You are assigned a new project which involves helping a food delivery company save more money. The problem is, companys delivery team arent able to deliver food on time. As a result, their customers get unhappy. And, to keep them happy, they end up delivering food for free. Which machine learning algorithm can save them?","You might have started hopping through the list of ML algorithms in your mind. But, wait! Such questions are asked to test your machine learning fundamentals.This is not a machine learning problem. This is a route optimization problem. A machine learning problem consist of three things:There exist a pattern.You cannot solve it mathematically (even by writing exponential equations).You have data on it."
Technical,ML,You came to know that your model is suffering from low bias and high variance. Which algorithm should you use to tackle it? Why?,"Low bias occurs when the models predicted values are near to actual values. In other words, the model becomes flexible enough to mimic the training data distribution. While it sounds like great achievement, but not to forget, a flexible model has no generalization capabilities. It means, when this model is tested on an unseen data, it gives disappointing results.In such situations, we can use bagging algorithm (like random forest) to tackle high variance problem. Bagging algorithms divides a data set into subsets made with repeated randomized sampling. Then, these samples are used to generate a set of models using a single learning algorithm. Later, the model predictions are combined using voting (classification) or averaging (regression).Also, to combat high variance, we can:Use regularization technique, where higher model coefficients get penalized, hence lowering model complexity.Use top n features from variable importance chart. May be, with all the variable in the data set, the algorithm is having difficulty in finding the meaningful signal."
Technical,ML,"You are given a data set. The data set contains many variables, some of which are highly correlated and you know about it. Your manager has asked you to run PCA. Would you remove correlated variables first? Why?","Chances are, you might be tempted to say No, but that would be incorrect. Discarding correlated variables have a substantial effect on PCA because, in presence of correlated variables, the variance explained by a particular component gets inflated.For example: You have 3 variables in a data set, of which 2 are correlated. If you run PCA on this data set, the first principal component would exhibit twice the variance than it would exhibit with uncorrelated variables. Also, adding correlated variables lets PCA put more importance on those variable, which is misleading."
Technical,ML,"After spending several hours, you are now anxious to build a high accuracy model. As a result, you build 5 GBM models, thinking a boosting algorithm would do the magic. Unfortunately, neither of models could perform better than benchmark score. Finally, you decided to combine those models. Though, ensembled models are known to return high accuracy, but you are unfortunate. Where did you miss?","As we know, ensemble learners are based on the idea of combining weak learners to create strong learners. But, these learners provide superior result when the combined models are uncorrelated. Since, we have used 5 GBM models and got no accuracy improvement, suggests that the models are correlated. The problem with correlated models is, all the models provide same information.For example: If model 1 has classified User1122 as 1, there are high chances model 2 and model 3 would have done the same, even if its actual value is 0. Therefore, ensemble learners are built on the premise of combining weak uncorrelated models to obtain better predictions."
Technical,ML,How is kNN different from kmeans clustering?,"Don't get mislead by in their names. You should know that the fundamental difference between both these algorithms is, kmeans is unsupervised in nature and kNN is supervised in nature. kmeans is a clustering algorithm. kNN is a classification (or regression) algorithm.kmeans algorithm partitions a data set into clusters such that a cluster formed is homogeneous and the points in each cluster are close to each other. The algorithm tries to maintain enough separability between these clusters. Due to unsupervised nature, the clusters have no labels.kNN algorithm tries to classify an unlabeled observation based on its k (can be any number ) surrounding neighbors. It is also known as lazy learner because it involves minimal training of model. Hence, it doesnt use training data to make generalization on unseen data set."
Technical,ML,"You have built a multiple regression model. Your model R isnt as good as you wanted. For improvement, your remove the intercept term, your model R becomes 0.8 from 0.3. Is it possible? How?","Yes, it is possible. We need to understand the significance of intercept term in a regression model. The intercept term shows model prediction without any independent variable i.e. mean prediction. The formula of R = 1 ?(y y)/?(y ymean) where y is predicted value.When intercept term is present, R value evaluates your model wrt. to the mean model. In absence of intercept term (ymean), the model can make no such evaluation, with large denominator, ?(y - y)/?(y) equations value becomes smaller than actual, resulting in higher R."
Technical,ML,"After analyzing the model, your manager has informed that your regression model is suffering from multicollinearity. How would you check if hes true? Without losing any information, can you still build a better model?","To check multicollinearity, we can create a correlation matrix to identify & remove variables having correlation above 75% (deciding a threshold is subjective). In addition, we can use calculate VIF (variance inflation factor) to check the presence of multicollinearity. VIF value <= 4 suggests no multicollinearity whereas a value of >= 10 implies serious multicollinearity. Also, we can use tolerance as an indicator of multicollinearity.But, removing correlated variables might lead to loss of information. In order to retain those variables, we can use penalized regression models like ridge or lasso regression. Also, we can add some random noise in correlated variable so that the variables become different from each other. But, adding noise might affect the prediction accuracy, hence this approach should be carefully used."
Technical,Statistics & Probability,Rise in global average temperature led to decrease in number of pirates around the world. Does that mean that decrease in number of pirates caused the climate change?,"After reading this question, you should have understood that this is a classic case of causation and correlation. No, we cant conclude that decrease in number of pirates caused the climate change because there might be other factors (lurking or confounding variables) influencing this phenomenon.Therefore, there might be a correlation between global average temperature and number of pirates, but based on this information we cant say that pirated died because of rise in global average temperature."
Technical,ML,"While working on a data set, how do you select important variables? Explain your methods.","Following are the methods of variable selection you can use:Remove the correlated variables prior to selecting important variablesUse linear regression and select variables based on p valuesUse Forward Selection, Backward Selection, Stepwise SelectionUse Random Forest, Xgboost and plot variable importance chartUse Lasso RegressionMeasure information gain for the available set of features and select top n features accordingly."
Technical,ML,"Both being tree based algorithm, how is random forest different from Gradient boosting algorithm (GBM)?","The fundamental difference is, random forest uses bagging technique to make predictions. GBM uses boosting techniques to make predictions.In bagging technique, a data set is divided into n samples using randomized sampling. Then, using a single learning algorithm a model is build on all samples. Later, the resultant predictions are combined using voting or averaging. Bagging is done is parallel. In boosting, after the first round of predictions, the algorithm weighs misclassified predictions higher, such that they can be corrected in the succeeding round. This sequential process of giving higher weights to misclassified predictions continue until a stopping criterion is reached.Random forest improves model accuracy by reducing variance (mainly). The trees grown are uncorrelated to maximize the decrease in variance. On the other hand, GBM improves accuracy my reducing both bias and variance in a model."
Technical,ML,Running a binary classification tree algorithm is the easy part. Do you know how does a tree splitting takes place i.e. how does the tree decide which variable to split at the root node and succeeding nodes?,"A classification trees makes decision based on Gini Index and Node Entropy. In simple words, the tree algorithm find the best possible feature which can divide the data set into purest possible children nodes.Gini index says, if we select two items from a population at random then they must be of same class and probability for this is 1 if population is pure. We can calculate Gini as following:Calculate Gini for sub-nodes, using formula sum of square of probability for success and failure (p^2+q^2).Calculate Gini for split using weighted Gini score of each node of that splitEntropy is the measure of impurity as given by (for binary class):Entropy, Decision TreeHere p and q is probability of success and failure respectively in that node. Entropy is zero when a node is homogeneous. It is maximum when a both the classes are present in a node at 50% 50%. Lower entropy is desirable."
Technical,ML,"Youve built a random forest model with 10000 trees. You got delighted after getting training error as 0.00. But, the validation error is 34.23. What is going on? Havent you trained your model perfectly?","The model has overfitted. Training error 0.00 means the classifier has mimiced the training data patterns to an extent, that they are not available in the unseen data. Hence, when this classifier was run on unseen sample, it couldnt find those patterns and returned prediction with higher error. In random forest, it happens when we use larger number of trees than necessary. Hence, to avoid these situation, we should tune number of trees using cross validation."
Technical,ML,Youve got a data set to work having p (no. of variable) > n (no. of observation). Why is OLS as bad option to work with? Which techniques would be best to use? Why?,"In such high dimensional data sets, we cant use classical regression techniques, since their assumptions tend to fail. When p > n, we can no longer calculate a unique least square coefficient estimate, the variances become infinite, so OLS cannot be used at all.To combat this situation, we can use penalized regression methods like lasso, LARS, ridge which can shrink the coefficients to reduce variance. Precisely, ridge regression works best in situations where the least square estimates have higher variance.Among other methods include subset regression, forward stepwise regression."
Technical,ML,"We know that one hot encoding increasing the dimensionality of a data set. But, label encoding doesnt. How ?","Using one hot encoding, the dimensionality (a.k.a features) in a data set get increased because it creates a new variable for each level present in categorical variables. For example: lets say we have a variable color. The variable has 3 levels namely Red, Blue and Green. One hot encoding color variable will generate three new variables as Color.Red, Color.Blue and Color.Green containing 0 and 1 value.In label encoding, the levels of a categorical variables gets encoded as 0 and 1, so no new variable is created. Label encoding is majorly used for binary variables."
Technical,ML,"You are given a data set consisting of variables having more than 30% missing values? Lets say, out of 50 variables, 8 variables have missing values higher than 30%. How will you deal with them?","Assign a unique category to missing values, who knows the missing values might decipher some trendWe can remove them blatantly.Or, we can sensibly check their distribution with the target variable, and if found any pattern well keep those missing values and assign them a new category while removing others."
Technical,ML,"You are working on a classification problem. For validation purposes, youve randomly sampled the training data set into train and validation. You are confident that your model will work incredibly well on unseen data since your validation accuracy is high. However, you get shocked after getting poor test accuracy. What went wrong?","In case of classification problem, we should always use stratified sampling instead of random sampling. A random sampling doesnt takes into consideration the proportion of target classes. On the contrary, stratified sampling helps to maintain the distribution of target variable in the resultant distributed samples also."
Technical,ML,"You have been asked to evaluate a regression model based on R, adjusted R and tolerance. What will be your criteria?","Tolerance (1 / VIF) is used as an indicator of multicollinearity. It is an indicator of percent of variance in a predictor which cannot be accounted by other predictors. Large values of tolerance is desirable.We will consider adjusted R as opposed to R to evaluate model fit because R increases irrespective of improvement in prediction accuracy as we add more variables. But, adjusted R would only increase if an additional variable improves the accuracy of model, otherwise stays same. It is difficult to commit a general threshold value for adjusted R because it varies between data sets. For example: a gene mutation data set might result in lower adjusted R and still provide fairly good predictions, as compared to a stock market data where lower adjusted R implies that model is not good."
Technical,ML,"In k-means or kNN, we use euclidean distance to calculate the distance between nearest neighbors. Why not manhattan distance ?","We dont use manhattan distance because it calculates distance horizontally or vertically only. It has dimension restrictions. On the other hand, euclidean metric can be used in any space to calculate distance. Since, the data points can be present in any dimension, euclidean distance is a more viable option.Example: Think of a chess board, the movement made by a bishop or a rook is calculated by manhattan distance because of their respective vertical & horizontal movements."
Technical,ML,I know that a linear regression model is generally evaluated using Adjusted R or F value. How would you evaluate a logistic regression model?,"Since logistic regression is used to predict probabilities, we can use AUC-ROC curve along with confusion matrix to determine its performance.Also, the analogous metric of adjusted R in logistic regression is AIC. AIC is the measure of fit which penalizes model for the number of model coefficients. Therefore, we always prefer model with minimum AIC value.Null Deviance indicates the response predicted by a model with nothing but an intercept. Lower the value, better the model. Residual deviance indicates the response predicted by a model on adding independent variables. Lower the value, better the model."
Technical,ML,"Considering the long list of machine learning algorithm, given a data set, how do you decide which one to use?","You should say, the choice of machine learning algorithm solely depends of the type of data. If you are given a data set which is exhibits linearity, then linear regression would be the best algorithm to use. If you given to work on images, audios, then neural network would help you to build a robust model.If the data comprises of non linear interactions, then a boosting or bagging algorithm should be the choice. If the business requirement is to build a model which can be deployed, then well use regression or a decision tree model (easy to interpret and explain) instead of black box algorithms like SVM, GBM etc.In short, there is no one master algorithm for all situations. We must be scrupulous enough to understand which algorithm to use."
Technical,ML,When does regularization becomes necessary in Machine Learning?,"Regularization becomes necessary when the model begins to ovefit / underfit. This technique introduces a cost term for bringing in more features with the objective function. Hence, it tries to push the coefficients for many variables to zero and hence reduce cost term. This helps to reduce model complexity so that the model can become better at predicting (generalizing)."
Technical,ML,OLS is to linear regression. Maximum likelihood is to logistic regression. Explain the statement.,"OLS and Maximum likelihood are the methods used by the respective regression methods to approximate the unknown parameter (coefficient) value. In simple words,Ordinary least square(OLS) is a method used in linear regression which approximates the parameters resulting in minimum distance between actual and predicted values. Maximum Likelihood helps in choosing the the values of parameters which maximizes the likelihood that the parameters are most likely to produce observed data."
Technical,ML,What's wrong with training and testing a machine learning model on the same data?,"This is one of the more common data scientist interview questions. When we are training a model, we are exposing it to the training data. This means it is learning the patterns from it. By the end of the training, it becomes very good at predicting this particular dataset. However, sometimes we may overfit. This is a situation where we keep improving the accuracy, but not because the model is good, but just because it has learned every little detail about the data it is given.If we test on that data, we will be checking the accuracy of the training. This is not a test per se. Thats simply a train accuracy check. Our model will seem to be very accurate and working properly. But that is because we trained it on that same data. We are essentially asking the model to predict what was already predicted, which is not a hard task.To truly test a model, we must expose it to data it has never seen before. This will reveal if it learned the patterns of the population, or simply the noise in the training data."
Technical,ML,How to make sure you are not overfitting while training a model?,"First, we need to clarify what overfitting is exactly. Usually, overfitting happens when your model fits the training data so well that it misses the point. In other words it doesnt look for the general patterns, but for the noise in the data provided. If that happens, when provided with new data, the model behaves disastrously in a real-life setting.Regularization - In the context of machine learning refers to the process of modifying a learning algorithm so as to make it simpler often to prevent overfitting or to solve a badly posed problem.Early stopping early stopping is the most common type of regularization. It is designed precisely to prevent overfitting. It consists of techniques that interrupt the training process, once the model starts overfitting.*Here you may be expected to say validation or cross-validation. In fact, early stopping methods always use the outputs from the validation to determine whether to stop the training process.Feature selection for some models, having useless input features leads to much worse performance. Therefore, you have to make sure to choose only the most relevant features for your problem otherwise this may affect (among other things) overfitting.Ensembles are methods to combine several base models in order to produce one optimal predictive model. A good example of the ensemble method is Random Forest (a collection of decision trees).It is very important to realize that overfitting is an extremely important issue. Every model will overfit if no preventative techniques have been implemented. Therefore, you should always aim to apply one or more of these techniques in your model building efforts."
Technical,ML,What is cross-validation? How to do it right?,"Cross-validation refers to many model validation techniques that use the same dataset for both training and validation. Usually, it is on a rotational basis so that observations are not overexposed to the training process and thus can serve as better validation. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.Why do we even need to validate?Well, when you use sample data (so most of the time), you need to make sure that your model is not overfitting the parameters.So how do we validate? We take out like 10% of the data for later use and train on the remaining 90%. Once we are done, we validate on the 10% we set aside at the beginning. This is a pretty common practice but has one major drawback - some of the data (these 10% precisely) is not really utilized in the training process.That is where cross-validation comes in.Cross-validation does the same thing as simple validation, but it first divides the dataset into equal parts (5,10,20 depending on the size of data). To cross-validate, it sets aside the first part and trains on the remaining parts. Then it sets aside the 2nd part and trains on the remaining ones (this time, including the first part). We continue in that way, utilizing a different subset for each validation. In that way, the model gets exposed to all the data in contrast to conventional validation."
Behavioral,Soft Skills,What motivates you about this position,"By asking this question, the recruiter wants to understand whether you are excited about the new opportunity that lies ahead of you. Your enthusiasm, of course, is highly correlated with the amount of effort you will put once the job is offered.A motivated person would try to be proactive and create a positive working environment, which is precisely what every company needs. The real question isnt whether you should say that you are motivated. Of course, you should. You need to think of a way that would best show that you are genuinely interested in the position under consideration. There are a lot of different things that can motivate you:The learning opportunities that you will have on the jobFuture growth prospectsYou like the team that you will be inserted in (if you have met them)You share the companys values/missionThe company operates in a dynamic, ever-changing industryThe companys prestigeOf course, remuneration is one of the main motivators for almost all people. However, talking about money is not a good idea at this point in the selection process. Instead, focus on some of the aspects that we listed above and customize them to the specific position that you are applying for.What you say while answering this question is not the only important thing. Your interviewer will be eager to see that all signs point in the same direction. Try to show that you are excited through your voice, posture and body language. This can be the critical difference that will determine whether or not you will be selected."
Behavioral,Soft Skills,Give me an example of a time when you had to go the extra mile?,"Going the extra-mile is rarely a one-time act. More often, it is an ingrained habit. You need to properly explain to your recruiter that you love the idea of working that job. Also, explain how you want to be excellent at it. Your internal drive towards excellence is what motivates you to go the extra mile to do the things that you are not expected to do:Study during the weekendsStay late in the officeStriving for excellence constantlyIf the job you are interviewing for is what you chose for your life, then you want to be excellent at it. Striving to achieve excellent performance is important. It means that you want to put quality in your work and create value for the company. Internal drive is probably the best reason to go the extra mile; you are willing to do what is necessary in order to be good at what you do.An example of such a situation:During your previous internship experience, you put in a lot of extra effort in order to show that your tutor who also recruited you did not make a mistake. You stayed late and studied during the weekend because you wanted to improve your skills and to do it faster. The positive impression that you left with your work led to an excellent valuation and very positive feedback about your willingness to learn."
Behavioral,Soft Skills,Can you tell me a time when you were able to build motivation in your co-workers?,"This question aims to assess whether you are a good leader and a positive influence at your workplace. Hiring managers look for people who are motivated themselves and are able to transmit their drive to their co-workers. Strong motivation makes for excellent results.In order to be able to motivate someone, you have to fully understand the person that you are approaching. What is it that they currently need in order to be excited about a project? Perhaps they need:One-on-one coachingInteresting tasksMore complicated tasksResponsibilityAutonomyRecognitionA positive perspectiveHere's an example of such a situation:During your previous internship within the Corporate Finance department of a large firm, you were asked to prepare a Valuation model. There was another intern who was assigned to work with you. Given that she had less experience with Financial Modeling, she could only help you with minor data entry and consistency checks.You noticed that this was not particularly stimulating for her, as this is something she already knew how to do and she really wanted to learn how to create the model itself. You realized that she would be more motivated to do her part if she was given the opportunity to learn as well. That is why you asked her whether she would like to sit next to you while you work on the model, so that the two of you can comment on what you are doing together. This greatly motivated her and she came up with some valuable suggestions when you had to prepare a presentation that summarizes the model that you prepared."
Behavioral,Soft Skills,How do you handle a challenge?,"First of all, you want to give the impression that you are someone who welcomes a challenge. You are a person who is willing to leave his/her comfort zone and embrace challenging situations. You learn the most when you are put in a difficult situation. And this is certainly something that the Hiring Manager is looking to hear from you. The second part of the question is how you actually handle a challenge. Do you have a structured approach? Are you a person who builds a plan of action and then sticks to it? It would be best if you could provide an example of your past experience. A story showing that you:understood the issuecreated a plan of actionexecuted the plan of action successfullyAn example of such a situation:Lets say that you were admitted to a Master's in Economics. A really challenging situation arose because you knew that most of the people in the class had already studied Finance and Econometrics, while you concentrated on Leadership courses. There was a significant gap between your skills and those of others. You realized that. You also realized that the only way to address the issue was to start with the very basics and fill the knowledge gap step by step; a very long process that required significant efforts on your end. An encouraging sign was that the results at the end of the first semester showed that you reduced the gap significantly and were heading in the right direction. By the end of the second semester, your GPA was slightly higher than the average for the class."
Behavioral,Soft Skills,What is your greatest weakness?,"The problem with this question is that you are being asked about your shortcomings, while you are doing an interview and you want to make a good impression. Make sure that you dont choose something that can impede you from being great at the job you are interviewing for. For example, if you are interviewing for a controller or a financial analyst, it is OK to say that you do not like to speak in public. However, if you are applying for a consulting or an investment bank job you should not say that, because public speaking can be essential for those professions.Choose a weakness that you can turn into a positive. I am usually not good atbut I am making an effort to improve that. Avoid clich answers like I work too hard and I am a perfectionist. No one is perfect that is why you need to indicate a weakness when you are asked about one. This shows that you are self-aware and have listened to feedback.Here's an example:The tutor at my previous internship gave me some interesting feedback: Dont try to do too much. I remembered that and had a chance to reflect on it, once the internship was over. He was right; I tried to do too much. I was eager to prove myself and implement everything that I learned in university so I could perform great. Trying to implement complex models and doing too much is something that I need to control in the future.This experience allowed me to understand that greatness is a lot of small things done well.Therefore, I decided that the next time when I am facing a similar situation, I will focus on my own duties and will make sure that I do everything that is expected of me well, instead of trying to invent the next formula of relativity."
Behavioral,Soft Skills,How do you handle working with numbers/clients/multiple tasks/stress?,"Each of these aspects can be really important for a given position and the Hiring Manager will want to make sure that you are the right person that he/she is looking for. Try to figure out the most important characteristics of the job that you are applying for. Are you expected to do multitasking? What part of your overall responsibilities would be related to financial figures? Are you going to interact with many people?Based on your findings, you will know what to expect. Prepare good examples from your past that can serve as proof of your statements."
Behavioral,Soft Skills,What would you do if the priorities of an important project you were working on suddenly changed?,"Its a very broad question, isnt it? Try answering by asking some questions that can guide you to the right answer:Who changed the projects priorities? Your boss? Clients? Suppliers? External Factors?Why did they change priorities?Try to understand the reason behind the decision and assess whether it is a valid one. Is there something that you can do about it?If you believe that you can propose a solution, dont be shy about contacting the responsible manager and sharing your idea.Or, if you believe that the reason for shifting priorities is not valid, raise your concerns with Management.Maybe there is nothing you can do about the decision. External factors that cant be changed are the reason or your Boss says that despite your concerns, the decision to change priorities remains. In this case, create a course of action and make sure that everybody on your team is aligned with the new priorities. Schedule a reasonable deadline and think of the best way that you can achieve the new goals."
Behavioral,Soft Skills,What would you do if someone at work resisted your ideas?,"Again, open communication is the best way to approach this problem. First of all, you need to make sure that you are fully explaining your ideas. Perhaps you can try an alternative approach? You can provide practical examples or make a list of the pros and cons of your suggestion. Then you should try to understand your colleagues point of view. What are the reasons behind his resistance? If his point is valid as well, think of an alternative approach together regarding the problem. Maybe you can create a hybrid solution that will include your ideas and will address his concerns."
Behavioral,Soft Skills,Is there anything else that we should know about you?,Yes. The answer to this question is always Yes. There are many things that they should know about you. This question typically comes at the end of the interview and it is an opportunity to close in a strong fashion. There is no need to pass up on this extra opportunity that the interviewer has given you. Try to address some of the following points that did not come up during the interview:Skills that are relevant for the job under considerationPast experience that will help you to be successful at this jobMotivation to work for the company in the particular role that you are interviewing forWhat is going to be your added value to the team that you will be placed inOne of the basic rules in sales is that you need to convince your client that he/she needs your product. This is a similar situation. Make a closing statement that will convince your interviewer that you are the right person that they are looking for.
Technical,Soft Skills,"Imagine youre in a room with 3 light switches. In the next room, there are 3 light bulbs, each controlled by one of the switches. You have to find out which switch controls each bulb by checking the room just once. Keep in mind that all lights are initially off, and you cant see into 1 room from the other. So, how can you figure out which switch is connected to which light bulb?","Lets say we have switches 1, 2, and 3. What you can do is leave switch 1 off, turn switch 2 on for 5 minutes, and then turn it off. Then turn switch 3 on and leave it like that. Then you enter the room. Obviously, switch 3 controls the light bulb you left on. The bulb that is off but still warm, is controlled by switch 2. And switch one controls the light bulb you never turned on."
Technical,Soft Skills,You want a work of art that was $400 but you can now buy at 25% off. How much is the promotion price?,"Its time for a quick calculation: Whats 75% off $400? The answer is $300. Of course, if youre into numbers and like using shortcuts, dont hesitate to think out loud."
Technical,ML,What are the assumptions required for linear regression? What if some of these assumptions are violated?,"There are four assumptions associated with a linear regression model:Linearity: The relationship between X and the mean of Y is linear.Homoscedasticity: The variance of the residual is the same for any value of X.Independence: Observations are independent of each other.Normality: For any fixed value of X, Y is normally distributed.Extreme violations of these assumptions will make the results redundant. Small violations of these assumptions will result in a greater bias or variance of the estimate."
Technical,Statistics & Probability,What is collinearity? What is multicollinearity? How do you deal with it?,"Collinearity is a linear association between two predictors. Multicollinearity is a situation where two or more predictors are highly linearly related.This can be problematic because it undermines the statistical significance of an independent variable. While it may not necessarily have a large impact on the models accuracy, it affects the variance of the prediction and reduces the quality of the interpretation of the independent variables.You could use the Variance Inflation Factors (VIF) to determine if there is any multicollinearity between independent variables a standard benchmark is that if the VIF is greater than 5 then multicollinearity exists."
Technical,ML,What are the drawbacks of a linear model?,"There are a couple of drawbacks of a linear model:A linear model holds some strong assumptions that may not be true in application. It assumes a linear relationship, multivariate normality, no or little multicollinearity, no auto-correlation, and homoscedasticity.A linear model cant be used for discrete or binary outcomes.You cant vary the model flexibility of a linear model."
Technical,ML,What are ridge and lasso regression and what are the differences between them?,"Both L1 and L2 regularization are methods used to reduce the overfitting of training data. Least Squares minimizes the sum of the squared residuals, which can result in low bias but high variance. L2 Regularization, also called ridge regression, minimizes the sum of the squared residuals plus lambda times the slope squared. This additional term is called the Ridge Regression Penalty. This increases the bias of the model, making the fit worse on the training data, but also decreases the variance. If you take the ridge regression penalty and replace it with the absolute value of the slope, then you get Lasso regression or L1 regularization.L2 is less robust but has a stable solution and always one solution. L1 is more robust but has an unstable solution and can possibly have multiple solutions."
Technical,ML,How does K-Nearest Neighbor work?,"K-Nearest Neighbors is a classification technique where a new sample is classified by looking at the nearest classified points, hence K-nearest. In the example above, if k=1 then the unclassified point would be classified as a blue point.If the value of k is too low, it can be subject to outliers. However, if its too high, it may overlook classes with only a few samples."
Technical,ML,How can you select k for k means?,"You can use the elbow method, which is a popular method used to determine the optimal value of k. Essentially, what you do is plot the squared error for each value of k on a graph (value of k on the x-axis and squared error on the y-axis). Once the graph is made, the point where the distortion declines the most is the elbow point."
Technical,ML,What are the support vectors in SVM?,The support vectors are the data points that touch the boundaries of the maximum margin
Technical,ML,What is pruning in decision trees?,Pruning is a technique in machine learning and search algorithms that reduces the size of decision trees by removing sections or branches of the tree that provide little to no power for classifying instances.
Technical,ML,What are random forests? Why is Naive Bayes better?,"Random forests are an ensemble learning technique that builds off of decision trees. Random forests involve creating multiple decision trees using bootstrapped datasets of the original data and randomly selecting a subset of variables at each step of the decision tree. The model then selects the mode of all of the predictions of each decision tree. By relying on a ""majority wins"" model, it reduces the risk of error from an individual tree. Random forests offer several other benefits including strong performance, can model non-linear boundaries, no cross-validation needed, and gives feature importance.Naive Bayes is better in the sense that it is easy to train and understand the process and results. A random forest can seem like a black box. Therefore, a Naive Bayes algorithm may be better in terms of implementation and understanding. However, in terms of performance, a random forest is typically stronger because it is an ensemble technique."
Technical,ML,When would you use random forests Vs SVM and why?,"There are a couple of reasons why a random forest is a better choice of an algorithm than a support vector machine:Random forests allow you to determine the feature importance. SVMs cant do this.Random forests are much quicker and simpler to build than an SVM.For multi-class classification problems, SVMs require a one-vs-rest method, which is less scalable and more memory intensive."
Technical,ML,Do you think 50 small decision trees are better than a large one? Why?,"Another way of asking this question is ""Is a random forest a better model than a decision tree?"" And the answer is yes because a random forest is an ensemble method that takes many weak decision trees to make a strong learner. Random forests are more accurate, more robust, and less prone to overfitting."
Technical,ML,Whats the difference between an AdaBoosted tree and a Gradient Boosted tree?,"AdaBoost is a boosted algorithm that is similar to Random Forests but has a couple of significant differences:Rather than a forest of trees, AdaBoost typically makes a forest of stumps (a stump is a tree with only one node and two leaves).Each stumps decision is not weighted equally in the final decision. Stumps with less total error (high accuracy) will have a higher say.The order in which the stumps are created is important, as each subsequent stump emphasizes the importance of the samples that were incorrectly classified in the previous stump.Gradient Boost is similar to AdaBoost in the sense that it builds multiple trees where each tree is built off of the previous tree. Unlike AdaBoost which builds stumps, Gradient Boost builds trees with usually 8 to 32 leaves.More importantly, Gradient differs from AdaBoost in the way that the decisions trees are built. Gradient boost starts with an initial prediction, usually the average. Then, a decision tree is built based on the residuals of the samples. A new prediction is made by taking the initial prediction + a learning rate times the outcome of the residual tree, and the process is repeated."
Technical,ML,What is the bias-variance tradeoff?,"The bias of an estimator is the difference between the expected value and true value. A model with a high bias tends to be oversimplified and results in underfitting. Variance represents the models sensitivity to the data and the noise. A model with high variance results in overfitting.Therefore, the bias-variance tradeoff is a property of machine learning models in which lower variance results in higher bias and vice versa. Generally, an optimal balance of the two can be found in which error is minimized."
Technical,ML,Explain what the bootstrap sampling method is and give an example of when its used.,"Technically speaking, the bootstrap sampling method is a resampling method that uses random sampling with replacement.Its an essential part of the random forest algorithm, as well as other ensemble learning algorithms."
Technical,ML,How does XGBoost handle the bias-variance tradeoff?,"XGBoost is an ensemble Machine Learning algorithm that leverages the gradient boosting algorithm. In essence, XGBoost is like a bagging and boosting technique on steroids. Therefore, you can say that XGBoost handles bias and variance similar to that of any boosting technique. Boosting is an ensemble meta-algorithm that reduces both bias and variance by takes a weighted average of many weak models. By focusing on weak predictions and iterating through models, the error (thus the bias) is reduced. Similarly, because it takes a weighted average of many** weak models, the final model has a lower variance than each of the weaker models themselves."
Technical,Statistics & Probability,Assume you need to generate a predictive model using multiple regression. Explain how you intend to validate this model.,"There are two main ways that you can do this:A) Adjusted R-squared.R Squared is a measurement that tells you to what extent the proportion of variance in the dependent variable is explained by the variance in the independent variables. In simpler terms, while the coefficients estimate trends, R-squared represents the scatter around the line of best fit.However, every additional independent variable added to a model always increases the R-squared value therefore, a model with several independent variables may seem to be a better fit even if it isnt. This is where adjusted R comes in. The adjusted R compensates for each additional independent variable and only increases if each given variable improves the model above what is possible by probability. This is important since we are creating a multiple regression model.B) Cross-ValidationA method common to most people is cross-validation, splitting the data into three sets: training, validating, and testing data."
Technical,ML,What is the difference between online and batch learning?,"Batch learning, also known as offline learning, is when you learn over groups of patterns. This is the type of learning that most people are familiar with, where you source a dataset and build a model on the whole dataset at once.Online learning, on the other hand, is an approach that ingests data one observation at a time. Online learning is data-efficient because the data is no longer required once it is consumed, which technically means that you dont have to store your data."
Technical,Statistics & Probability,Give several ways to deal with missing values,"There are a number of ways to handle null values including the following:You can omit rows with null values altogetherYou can replace null values with measures of central tendency (mean, median, mode) or replace it with a new category (eg. None)You can predict the null values based on other variables. For example, if a row has a null value for weight, but it has a value for height, you can replace the null value with the average weight for that given height.Lastly, you can leave the null values if you are using a machine learning model that automatically deals with null values."
Technical,Statistics & Probability,Is mean imputation of missing data acceptable practice? Why or why not?,"Mean imputation is the practice of replacing null values in a data set with the mean of the data.Mean imputation is generally bad practice because it doesnt take into account feature correlation. For example, imagine we have a table showing age and fitness score and imagine that an eighty-year-old has a missing fitness score. If we took the average fitness score from an age range of 15 to 80, then the eighty-year-old will appear to have a much higher fitness score than he actually should.Second, mean imputation reduces the variance of the data and increases bias in our data. This leads to a less accurate model and a narrower confidence interval due to a smaller variance."
Technical,Statistics & Probability,How can you identify outliers?,"There are a couple of ways to identify outliers:Z-score/standard deviations: if we know that 99.7% of data in a data set lie within three standard deviations, then we can calculate the size of one standard deviation, multiply it by 3, and identify the data points that are outside of this range. Likewise, we can calculate the z-score of a given point, and if its equal to +/- 3, then its an outlier. Note: that there are a few contingencies that need to be considered when using this method; the data must be normally distributed, this is not applicable for small data sets, and the presence of too many outliers can throw off z-score.Interquartile Range (IQR): IQR, the concept used to build boxplots, can also be used to identify outliers. The IQR is equal to the difference between the 3rd quartile and the 1st quartile. You can then identify if a point is an outlier if it is less than Q11.5IRQ or greater than Q3 + 1.5IQR. This comes to approximately 2.698 standard deviations.Other methods include DBScan clustering, Isolation Forests, and Robust Random Cut Forests."
Technical,ML,How can outliers be treated?,"There are a couple of ways:Remove outliers if theyre a garbage value.You can try a different model. For example, a non-linear model might treat an outlier differently than a linear model.You can normalize the data to narrow the range.You can use algorithms that account for outliers, such as random forests."
Technical,ML,What is principal component analysis? Explain the sort of problems you would use PCA for.,"In its simplest sense, PCA involves project higher dimensional data (eg. 3 dimensions) to a smaller space (eg. 2 dimensions). This results in a lower dimension of data, (2 dimensions instead of 3 dimensions) while keeping all original variables in the model.PCA is commonly used for compression purposes, to reduce required memory and to speed up the algorithm, as well as for visualization purposes, making it easier to summarize data."
Technical,ML,How can you avoid overfitting your model?,"For those who dont know, overfitting is a modeling error when a function fits the data too closely, resulting in high levels of error when new data is introduced to the model.There are a number of ways that you can prevent overfitting of a model:Cross-validation: Cross-validation is a technique used to assess how well a model performs on a new independent dataset. The simplest example of cross-validation is when you split your data into two groups: training data and testing data, where you use the training data to build the model and the testing data to test the model.Regularization: Overfitting occurs when models have higher degree polynomials. Thus, regularization reduces overfitting by penalizing higher degree polynomials.Reduce the number of features: You can also reduce overfitting by simply reducing the number of input features. You can do this by manually removing features, or you can use a technique, called Principal Component Analysis, which projects higher dimensional data (eg. 3 dimensions) to a smaller space (eg. 2 dimensions).Ensemble Learning Techniques: Ensemble techniques take many weak learners and converts them into a strong learner through bagging and boosting. Through bagging and boosting, these techniques tend to overfit less than their alternative counterparts."
Technical,ML,What are some of the steps for data wrangling and data cleaning before applying machine learning algorithms?,"There are many steps that can be taken when data wrangling and data cleaning. Some of the most common steps are listed below:Data profiling: Almost everyone starts off by getting an understanding of their dataset. More specifically, you can look at the shape of the dataset with .shape and a description of your numerical variables with .describe().Data visualizations: Sometimes, its useful to visualize your data with histograms, boxplots, and scatterplots to better understand the relationships between variables and also to identify potential outliers.Syntax error: This includes making sure theres no white space, making sure letter casing is consistent, and checking for typos. You can check for typos by using .unique() or by using bar graphs.Standardization or normalization: Depending on the dataset your working with and the machine learning method you decide to use, it may be useful to standardize or normalize your data so that different scales of different variables dont negatively impact the performance of your model.Handling null values: There are a number of ways to handle null values including deleting rows with null values altogether, replacing null values with the mean/median/mode, replacing null values with a new category (eg. unknown), predicting the values, or using machine learning models that can deal with null values. Read more here.Other things include: removing irrelevant data, removing duplicates, and type conversion."
Technical,ML,How should you deal with unbalanced binary classification?,"There are a number of ways to handle unbalanced binary classification (assuming that you want to identify the minority class):First, you want to reconsider the metrics that youd use to evaluate your model. The accuracy of your model might not be the best metric to look at because and Ill use an example to explain why. Lets say 99 bank withdrawals were not fraudulent and 1 withdrawal was. If your model simply classified every instance as ""not fraudulent"", it would have an accuracy of 99%! Therefore, you may want to consider using metrics like precision and recall.Another method to improve unbalanced binary classification is by increasing the cost of misclassifying the minority class. By increasing the penalty of such, the model should classify the minority class more accurately.Lastly, you can improve the balance of classes by oversampling the minority class or by undersampling the majority class."
Technical,ML,Why is mean square error a bad measure of model performance? What would you suggest instead?,"Mean Squared Error (MSE) gives a relatively high weight to large errors therefore, MSE tends to put too much emphasis on large deviations. A more robust alternative is MAE (mean absolute deviation)."
Technical,Statistics & Probability,Explain what a false positive and a false negative are. Why is it important these from each other? Provide examples when false positives are more important than false negatives and when false negatives are more important than false positives.,"A false positive is an incorrect identification of the presence of a condition when its absent.A false negative is an incorrect identification of the absence of a condition when its actually present.An example of when false negatives are more important than false positives is when screening for cancer. Its much worse to say that someone doesnt have cancer when they do, instead of saying that someone does and later realizing that they dont.This is a subjective argument, but false positives can be worse than false negatives from a psychological point of view. For example, a false positive for winning the lottery could be a worse outcome than a false negative because people normally dont expect to win the lottery anyway."
Technical,ML,What are the feature selection methods used to select the right variables?,There are two types of methods for feature selection: filter methods and wrapper methods.Filter methods include the following:Linear discrimination analysisANOVAChi-SquareWrapper methods include the following:Forward Selection: We test one feature at a time and keep adding them until we get a good fitBackward Selection: We test all the features and start removing them to see what works better
Technical,ML,How are weights initialized in a Network?,"The weights of a neural network MUST be initialized randomly because this is an expectation of stochastic gradient descent.If you initialized all weights to the same value (i.e. zero or one), then each hidden unit will get exactly the same signal. For example, if all weights are initialized to 0, all hidden units will get zero signal."
Technical,ML,What happens if the learning rate is set too high or too low?,"If the learning rate is too low, your model will train very slowly as minimal updates are made to the weights through each iteration. Thus, it would take many updates before reaching the minimum point.If the learning rate is set too high, this causes undesirable divergent behavior to the loss function due to drastic updates in weights, and it may fail to converge."
Technical,SQL,What is the difference between an aggregation function and a window function?,"A window function is like an aggregate function in the sense that it returns aggregate values (eg. SUM(), COUNT(), MAX()).What makes window functions different is that it does not group the result set. The number of rows in the output is the same as the number of rows in the input."
Technical,SQL,When will ROW_NUMBER and RANK give different results? Give an example.,ROW_NUMBER and RANK will give different results when there are ties within a partition for a particular ordering value.Example:SELECT ROW_NUMBER() OVER(ORDER BY GPA) as ROW_NUMBERSELECT RANK() OVER(ORDER BY GPA) as RANK
Technical,SQL,Is it possible for LEFT JOIN and FULL OUTER JOIN to produce the same results? Why or Why not?,"Yes. If every row in the second table can be joined to the first table and every row in the first table can be joined to the second table using a LEFT JOIN, then the result will be the same for a FULL OUTER JOIN."
Technical,SQL,What happens if I GROUP BY a column that is not in the SELECT statement? Why does this happen?,"Your query will return no results. This is because the column that you are grouping by needs to be included in the SELECT statement so that the query can identify the values for that given column.Example:SELECT COUNT(DISTINCT ID)FROM tableGROUP BY DateSince Date is not included in the SELECT clause, it will return nothing."
Technical,SQL,LAG and LEAD are especially useful in what type of scenarios?,"LAG and LEAD functions are useful when you want to compare values from different periods. For example, if you want to compare each weeks sales with the previous weeks."
Technical,SQL,What is the difference between IFNULL and COALESCE?,"There are a couple of differences:ISNULL is evaluated once only since is a function. On the other hand, the input values for the COALESCE expression can be evaluated multiple times.Data type determination of the resulting expression is different for each. ISNULL uses the data type of the first parameter, while COALESCE follows the CASE expression rules and returns the data type of value with the highest precedence.Lastly, validations for ISNULL and COALESCE are different. For example, a NULL value for ISNULL is converted to int, unlike COALESCE where you must provide a data type. ISNULL takes only 2 parameters whereas COALESCE takes a variable number of parameters."
Technical,SQL,"Do temp tables make your code cleaner and faster, one of the two, or none? Why?","Generally, temp tables are both faster and cleaner. It is much easier to read and follow than subqueries, and in terms of speed, SQL is optimized to do joins rather than subqueries."
Technical,SQL,Write SQL queries to find a time difference between two events.,"First, you can use the LEAD() or LAG() function to create a new column of dates that you want to compare.Then, you can use DATEDIFF to get the difference time between the two events."
Technical,SQL,Does creating a view require storage in a database?,"No, A view does not require any storage in a database because it does not exist physically. The only space that would be required for a view is the space to store the definition of the view, not the data that it presents."
Technical,SQL,How would you handle NULLs when querying a data set?,"You can handle NULLs when querying using CASE WHEN statements, IFNULL, or COALESCE."
Technical,Statistics & Probability,What are covariance and correlation? How are they related?,"Covariance is a quantitative measure of the extent to which the deviation of one variable from its mean matches the deviation of the other from its mean.Correlation is a measurement of the relationship between two variables. It is the covariance of the two variables, normalized by the variance of each variable."
Technical,Statistics & Probability,What is the law of large numbers?,"The Law of Large Numbers is a theory that states that as the number of trials increases, the average of the result will become closer to the expected value.Eg. flipping heads from fair coin 100,000 times should be closer to 0.5 than 100 times."
Technical,Statistics & Probability,What is the Central Limit Theorem? Explain it. Why is it important?,The central limit theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size gets larger no matter what the shape of the population distribution.The central limit theorem is important because it is used in hypothesis testing and also to calculate confidence intervals.
Technical,Statistics & Probability,How does experimental data contrast with observational data?,Observational data comes from observational studies which are when you observe certain variables without intervening and try to determine if there is any correlation.Experimental data comes from experimental studies (with intervention) which are when you control certain variables and hold them constant to determine if there is any causality.
Technical,Statistics & Probability,"Explain selection bias (with regard to a dataset, not variable selection). Why is it important? How can data management procedures such as missing data handling make it worse?","Selection bias is the phenomenon of selecting individuals, groups, or data for analysis in such a way that proper randomization is not achieved, ultimately resulting in a sample that is not representative of the population.Understanding and identifying selection bias is important because it can significantly skew results and provide false insights about a particular population group.Types of selection bias include:Sampling bias: a biased sample caused by non-random samplingTime interval: selecting a specific time frame that supports the desired conclusion. e.g. conducting a sales analysis near Christmas.Exposure: includes clinical susceptibility bias, protopathic bias, indication bias. _Read more here._Data: includes cherry-picking, suppressing evidence, and the fallacy of incomplete evidence.Attrition: attrition bias is similar to survivorship bias, where only those that survived a long process are included in an analysis, or failure bias, where those that failed are only includedObserver selection: related to the Anthropic principle, which is a philosophical consideration that any data we collect about the universe is filtered by the fact that, in order for it to be observable, it must be compatible with the conscious and sapient life that observes it.Handling missing data can make selection bias worse because different methods impact the data in different ways. For example, if you replace null values with the mean of the data, you adding bias in the sense that youre assuming that the data is not as spread out as it might actually be."
Technical,Statistics & Probability,What is the difference between interpolation and extrapolation and why does it matter?,Interpolation is when a prediction is made using inputs that lie within the set of observed values.Extrapolation is when a prediction is made using an input thats outside of the set of observed values.Its important to know the distinction because interpolations are generally more accurate than extrapolations.
Technical,Statistics & Probability,What is survivorship bias?,"The phenomenon where only those that survived a long process are included or excluded in an analysis, thus creating a biased sample.A great example provided by Sreenivasan Chandrasekar is the following:""We enroll for gym membership and attend for a few days. We see the same faces of many people who are fit, motivated and exercising everyday whenever we go to gym. After a few days we become depressed why we arent able to stick to our schedule and motivation more than a week when most of the people who we saw at gym could. What we didnt see was that many of the people who had enrolled for gym membership had also stopped turning up for gym just after a week and we didnt see them."""
Technical,Statistics & Probability,What is root cause analysis? How can you identify a cause vs. a correlation? Give examples.,Root cause analysis is a method of problem-solving used for identifying the root cause(s) of a problemYou can identify a correlation using simple data analyses. You can then identify causation by conducting an experiment so that all other variables are isolated (ideally).
Technical,Statistics & Probability,Explain what a long-tailed distribution is and provide three examples of relevant phenomena that have long tails. Why are they important in classification and regression problems?,"A long-tailed distribution is a type of heavy-tailed distribution that has a tail (or tails) that drop off gradually and asymptotically.3 practical examples include the power law, the Pareto principle (more commonly known as the 8020 rule), and product sales (i.e. best selling products vs others).Its important to be mindful of long-tailed distributions in classification and regression problems because the least frequently occurring values make up the majority of the population. This can ultimately change the way that you deal with outliers, and it also conflicts with some machine learning techniques with the assumption that the data is normally distributed."
Technical,Statistics & Probability,How do you control for biases?,"There are many things that you can do to control and minimize bias. Two common things include randomization, where participants are assigned by chance, and random sampling, sampling in which each member has an equal probability of being chosen."
Technical,Statistics & Probability,"Give examples of data that does not have a Gaussian distribution, nor log-normal.",Any type of categorical data wont have a gaussian distribution or lognormal distribution.Exponential distributions eg. the amount of time that a car battery lasts or the amount of time until an earthquake occurs.
Technical,Statistics & Probability,How do you assess the statistical significance of an insight?,"You would perform hypothesis testing to determine statistical significance. First, you would state the null hypothesis and alternative hypothesis. Second, you would calculate the p-value, the probability of obtaining the observed results of a test assuming that the null hypothesis is true. Last, you would set the level of the significance (alpha) and if the p-value is less than the alpha, you would reject the null in other words, the result is statistically significant."
Technical,Statistics & Probability,The homicide rate in Scotland fell last year to 99 from 115 the year before. Is this reported change really noteworthy?,"Since this is a Poisson distribution question, mean = lambda = variance, which also means that standard deviation = square root of the mean.a 95% confidence interval implies a z score of 1.96one standard deviation = sqrt(115) = 10.724Therefore the confidence interval = 115+/- 21.45 = [93.55, 136.45]. Since 99 is within this confidence interval, we can assume that this change is not very noteworthy."
Technical,Statistics & Probability,What is the meaning of ACF and PACF?,"To understand ACF and PACF, you first need to know what autocorrelation or serial correlation is. Autocorrelation looks at the degree of similarity between a given time series and a lagged version of itself.Therefore, the autocorrelation function (ACF) is a tool that is used to find patterns in the data, specifically in terms of correlations between points separated by various time lags. For example, ACF(0)=1 means that all data points are perfectly correlated with themselves and ACF(1)=0.9 means that the correlation between one point and the next one is 0.9.The PACF is short for partial autocorrelation function. Quoting a text from StackExchange, ""It can be thought as the correlation between two points that are separated by some number of periods n, but with the effect of the intervening correlations removed."" For example. If T1 is directly correlated with T2 and T2 is directly correlated with T3, it would appear that T1 is correlated with T3. PACF will remove the intervening correlation with T2."
Technical,Statistics & Probability,How can you generate a random number between 17 with only a die?,"If you roll a die twice and consider the event of two rolls, there are 36 different outcomes. If we exclude the combination (6,6), there will be 35 possible outcomes. You can then assign 5 combinations to each number from 1 to 7."
Technical,Statistics & Probability,"Theres a game where you are given two fair six-sided dice and asked to roll. If the sum of the values on the dice equals seven, then you win $21. However, you must pay $5 to play each time you roll both dice. Do you play this game?","The odds of rolling a 7 is 1/6. This means that you are expected to pay $30 (5*6) to win $21. Take these two numbers and the expected payout is -$9 (2130). Since the expected payout is negative, you should not play this game."
Technical,Statistics & Probability,"The probability that item an item at location A is 0.6, and 0.8 at location B. What is the probability that item would be found on Amazon website?","We need to make some assumptions about this question before we can answer it. Lets assume that there are two possible places to purchase a particular item on Amazon and the probability of finding it at location A is 0.6 and B is 0.8. The probability of finding the item on Amazon can be explained as so:We can reword the above as P(A) = 0.6 and P(B) = 0.8. Furthermore, lets assume that these are independent events, meaning that the probability of one event is not impacted by the other. We can then use the formulaP(A or B) = P(A) + P(B) P(A and B) P(A or B) = 0.6 + 0.8 (0.6*0.8) P(A or B) = 0.92"
Technical,Statistics & Probability,How do you prove that males are on average taller than females by knowing just gender height?,"You can use hypothesis testing to prove that males are taller on average than females.The null hypothesis would state that males and females are the same height on average, while the alternative hypothesis would state that the average height of males is greater than the average height of females.Then you would collect a random sample of heights of males and females and use a t-test to determine if you reject the null or not."
Technical,Case Study,"If a PM says that they want to double the number of ads in Newsfeed, how would you figure out if this is a good idea or not?","You can perform an A/B test by splitting the users into two groups: a control group with the normal number of ads and a test group with double the number of ads. Then you would choose the metric to define what a ""good idea"" is. For example, we can say that the null hypothesis is that doubling the number of ads will reduce the time spent on Facebook and the alternative hypothesis is that doubling the number of ads wont have any impact on the time spent on Facebook. However, you can choose a different metric like the number of active users or the churn rate. Then you would conduct the test and determine the statistical significance of the test to reject or not reject the null."
Technical,Statistics & Probability,"A box has 12 red cards and 12 black cards. Another box has 24 red cards and 24 black cards. You want to draw two cards at random from one of the two boxes, one card at a time. Which box has a higher probability of getting cards of the same color and why?","The box with 24 red cards and 24 black cards has a higher probability of getting two cards of the same color. Lets walk through each step.Lets say the first card you draw from each deck is a red Ace.This means that in the deck with 12 reds and 12 blacks, theres now 11 reds and 12 blacks. Therefore your odds of drawing another red are equal to 11/(11+12) or 11/23.In the deck with 24 reds and 24 blacks, there would then be 23 reds and 24 blacks. Therefore your odds of drawing another red are equal to 23/(23+24) or 23/47.Since 23/47 > 11/23, the second deck with more cards has a higher probability of getting the same two cards."
Technical,Statistics & Probability,How can you tell if a given coin is biased?,"The answer is simply to perform a hypothesis test:The null hypothesis is that the coin is not biased and the probability of flipping heads should equal 50% (p=0.5). The alternative hypothesis is that the coin is biased and p != 0.5.Flip the coin 500 times.Calculate Z-score (if the sample is less than 30, you would calculate the t-statistics).Compare against alpha (two-tailed test so 0.05/2 = 0.025).If p-value > alpha, the null is not rejected and the coin is not biased. If p-value < alpha, the null is rejected and the coin is biased."
Technical,Statistics & Probability,Make an unfair coin fair,"Since a coin flip is a binary outcome, you can make an unfair coin fair by flipping it twice. If you flip it twice, there are two outcomes that you can bet on: heads followed by tails or tails followed by heads.P(heads) P(tails) = P(tails) P(heads)This makes sense since each coin toss is an independent event. This means that if you get heads ? heads or tails ? tails, you would need to reflip the coin."
Technical,Statistics & Probability,Geiger counter records 100 radioactive decays in 5 minutes. Find an approximate 95% interval for the number of decays per hour.,"Since this is a Poisson distribution question, mean = lambda = variance, which also means that standard deviation = square root of the meana 95% confidence interval implies a z score of 1.96one standard deviation = 10Therefore the confidence interval = 100 +/- 19.6 = [964.8, 1435.2]"
Technical,Statistics & Probability,The homicide rate in Scotland fell last year to 99 from 115 the year before. Is this reported change really noteworthy?,"Since this is a Poisson distribution question, mean = lambda = variance, which also means that standard deviation = square root of the meana 95% confidence interval implies a z score of 1.96one standard deviation = sqrt(115) = 10.724Therefore the confidence interval = 115+/- 21.45 = [93.55, 136.45]. Since 99 is within this confidence interval, we can assume that this change is not very noteworthy."
Technical,Statistics & Probability,Consider influenza epidemics for two-parent heterosexual families. Suppose that the probability is 17% that at least one of the parents has contracted the disease. The probability that the father has contracted influenza is 12% while the probability that both the mother and father have contracted the disease is 6%. What is the probability that the mother has contracted influenza?,Using the General Addition Rule for probability: P(mother or father) = P(mother) + P(father) P(mother and father) P(mother) = P(mother or father) + P(mother and father) P(father) P(mother) = 0.17 + 0.060.12 P(mother) = 0.11
Technical,Statistics & Probability,A diet pill is given to 9 subjects over six weeks. The average difference in weight (follow up baseline) is -2 pounds. What would the standard deviation of the difference in weight have to be for the upper endpoint of the 95% T confidence interval to touch 0?,Upper bound = mean + t-score(standard deviation/sqrt(sample size)) 0 = -2 + 2.306(s/3) 2 = 2.306 * s / 3 s = 2.601903 Therefore the standard deviation would have to be at least approximately 2.60 for the upper bound of the 95% T confidence interval to touch 0.
Technical,Case Study,"You are compiling a report for user content uploaded every month and notice a spike in uploads in October. In particular, a spike in picture uploads. What might you think is the cause of this, and how would you test it?","There are a number of potential reasons for a spike in photo uploads:A new feature may have been implemented in October which involves uploading photos and gained a lot of traction by users. For example, a feature that gives the ability to create photo albums.Similarly, its possible that the process of uploading photos before was not intuitive and was improved in the month of October.There may have been a viral social media movement that involved uploading photos that lasted for all of October. Eg. Movember but something more scalable.Its possible that the spike is due to people posting pictures of themselves in costumes for Halloween.The method of testing depends on the cause of the spike, but you would conduct hypothesis testing to determine if the inferred cause is the actual cause."
Technical,Statistics & Probability,How to define/select metrics?,"There isnt a one-size-fits-all metric. The metric(s) chosen to evaluate a machine learning model depends on various factors:Is it a regression or classification task?What is the business objective? Eg. precision vs recallWhat is the distribution of the target variable?There are a number of metrics that can be used, including adjusted r-squared, MAE, MSE, accuracy, recall, precision, f1 score, and the list goes on."
Technical,ML,"Suppose you had bank transaction data, and wanted to separate out likely fraudulent transactions. How would you approach it? Why might accuracy be a bad metric for evaluating success?","In Machine Learning, problems like fraud detection are usually framed as classification problems. In order to solve this problem we may use different features like amount, merchant, location, time etc associated with each transaction. * One of the biggest challenge with fraud transaction detection is- majority of transactions are not fraud, so we have inbalance data! * First step will be to do EDA and understand our data and intesity of class inbalance. * In order to handle inbalance data problem we can use one of the following method * Oversampling SMOTE (Synthetic Minority Over-sampling Technique) * Undersampling One simple way of undersampling is randomly selecting a handful of samples from the class that is overrepresented. * Combined Class Methods Use SMOTE together with edited nearest-neighbours (ENN). Here, ENN is used as the cleaning method after SMOTE over-sampling to obtain a cleaner space. * Developed by Wilson (1972), the ENN method works by finding the K-nearest neighbor of each observation first, then check whether the majority class from the observations k-nearest neighbor is the same as the observations class or not. * If the majority class of the observations K-nearest neighbor and the observations class is different, then the observation and its K-nearest neighbor are deleted from the dataset. In default, the number of nearest-neighbor used in ENN is K=3. * As ENN removes the observation and its K-nearest neighbor instead of just removing observation and its 1-nearest neighbor that are having different classes. Thus, ENN can be expected to give more in-depth data cleaning. * Test model performane for each of above technique and choose best performing model."
Technical,ML,Why might accuracy be a bad metric for evaluating success?,"In case of inbalance data accuracy metric is not usefull. Accuracy tells us how close a measured value is to the actual (true) value. But here we are more interested in fraud transactions. We dont mind declaring few good transactions as fruad but failing to identify fraud transaction is not acceptable. In such cases classification of true positives is a priority, hence precision metric make more sense."
Technical,ML,What are the assumptions for linear regression Linear regression assumptions,"Data should have linear relationship between X and Y (actually mean of Y) * Data should be normally distributed * No or little multicollinearity (observations should be independent of each other) * Assumption of additivity: This means that each feature (X) should affect the target (Y) independently. In other words, the influence of one feature on the target does not change because of another feature. * Example: Let's say you're predicting house prices based on square footage and number of bedrooms. Additivity assumes that adding an extra bedroom increases the price by a certain amount, regardless of whether the house is large or small.If the impact of the bedroom varied based on house size (e.g., extra bedroom matters more in small houses), then the additivity assumption would be violated. * Homoscedasticity: This means that the spread of the errors (the differences between your model's predictions and the actual values) should be roughly the same for all the predicted values of Y. * Example: If you're predicting a person's weight based on their height, the spread of actual weights around the predicted weight should be similar whether the person is tall or short. If the spread varies widely (e.g., much larger for taller people), this assumption is violated."
Technical,ML,How can AI be used in spam email detection?,"AI, particularly through NLP techniques, analyzes the content of emails to determine if they are spam. It does this by identifying patterns, keywords, and stylistic choices that are common in spam messages. Here's a simplified breakdown of the process: **Steps in AI-Based Spam Detection** 1. **Data Collection:** * A large dataset of emails, both spam and legitimate (""ham""), is gathered. 2. **Preprocessing:** * Emails are cleaned and standardized. * This often involves: * Removing punctuation and special characters. * Converting all text to lowercase. * Tokenization: Breaking down sentences into individual words. * Removing stop words (common words like 'the', 'and', etc.). * Stemming/Lemmatization: Reducing words to their base form (e.g., ""running"" becomes ""run""). 3. **Feature Extraction:** * Key characteristics are extracted from the preprocessed emails to help identify spam. These might include: * Word frequency: How often specific words appear. Let's go through an example to understand how **Bag of Words (BoW)** are used as input features for a machine learning model in spam email detection. **Example Scenario:** Suppose we have a small dataset of three emails: 1. Email 1: ""Buy cheap watches now"" 2. Email 2: ""Exclusive watches on sale"" 3. Email 3: ""Meeting tomorrow, please prepare the slides"" **Bag of Words** creates a feature vector that counts how often each word appears in each email. Let's build a vocabulary of all unique words from our emails: - Vocabulary: [""buy"", ""cheap"", ""watches"", ""now"", ""exclusive"", ""on"", ""sale"", ""meeting"", ""tomorrow"", ""please"", ""prepare"", ""the"", ""slides""] Each email is represented as a vector of word counts: - **Email 1**: [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0] - ""buy"": 1, ""cheap"": 1, ""watches"": 1, ""now"": 1, all other words: 0 - **Email 2**: [0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0] - ""exclusive"": 1, ""watches"": 1, ""on"": 1, ""sale"": 1, all other words: 0 - **Email 3**: [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1] - ""meeting"": 1, ""tomorrow"": 1, ""please"": 1, ""prepare"": 1, ""the"": 1, ""slides"": 1 These vectors (lists of numbers) become the input features for the machine learning model. The model learns patterns based on word presence and frequency to distinguish between spam and non-spam emails. * N-grams: Groups of consecutive words (e.g., ""free offer""). To understand how the n-grams input vector looks, let's use same example and see how we can represent emails using n-grams. Suppose we have the following emails: 1. **Email 1**: ""Buy cheap watches now"" 2. **Email 2**: ""Exclusive watches on sale"" 3. **Email 3**: ""Meeting tomorrow, please prepare the slides"" We will use **bigrams** (2-grams) for this example. 1. **Generate Bigrams**: - For **Email 1**: ""Buy cheap"", ""cheap watches"", ""watches now"" - For **Email 2**: ""Exclusive watches"", ""watches on"", ""on sale"" - For **Email 3**: ""Meeting tomorrow"", ""tomorrow please"", ""please prepare"", ""prepare the"", ""the slides"" 2. **Build the Vocabulary**: - List all unique bigrams from the emails: - [""Buy cheap"", ""cheap watches"", ""watches now"", ""Exclusive watches"", ""watches on"", ""on sale"", ""Meeting tomorrow"", ""tomorrow please"", ""please prepare"", ""prepare the"", ""the slides""] 3. **Create the Input Vectors**: - Each email will be represented as a vector where each element corresponds to the count of a bigram in the email, based on the vocabulary. #### Input Vectors for Each Email Let's create the input vector for each email using the bigrams vocabulary: - **Email 1**: ""Buy cheap watches now"" - ""Buy cheap"": 1 - ""cheap watches"": 1 - ""watches now"": 1 - All other bigrams: 0 **Email 1 vector**: [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0] - **Email 2**: ""Exclusive watches on sale"" - ""Exclusive watches"": 1 - ""watches on"": 1 - ""on sale"": 1 - All other bigrams: 0 **Email 2 vector**: [0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0] - **Email 3**: ""Meeting tomorrow, please prepare the slides"" - ""Meeting tomorrow"": 1 - ""tomorrow please"": 1 - ""please prepare"": 1 - ""prepare the"": 1 - ""the slides"": 1 - All other bigrams: 0 **Email 3 vector**: [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1] These vectors are then used as input to a machine learning model to learn patterns and predict whether new emails are spam or not, based on the bigrams present in them. * Email metadata: Sender's address, subject line, etc. * This can be a mix of numeric and categorical data: * Numeric: Length of subject line, number of recipients, etc. * Categorical: Sender's domain, top-level domain (TLD) of URLs in the email, presence of specific keywords in the subject, etc. Categorical data would need to be encoded into numerical format (e.g., one-hot encoding, label encoding). * Sentiment analysis: Determining the emotional tone of the email. 4. **Model Training:** * A machine learning model (e.g., Naive Bayes, Support Vector Machine, or even deep learning models) is trained on the labeled dataset. The model learns to distinguish between spam and ham based on the extracted features. 5. **Model Evaluation:** Test the model on a separate set of emails to see how well it distinguishes between spam and non-spam emails. Metrics like accuracy, precision, recall, and F1-score are used to evaluate performance. 6. **Prediction:** * New incoming emails are preprocessed and their features are extracted. * The trained model analyzes these features and predicts whether the email is spam or not. 7. **Filtering:** * Emails classified as spam are automatically moved to the spam folder or blocked, while legitimate emails are delivered to the inbox. 8. **Continuous Improvement:** Regularly update the model with new data and retrain it to improve its accuracy and adapt to new spam tactics."
Technical,ML,How to build sentiment analysis model from scratch?,"1. Data Gathering:** * Collect a set of text data (e.g., movie reviews, tweets, product feedback) where each piece of text is labeled with its corresponding sentiment (positive, negative, or neutral). **2. Text Preprocessing:** * Clean and standardize the text: * Remove punctuation, special characters, and HTML tags. * Convert all text to lowercase. * Remove stop words (common words like 'the', 'and', etc.) * Tokenize the text into individual words. * Consider stemming or lemmatization to reduce words to their base forms (e.g., ""running"" becomes ""run""). **3. Feature Representation:** * Convert the preprocessed text into numerical features that a machine learning model can understand. Common approaches include: * Bag-of-Words (BoW): Create a dictionary of all unique words in your dataset. Represent each text as a vector where each element corresponds to the count of a specific word in that text. * TF-IDF: Similar to BoW but weighs words based on their frequency in the text and their rarity across the entire dataset. **4. Model Training:** * Choose a simple classification algorithm (e.g., Naive Bayes, Logistic Regression). * Split your dataset into training and testing sets. * Train your chosen model on the training set, where it learns to associate the text features with their corresponding sentiment labels. **5. Model Evaluation:** * Use the trained model to predict the sentiment of the text in the testing set. * Compare the model's predictions to the actual labels and calculate evaluation metrics like accuracy, precision, recall, and F1-score. **6. Prediction on New Data:** * Once your model is trained and evaluated, you can use it to predict the sentiment of new, unseen text. **Note:** * This is a very simplified explanation of building a basic sentiment analysis model. In practice, more sophisticated techniques like deep learning models (e.g., Recurrent Neural Networks, Transformers) and word embeddings (e.g., Word2Vec, GloVe) can be used to achieve better performance."
Technical,ML,When to use tokenization and stemming/Lemmatization?,"Tokenization:** * **Always** use tokenization as the first step in any NLP task that involves analyzing the text at the word level. * It's fundamental for breaking down sentences into individual words or subwords, which is necessary for further processing and analysis. * **Stemming/Lemmatization:** * Use these when you want to reduce words to their base or root forms. This can be helpful for: * **Reducing dimensionality:** Fewer unique words to deal with. * **Improving generalization:** Models can learn patterns across different word forms (e.g., ""run"", ""running"", ""runs"" all become ""run""). * **Applications where exact word forms are less important:** Sentiment analysis, topic modeling, information retrieval. **Can They Be Used Together?** * **Yes**, tokenization is typically followed by stemming/lemmatization. **Typical NLP Pipeline:** 1. **Tokenization:** Break text into words/subwords. 2. **Stemming/Lemmatization (Optional):** Reduce words to their base forms. 3. **Other Preprocessing:** Remove stop words, handle punctuation, etc. 4. **Feature Extraction:** Convert text into numerical features (e.g., BoW, TF-IDF). 5. **Modeling:** Train and evaluate machine learning models. **Choosing Stemming vs. Lemmatization:** * **Stemming:** * **Faster** and computationally less expensive. * Can produce non-words (e.g., ""studies"" -> ""studi""). * Suitable when meaning preservation is less critical. * **Lemmatization:** * **Slower** but produces valid words. * Requires part-of-speech tagging to work accurately. * Better when preserving meaning is important. **Example:** * **Original sentence:** ""The cats were running and playing in the garden."" * **Tokenization:** [""The"", ""cats"", ""were"", ""running"", ""and"", ""playing"", ""in"", ""the"", ""garden"", "".""] * **Stemming:** [""the"", ""cat"", ""were"", ""run"", ""and"", ""play"", ""in"", ""the"", ""garden"", "".""] * **Lemmatization:** [""the"", ""cat"", ""be"", ""run"", ""and"", ""play"", ""in"", ""the"", ""garden"", "".""]"
Technical,ML,What are the advantages and disadvantages of neural networks?,"Here are some advantages of Neural Networks** * Storing information on the entire network: Information such as in traditional programming is stored on the entire network, not on a database. The disappearance of a few pieces of information in one place does not restrict the network from functioning. * The ability to work with inadequate knowledge: After ANN training, the data may produce output even with incomplete information. The lack of performance here depends on the importance of the missing information. * It has fault tolerance: Corruption of one or more cells of ANN does not prevent it from generating output. This feature makes the networks fault-tolerant. * Having a distributed memory: For ANN to be able to learn, it is necessary to determine the examples and to teach the network according to the desired output by showing these examples to the network. The network's progress is directly proportional to the selected instances, and if the event can not be shown to the network in all its aspects, the network can produce incorrect output * Gradual corruption: A network slows over time and undergoes relative degradation. The network problem does not immediately corrode. * Ability to train machine: Artificial neural networks learn events and make decisions by commenting on similar events. * Parallel processing ability: Artificial neural networks have numerical strength that can perform more than one job at the same time. **Disadvantages of Neural Networks** * Hardware dependence: Artificial neural networks require processors with parallel processing power, by their structure. For this reason, the realization of the equipment is dependent. * Unexplained functioning of the network: This is the most important problem of ANN. When ANN gives a probing solution, it does not give a clue as to why and how. This reduces trust in the network. * Assurance of proper network structure: There is no specific rule for determining the structure of artificial neural networks. The appropriate network structure is achieved through experience and trial and error. * The difficulty of showing the problem to the network: ANNs can work with numerical information. Problems have to be translated into numerical values before being introduced to ANN. The display mechanism to be determined here will directly influence the performance of the network. This depends on the user's ability. * The duration of the network is unknown: The network is reduced to a certain value of the error on the sample means that the training has been completed. This value does not give us optimum results."
Technical,ML,What is the difference between bias and variance?,"Bias comes from model underfitting some set of data, whereas variance is the result of model overfitting some set of data. * Underfitting models have high error in training as well as test set. This behavior is called as High Bias * Consider below example of bias(underfitting) where we are trying to fit linear function for nonlinear data. * Overfitting models have low error in training set but high error in test set. This behavior is called as High Variance * Consider below example of variance(overfitting) where complicated function creates lots of unnecessary curves and angles that are not related with data. * Low bias (low underfitting) ML algorithms: Decision Tree, k-NN, SVM * High bias (high underfitting) ML algorithms: Linear regression, Logistic regression * High Variance (high overfitting) ML algorithms: Polynimial regression."
Technical,ML,What is bias-variance tradeoff,"As we increase the complexity of the model, error will reduce due to lower bias in the model. However, this will happen until a particular point. If we continue to make our model complex then model will overfit and lead to high variance. * The goal of any supervised ML algorithm to have low bias and low variance to achieve good prediction performance. This is referred as bias-variance tradeoff. We can acheive bias-variance tradeoff by selecting optimum model complexity. **We can also use hyperparamters to adjust model complexity, few examples are as below** * The K-NN algorithm has low bias(underfitting) and high variance(overfitting), tradeoff can be achieved by increasing the value of 'K'. * Higher the value of 'K' means higher the number of neighbours, which in turn increases the bias of the model. * The SVM algorithm has low bias(underfitting) and high variance(overfitting), trade off can be achived by changing the 'C' paramter. * The C parameter tells the SVM optimization how much you want to avoid misclassifying each training example. * For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. * Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points. * For very tiny values of C, you should get misclassified examples, often even if your training data is linearly separable. * The decision trees has low bias(underfitting) and high variance(overfitting), bias-variance tradeoff can be achived by changing the tree depth. * If the tree is shallow then we're not checking a lot of conditions/constrains i.e. the logic is simple or less complex, hence it automatically reduces over-fitting. This introduces more bias compared to deeper trees where we overfit the data. It can be imagined as we're deliberately not calculating more conditions means we're making some assumption (introduces bias) while creating the tree. * The linear regression has low variance(overfitting) and high bias(underfitting), bias-variance tradeoff can be acheived by increasing the number of features or by using another regression technique that can fit data better. * If data is not linearly separable then linear regression algorithm will result in low variance and high bias."
Technical,ML,What is more important model accuracy or model performance?,"Model accuracy matters the most! inaccurate information is not usefull. * Model performance can be improved by increasing the compute resources. * Model accuracy and performance can be subjective to the problem in hand. For example, in analysis of medical images to determine if there is a disease (such as cancer), the accuracy extremely critical, even if the models would take minutes or hours to make a prediction. * Some applications require real time performance, even if this comes at a cost of accuracy. For example, imagine a machine that views a fast conveyor belt carrying tomatoes, where it must separate the green from the red ones. Though an occasional error is undesired, the success of this machine is more determined by its ability to withstand its throughput. * A more common example is face detection for recreational applications. People would expect a fast response from the app, though the occasional missed face would not render it useless."
Technical,ML,What is the difference between machine learning and deep learning?,"Deep Learning out performs traditional ML techniques if the data size is large. But with small data size, traditional Machine Learning algorithms are preferable. Deep Learning really shines when it comes to complex problems such as image classification, natural language processing, and speech recognition. Few important differences are as below, |Machine Learning|Deep Learning| |:-|:-| | Machine learning uses algorithms to parse data, learn from that data, and make informed decisions based on what it has learned|Deep learning structures algorithms in layers to create an ""artificial neural network that can learn and make intelligent decisions on its own| |Using handcrafted rules and feature engineering, ML algorithms can work well with small data. But its performance plateau once data increases.|Deep Learning algorithms need large data to understand it perfectly. Deep learning performances increases as data increases.| |Traditional ML algorithms can work on less computing power.|DL algorithms need high compute. There also special purpose compute for DL like GPu and TPU.| |In case of ML domain experts/Data scientists needs to do feature engineering in order to enable model o learn all data patterns.|One advantage with DL that it learns high level features from data.No extrnal feature engineering is required.| |ML models take less time to train|DL models take more time to train| | Ml models are easy to interpret as comare to DL models.|DL models are black box and its very difficult to interpret the results.| # ."
Technical,ML,Explain standard deviation and variance,"Variance:** * **In a Nutshell:** Variance measures how spread out a set of data is. A high variance indicates that the data points are widely scattered, while a low variance means they are clustered close together. * **Example:** Imagine two groups of students taking a test. * Group A: Scores are 60, 70, 75, 80, 90 * Group B: Scores are 40, 70, 70, 70, 100 * Group B has a higher variance because its scores are more spread out from the average (70) compared to Group A. **Standard Deviation:** * **In a Nutshell:** Standard deviation is simply the square root of the variance. It's a more interpretable measure of spread because it's in the same units as the original data. * **Example:** Continuing with the student test scores: * If the variance for Group A is 150, then its standard deviation is 150 12.25. * This means that, on average, the scores in Group A deviate from the mean by about 12.25 points. **Why They Matter:** * **Understanding Data Distribution:** Variance and standard deviation help you understand how much your data varies from the average. This is crucial in many fields like finance (risk assessment), manufacturing (quality control), and social sciences (analyzing survey results). * **Comparing Data Sets:** These measures allow you to compare the spread of different datasets. For instance, you can see if one group of students has more varied test scores than another. * **Statistical Analysis:** Variance and standard deviation are fundamental in many statistical tests and models. They help determine if differences between groups are significant or just due to chance. **In Simpler Terms:** * **Variance:** Think of it as the ""average squared distance"" of each data point from the mean. * **Standard Deviation:** It's like the ""typical"" or ""average"" distance of data points from the mean. Remember, a higher variance or standard deviation means the data is more spread out, while a lower value indicates the data is more tightly clustered around the average. > We can expect about 68% of values to be within plus-or-minus 1 standard deviation."
Technical,ML,Explain confusion matrix,"What is a Confusion Matrix?** * Imagine a table that helps you see how well your machine learning model is performing, especially when it comes to classification tasks (like deciding if an email is spam or not). * It compares the model's predictions to the actual, true labels of your data. **Why is it useful?** * Beyond Accuracy: A confusion matrix gives you a more detailed look at your model's performance than just overall accuracy. * Spotting Weaknesses: You can see which classes your model struggles with. * Choosing the Right Metric: Depending on your problem, you might care more about minimizing false positives or false negatives. The confusion matrix helps you calculate metrics like precision, recall, and F1-score which focus on these aspects. **In essence:** A confusion matrix is like a scorecard for your classification model, showing you not just how many it got right, but also the specific types of errors it's making. This helps you understand its strengths and weaknesses, and make informed decisions about how to improve it. * From our confusion matrix, we can calculate five different metrics measuring the validity of our model. * **ACCURACY** Accuracy is the ratio of correctly identified subjects in a pool of subjects. Accuracy = (all correct / all) = (TP+TN)/(TP+FP+FN+TN). * **PRECISION** Precision is the ratio of correctly identified +ve subjects by test, against all +ve subjects identified by test. Precision = (true positives / predicted positives) = TP/(TP+FP). This metric is often used in cases where classification of true positives is a priority. For example, a spam email classifier would rather classify some spam emails as regular emails rather than classify some regular emails as spam. Thats why some spam emails end up in your main inbox, just to be safe. (Here true positives are the spam emails) * **SENSITIVITY (RECALL)** Sensitivity is the ratio of correctly identified +ve subjects by test against all +ve subjects in reality. Sensitivity = (true positives / all actual positives)= TP/(TP+FN). This metric is often used in cases where classification of false negatives is a priority. A good example is the medical test that we used for illustration above. The government would rather have some healthy people labeled +ve than have an infected individual labeled -ve and spread the disease. We would rather be overly cautious and have false positives than risk wrongly identifying false negatives. * **SPECIFICITY** Specificity is the ratio of correctly identified -ve subjects by test against all -ve subjects in reality. Specificity = (true negatives / all actual negatives) = TN/(TN+FP). This metric is often used in cases where classification of true negatives is a priority. For example, a doping test will immediately ban an athlete if they are tested positive. We would not want to any drug-free athlete to be wrongly classified and banned. * **F1 SCORE** F1 Score accounts for both precision and sensitivity. F1 Score = 2 * (Recall * Precision)/(Recall + Precision) It is often considered a better indicator of a classifiers performance than a regular accuracy measure as it compensates for uneven class distribution in the training dataset. For example, an uneven class distribution is likely to occur in insurance fraud detection, where a large majority of claims are legitimate and only a very small minority are fraudulent. **Which metric to use is depends on the problem in hand**"
Technical,ML,Why do we need confusion matrix?,"We can not rely on a single value of accuracy in classification when the classes are imbalanced. * For example, we have a dataset of 100 patients in which 5 have diabetes and 95 are healthy. However, if our model only predicts the majority class i.e. all 100 people are healthy then also we will have a classification accuracy of 95%. * Confusion matrices are used to visualize important predictive analytics like recall, specificity, accuracy, and precision. * Confusion matrices are useful because they give direct comparisons of values like True Positives, False Positives, True Negatives and False Negatives."
Technical,Statistics & Probability,Explain collinearity and technique to reduce it?,"In statistics collinearity or multicollinearity is the phenomenon where one or more predictive variables(features) in multiple regression models are highly linearly related to each other. Technique to reduce multicollearity * **Remove highly correlated predictors from the model**. If you have two or more factors with a high collinearity, remove one from the model. Because they supply redundant information, removing one of the correlated factors usually doesn't drastically reduce the R-squared. Consider using stepwise regression, best subsets regression, or specialized knowledge of the data set to remove these variables. Select the model that has the highest R-squared value. * **Principal Components Analysis(PCA)** regression methods that cut the number of predictors to a smaller set of uncorrelated components. # Difference between statistics and machine learning * The major difference between machine learning and statistics is their purpose. Machine learning models are designed to make the most accurate predictions possible. Statistical models are designed for inference about the relationships between variables. * Statistics is mathematical study of data. Lots of statistical models that can make predictions, but predictive accuracy is not their strength."
Technical,Statistics & Probability,"In a test, students in section A scored with a mean of 75 and standard deviation of 10, while students in section B scored with a mean of 80 and standard deviation of 12? Melissa from section A and Ryan from section B both have scored 90 in this test. Who had a better performance in this test as compared to their classmates?","To compare the two scores we need to standardize them to the same scale. We do that by calculating the Z score, which allows us to compare the 2 scores in units of standard deviations. ```Z score= (X- mean)/Standard Deviation``` Melissa's Z score = (90-75)/10 = 1.5 Ryan's Z score = (90-80)/12 = 0.83 Melissa has performed better."
Technical,Statistics & Probability,What is null hypothesis and alternate hypothesis?,"The null hypothesis states that a population parameter (such as the mean, the standard deviation, and so on) is equal to a hypothesized value. The null hypothesis is often an initial claim that is based on previous analyses or specialized knowledge. * The alternative hypothesis states that a population parameter is smaller, greater, or different than the hypothesized value in the null hypothesis. The alternative hypothesis is what you might believe to be true or hope to prove true. * So when running a hypothesis test/experiment, the null hypothesis says that there is no difference or no change between the two tests. The alternate hypothesis is the opposite of the null hypothesis and states that there is a difference between the two tests."
Technical,Statistics & Probability,What is a hypothesis test and p-value?,"A hypothesis test examines two opposing hypotheses about a population: the null hypothesis and the alternative hypothesis. The null hypothesis is the statement being tested. Usually the null hypothesis is a statement of ""no effect"" or ""no difference"". The alternative hypothesis is the statement you want to be able to conclude is true based on evidence provided by the sample data. * Based on the sample data, the test determines whether to reject the null hypothesis. You use a p-value, to make the determination. If the p-value is less than the significance level (denoted as alpha), then you can reject the null hypothesis. * In laymans term the p-value is the probability that the null hypothesis is true. * Consider the example where we are trying to test whether a new marketing campaign generates more revenue * Here null hypothesis states that there is no change in the revenue as a result of the new marketing campaign * Based on p-value we can accept or reject the null hypothesis. 0.25 p-value means there is 25% chance that new marketing campaign will not change revenue * Lower the p-value, the more confident we are that the alternate hypothesis is true, which, in this case, means that the new marketing campaign causes an increase or decrease in revenue. * In most fields, acceptable p-values should be under 0.05 while in other fields a p-value of under 0.01 is required. * So when a result has a p-value of 0.05 or lower we can reject null hypothesis and accept the alternate hypothesis. ## More Info: Basic Concepts of Hypothesis Testing * In simple term hypothesis is a assumption. Since its assumption, after our testing it can hold true may not. * If our assumption holds true after testing then it is termed as 'Null Hypothesis' unless there is evidence against it. * If our assumption dont hold true and there is claim aganist it, then it is termed as alternate hypothesis. * So when our assumption dont hold true then Type I error occurs. Here we are going against the Null Hypothesis. * p-value: It is calculated probability of making type I error * Reference: https://www.youtube.com/watch?v=d0eVIUyt_Uc"
Technical,Statistics & Probability,What is power of hypothesis test? Why is it important?,"Remember that if actual value is positive and our model predicts it as negative then Type II error occuras (False negative). e.g. Calling a guilty person innocent, diaognosing cancer infected person as healthy etc. * The probability of not commiting Type II error is called as power of hypothesis test. The higher probability we have of not commiting a type 2 error, the better our hypothesis test is."
Technical,ML,What is the difference betweeen K nearest neighbors and K means,"KNN or K nearest neighbor is a classification algorithm, while K-Means is clustering technique. * KNN is supervised algorithm, K means is unsupervised algorithm. * In KNN prediction of the test sample is based on the similarity of its features to its neighbors. The similarity is computed based on the measure such as euclidean distance. Here K referes to the number of neighbors with whom similarity is being compared. * K-means is the process of defining clusters or groups around predefined centroids based on the similarity of each data point to each other. Here K referes to the number of centroids around which clusters will be formed. # Explain Random forest algorithm * Random forest is supervised learning algorithm and can be used to solve classification and regression problems. * Since decision-tree create only one tree to fit the dataset, it may cause overfitting and model may not generalize well. Unlike decision tree random forest fits multiple decision trees on various sub samples of dataset and make the predictions by averaging the predictions from each tree. * Averaging the results from multiple decision trees help to control the overfitting and results in much better prediction accuracy. As you may have noticed, since this algorithm uses multiple trees hence the name Random Forest * Reference: [Random Forest](https://satishgunjal.com/random_forest/) * This algorithm is heavily used in various industries such as Banking and e-commerce to predict behavior and outcomes."
Technical,ML,Can Random Forest Algorithm be used both for Continuous and Categorical Target Variables?,"Yes, Random Forest can be used for both continuous and categorical target (dependent) variables. * In a random forest the classification model refers to the categorical dependent variable, and the regression model refers to the numeric or continuous dependent variable."
Technical,ML,What do you mean by Bagging?,"In bagging we build independent estimators on different samples of the original data set and average or vote across all the predictions. * Bagging is a short form of **Bootstrap Aggregating**. It is an ensemble learning approach used to improve the stability and accuracy of machine learning algorithms. * Since multiple model predictions are averaged together to form the final predictions, Bagging reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. * Bagging is a special case of the model averaging approach, in case of regression problem we take mean of the output and in case of classification we take the majority vote. * Bagging is more helpfull if we have over fitting (high variance) base models. * We can also build independent estimators of same type on each subset. These independent estimators also enable us to parallelly process and increase the speed. * Most popular bagging estimator is 'Bagging Tress' also knows as 'Random Forest' ## Bootstrapping * It is a resampling technique, where large numbers of smaller samples of the same size are repeatedly drawn, with replacement, from a single original sample. * So this technique will enable us to produce as many subsample as we required from the original training data. * The defination is simple to understand, but ""replacement"" word may be confusing sometimes. Here 'replacement' word signifies that the same obervation may repeat more than once in a given sample, and hence this technique is also known as **sampleing with replacement** . As you can see in above image we have training data with observations from X1 to X10. In first bootstrap training sample X6, X10 and X2 are repeated where as in second training sample X3, X4, X7 and X9 are repeated. * Bootstrap sampling helps us to generate random sample from given training data for each model in order to genralise the final estimation. * So in case of Bagging we create multiple number of bootstrap samples from given data to train our base models. Each sample will contain training and test data sets which are different from each other and remember that training sample may contain duplicate observations."
Technical,ML,What is Out-of-Bag Error in Random Forests?,"Out-of-Bag is equivalent to validation or test data but it is calculated internally by Random Forest algorithm. In case of Sklearn if we set hyperparameter 'oob_score = True' then Out-of-Bag score will be calculated for every decision tree. * Finally, we aggregate all the errors from all the decision trees and we will determine the overall OOB error rate for the classification."
Technical,ML,What is the use of proximity matrix in the random forest algorithm?,A proximity matrix is used for the following cases : * Missing value imputation * Detection of outliers.
Technical,ML,List down the parameters used to fine-tune the Random Forest,Two parameters that have to fine-tune to improve the predictions that are important in the random forest algorithm are as follows: * Number of trees used in the forest (n_tree) * Number of random variables used in each of the trees in the forest (mtry)
Technical,ML,What is K Fold cross validation? Why do you use it?,"In case of K Fold cross validation input data is divided into K number of folds, hence the name K Fold. Suppose we have divided data into 5 folds i.e. K=5. Now we have 5 sets of data to train and test our model. So the model will get trained and tested 5 times, but for every iteration we will use one fold as test data and rest all as training data. Note that for every iteration, data in training and test fold changes which adds to the effectiveness of this method. * This significantly reduces underfitting as we are using most of the data for training(fitting), and also significantly reduces overfitting as most of the data is also being used in validation set. * K Fold cross validation helps to generalize the machine learning model, which results in better predictions on unknown data."
Technical,ML,How to handle missing data?,"Data can be missing because of mannual error or can be gennualy missing. * Delete low quality records completely which have too much missing data * Impute the values by educated guess, taking average or regression * Use domain knwledge to impute values"
Technical,ML,What is the difference between Bar graph and histogram?,Bar graph is used for descreate data where as histogram is used for continuous data. * In bar graph there is space between the bars and in case of histogram there is no space between the bars(contnuous scale). * In bar graph the order of the bars can be changed and in histogram order remains same.
Technical,ML,What is the Box and Whisker plot? When should use it?,"Box and whisker plots are ideal for comparing distributions because the centre, spread and overall range are immediately apparent. * A box and whisker plot is a way of summarizing a set of data measured on an interval scale. * It is often used in explanatory data analysis * Boxplots are a standardized way of displaying the distribution of data based on a five number summary (minimum, first quartile (Q1), median, third quartile (Q3), and maximum). * median (Q2/50th Percentile): the middle value of the dataset. * first quartile (Q1/25th Percentile): the middle number between the smallest number (not the minimum) and the median of the dataset. * third quartile (Q3/75th Percentile): the middle value between the median and the highest value (not the maximum) of the dataset. * interquartile range (IQR): 25th to the 75th percentile. * whiskers (shown in blue) * outliers (shown as green circles) * maximum: Q3 + 1.5*IQR * minimum: Q1 -1.5*IQR"
Technical,ML,What is outlier? How to handle them?,"An outlier is an observation that lies an abnormal distance from other values in a random sample from a population. * Data points above and below 1.5*IQR, are most commonly outliers. Outliers can drastically change the results of the data analysis and statistical modeling. ## Types of the outliers * **Data entry errors** * **Measuremental errors** * **Intentional outliers**. This is commonly found in self-reported measures that involves sensitive data. For example: Teens would typically under report the amount of alcohol that they consume. * **Data processing erros**. Whenever we perform data mining, we extract data from multiple sources. It is possible that some manipulation or extraction errors may lead to outliers in the dataset. * **Sampling error**. For instance, we have to measure the height of athletes. By mistake, we include a few basketball players in the sample. This inclusion is likely to cause outliers in the dataset. * **Natutal oulier**. When an outlier is not artificial (due to error), it is a natural outlier. For instance: In my problem assignment with one of the renowned insurance company, I noticed that the performance of top 50 financial advisors was far higher than rest of the population. Surprisingly, it was not due to any error. Hence, whenever we perform any data mining activity with advisors, we used to treat this segment separately. How to detect Outliers? Most commonly used method to detect outliers is visualization. * We use various visualization methods, like Box-plot, Histogram, Scatter Plot * Use capping methods. Any value which out of range of 5th and 95th percentile can be considered as outlier * Data points, three or more standard deviation away from mean are considered outlier Apart from visualization we can also use Z-Score or Extreme Value Analysis (parametric) to detect outliers. How to detect Outliers? Most commonly used method to detect outliers is visualization. * We use various visualization methods, like Box-plot, Histogram, Scatter Plot * Use capping methods. Any value which out of range of 5th and 95th percentile can be considered as outlier * Data points, three or more standard deviation away from mean are considered outlier Apart from visualization we can also use Z-Score or Extreme Value Analysis (parametric) to detect outliers. How to remove outliers? Most of the methods used to handle missing values are aslo application in case ot outliers ### Deleting observations We delete outlier values if it is due to data entry error, data processing error or outlier observations are very small in numbers. We can also use trimming at both ends to remove outliers. ### Transforming and binning values Transforming variables can also eliminate outliers. Natural log of a value reduces the variation caused by extreme values. Binning is also a form of variable transformation. Decision Tree algorithm allows to deal with outliers well due to binning of variable. ### Imputing We can use mean, median, mode imputation methods. ### Treat separately If there are significant number of outliers, we should treat them separately in the statistical model. One of the approach is to treat both groups as two different groups and build individual model for both groups and then combine the output."
Technical,ML,"If deleting outliers is not an option, how will you handle them?","I will try differen models. Data detected as outliers by linear model, can be fit by non-linear model. * Try normalizing the data, this way the extreame datapoints are pulled to the similar range. * We can use algorithms which are less affected by outliers. * We can also create separate model to handle the outlier data points."
Technical,ML,You fit two linear models on a dataset. Model 1 has 25 predictors and model 2 has 10 predictors. What performance metric would you use to select the best model based on training dataset?,"First of all model performace is not directly proportional to the number of predictors, so we cant say that model with 25 predictors is better than the model with 10 predictors * Here important thing is to understand different evaluation metric for linear regresion and which one of them can help us identify the impact of number of predictors on model performance. * Evaluation metric used for linear regression are MSE, MAE, R-squared, Adjusted R-squared, and RMSE. * MSE penalizes large errors, MAE does not penalize large errors, RMSE penalizes large errors and R-squared or Coefficient of Determination represent the strength of the relationship between your model and the dependent variable. * Though R-squared represent the strength of relationship between model and the dependent variables, it is never used for comparing the models as the value of R increases with the increase in the number of predictors (even if these predictors do not add any value to the model) * Now only remaining metric is **Adjusted R-squared**. Unlike R-squared, Adjusted R-squared measures variation explained by only the independent variables that actually affect the dependent variable. * So the Adjusted R-squre score will increase only if addition of predictors improve the models performance significantly or else it will decrease. Hence correct answer is Adjusted R-squared."
Technical,ML,Suppose we have a function -4x^2 + 4x + 3. Find the maximum or minimum of this function.,"This is quadratic equation, f(x) = -4x^2 + 4x + 13 (for a function: ax^2 + bx + c, when a < 0, then function has maximum value) * To find the slope of the function, lets take derivative of it f'(x)= -8x + 4 * At maximum point, slope will be 0 -8x + 4 = 0 x = 0.5 * Now lets put 0.5 in equation to find the maximum values f(0,5) = -4(0.5)^2 + 4(0.5) + 13 = -1 + 2 +13 = 14 * This functiona will have concave shape. So the maximum point is (0.5, 14)"
Technical,ML,What is stepwise regression?,"Stepwise regression is a method of fitting regression models in which the choice of predictive variables is carried out by an automatic procedure. In each step, a variable is considered for addition to or subtraction from the set of explanatory variables based on some prespecified criterion. Stepwise regression is classified into backward and forward selection. * **Backward selection** starts with a full model, then step by step we reduce the regressor variables and find the model with the least RSS, largest R, or the least MSE. The variables to drop would be the ones with high p-values. * **Forward selection** starts with a null model, then step by step we increase the regressor variables until we can no longer improve the error performance of the model. We usually pick the model with the highest adjusted R."
Technical,ML,You have two buckets - one of 3 liters and other of 5 liters. You are expected to mesure exactly 4 liters. How will you complete the task? Note: There is no thrid bucket,"Questions like this will test your out of the box thinking * Step1: Fill 5 lts bucket and empty it in 3 ltr bucket. Now we are left with 2 ltr in 5 ltr bucket. * Step2: Empty 3 ltr bucket and pour the contents of 5 ltr bucket in 3 ltr bucket. Now our 5 ltr bucket is empty and 3 ltr bucket has 2 ltr content in it. * Now fill the 5 ltr bucket again. Remember that our 3 ltr bucket has 2 ltr content in it, so if we pour 1 ltr content from 5 ltr bucket to 3 ltr bucket we are left with 4 ltr content in 5 ltr bucket."
Technical,ML,Lis the differences between supervised and unsupervised learning,"| Supervised learning | Unsupervised leanring |:- |:- Uses labeled data as input | Uses unlabeled data as input Supervised learning has feedback mechanism | Unsupervised learning has no feedback mechanism Common supervised learning algorithms are decision tree, logistic regression, support vector machine etc | K Means clustering, hierarchical clustering etc"
Technical,ML,Explain the steps in making decision tree?,Below are the common steps in decision tree algorithm * Take the entire data as input * At the root node decision tree selects feature to split the data in two major categories. * Different criteria will be used to split the data. We generally use 'entropy' or 'gini' in case of classification and 'mse' or 'mae' in case of regression problems. * Features are selected for spliting based on highest information gain. * After every split we get decision rules and sub trees. * This process will continue until every training example is grouped together or maxinum allowed tree depth is reached. * So at the end of decision tree we end up with leaf node. Which represent the class or a continuous value that we are trying predict
Technical,ML,How do you build random forest model?,Random forest is made up of multiple decision trees. Unlike decision tree random forest fits multiple decision trees on various sub samples of dataset and make the predictions by averaging the predictions from each tree. * Select few random sub sample from given dataset * Construct a decision tree for every sub sample and predict the result. * Perform the voting on prediction from each tree. * At the end select the most voted result as final prediction.
Technical,ML,How do Random Forest handle missing data?,"Random Forests inherently have two primary ways of handling missing data: 1. **During Training (Building the Trees):** * **For Numerical Features:** Missing values can be imputed using simple strategies like mean or median. * **For Categorical Features:** A new ""missing"" category is often created to handle missing values. This ensures that data points with missing categorical values are still considered during the tree building process. 2. **During Prediction (Making New Predictions):** * **""Surrogate"" Splits:** Each tree in the forest stores ""surrogate"" splits along with the primary split at each node. Surrogate splits are based on other features that are highly correlated with the primary split feature. If a new data point encounters a missing value during prediction, the tree will use the surrogate split to guide the data point down the appropriate branch. * **Proximity Measures:** Random Forests also calculate ""proximity measures"" between data points based on how often they end up in the same leaf nodes across all the trees. These proximities can be used to impute missing values by taking a weighted average of the values from similar data points. *Need to review below answer....* Note that handling missing data is one of the advantages of Random Forest algorithm over Decision tree. Please refer below diagram where we have training data set of circle, square and triangle of color red, green and blue respectively. There are total 27 training examples. Random forest will create three sub sample of 9 training examples each * Random forest algorithm will create three different decision tree for each sub sample * Notice that each tree uses different criteria to split the data * Now it is straight forward analysis for the algorithm to predict the shape of given figure if its shape and color is known. Lets check the predictions of each tree for blue color triangle, (here shape input is missing) * Tree 1 will predict: triangle * Tree 2 will predict: square * Tree 2 will predict: triangle Since the majority of voting is for triangle final prediction is triangle shape * Now, lets check predictions for circle with no color defined (color attribute is missing here) * Tree 1 will predict: triangle * Tree 2 will predict: circle * Tree 2 will predict: circle Since the majority of voting is for circle final prediction is circle shape Please note this is over simplified example, but you get an idea how multiple tree with different split criteria helps to handle missing features."
Technical,ML,What is model overfitting? How can you avoid it?,"Overfitting occurs when your model learns too much from training data and isn't able to generalize the underlying information. When this happens, the model is able to describe training data very accurately but loses precision on every dataset it has not been trained on. Below images represent the overfitting linear and logistic regression models. **How To Avoid Overfitting?** * Since overfitting algorithm captures the noise in data, reducing the number of features will help. We can manually select only important features or can use model selection algorithm for same * We can also use the Regularization technique. It works well when we have lots of slightly useful features. Sklearn linear model(Ridge and LASSO) uses regularization parameter alpha to control the size of the coefficients by imposing a penalty. * K-fold cross validation. In this technique we divide the training data in multiple batches and use each batch for training and testing the model. * Increasing the training data also helps to avoid overfitting."
Technical,ML,There are 9 balls out of which one ball is heavy in weight and rest are of the same weight. In how many minimum weightings will you find the heavier ball?,"To find the heavier ball among 9 balls using a balance scale, you can determine the minimum number of weighings required by strategically dividing the balls and comparing their weights. ### Step-by-Step Solution: 1. **First Weighing**: - Divide the 9 balls into three groups of 3 balls each: Group A, Group B, and Group C. - Weigh Group A against Group B. 2. **Analyzing the First Weighing**: - **Case 1**: If the scales balance (i.e., Group A = Group B), it means the heavier ball is in Group C. - **Case 2**: If the scales do not balance (i.e., Group A Group B), the heavier ball is in the group that tips the scale. 3. **Second Weighing**: - You now have 3 balls (either all from Group C in Case 1, or from the heavier group in Case 2). - Take 2 of these 3 balls and weigh them against each other. 4. **Analyzing the Second Weighing**: - **Case 1**: If the scales balance, the heavier ball is the one that was not weighed. - **Case 2**: If the scales do not balance, the heavier ball is the one that tips the scale. ### Conclusion: The minimum number of weighings required to find the heavier ball among the 9 balls is **2**. By dividing the balls into three groups and strategically using the balance scale, you can ensure that you find the heavier ball in just two weighings."
Technical,ML,What are feature selection methods to select right variables?,"Feature selection is the process of reducing the number of input variables when developing a predictive model. There are two methods for feature selection. Filter method and wrapper methods. Best analogy for selecting features is bad data in bad answers out. ## Filter Methods * Filter feature selection methods use statistical techniques to evaluate the relationship between each input variable and the target variable, and these scores are used as the basis to choose (filter) those input variables that will be used in the model. * These methods are faster and less computationally expensive than wrapper methods. #### Information Gain Information gain calculates the reduction in entropy from the transformation of a dataset. It can be used for feature selection by evaluating the Information gain of each variable in the context of the target variable. #### Chi-square Test The Chi-square test is used for categorical features in a dataset. We calculate Chi-square between each feature and the target and select the desired number of features with the best Chi-square scores. #### Correlation Coefficient Correlation is a measure of the linear relationship of 2 or more variables. Through correlation, we can predict one variable from the other. The logic behind using correlation for feature selection is that the good variables are highly correlated with the target. Furthermore, variables should be correlated with the target but should be uncorrelated among themselves. ## Wrapper Methods * Wrapper feature selection methods create many models with different subsets of input features and select those features that result in the best performing model according to a performance metric. * These methods are unconcerned with the variable types, although they can be computationally expensive. * The wrapper methods usually result in better predictive accuracy than filter methods. #### Forward Feature Selection This is an iterative method wherein we start with the best performing variable against the target. Next, we select another variable that gives the best performance in combination with the first selected variable. This process continues until the preset criterion is achieved. #### Backward Feature Elimination This method works exactly opposite to the Forward Feature Selection method. Here, we start with all the features available and build a model. Next, we remove the variable from the model which gives the best evaluation measure value. This process is continued until the preset criterion is achieved. #### Exhaustive Feature Selection This is the most robust feature selection method covered so far. This is a brute-force evaluation of each feature subset. This means that it tries every possible combination of the variables and returns the best performing subset."
Technical,ML,You are given a dataset consisting of variables having more than 30% missing values? How will you deal with them?,"There are multiple ways to handle missing values in the data * If dataset is huge we can simply remove the rows containing the missing data * If dataset is small then we have to impute the missing values. There are multiple ways to impute the missing values. In case of categorical data we may use the most common values and in case numerical data we can use mean, median etc."
Technical,ML,How should you maintain your deployed model?,### Monitor Constant monitoring of all the models is needed to determine the performance accuracy of the models ### Evaluate Evaluation metric of the current model is calculated to determine if new algorithm is needed. ### Compare The new models are compared against each other to determine which model performs the best. ### Rebuild The best performing model is re-built on the current set of data.
Technical,ML,What are recommender systems?,"The purpose of a recommender system is to suggest relevant items or services to users. * Two major categories of recommender systems are collaboarative filtering and cotent based filtering methods ### Collaborative Filtering * It is based on the past interactions recorded between users and items in order to produce new recommendations. * e.g. Music service recommends track that are often played by other users with similar interests ### Content Based Filtering * Unlike collaborative methods that only rely on the user-item interactions, content based approaches use additional information about the content consumed by the user to produce new recommendations * e.g. Music service recommends new song based on properties of the songs user listens to."
Technical,ML,"People who bought this, also bought...'recommendations seen on Amazon is a result of which algorithm?",Its done by recommendation system using collaborative filtering approach. * In case of collaborative filtering past interactions recorded between users and items are used to produce new recommendations.
Technical,Statistics & Probability,"If it rains on saturday with probability 0.6, and it rains on sunday with probability 0.2, what is the probability that it rains this weekend?","* Since we know the probability of rain on Saturday and Sunday, the probability of raining on Weekend is combination of both of these events. * Trick here is to know the probability of not raining on Saturday and Sunday. * If we subtract the intersection() of both the events of not raining on Saturday and Sunday from total probability then we get the probability of raining on weekend. ``` = Total probability - (Probability that it will not rain on Saturday) (Probability that it will not rain on Sunday) = 1 - (1 - 0.6)*(1 - 0.2) = 0.68 ``` *"
Technical,ML,How can you select K for K-Means?,"There are two ways to select the number of clusters in case K-Means clustering algorithm ### Visualization * To find the number of clusters manually by data visualization is one of the most common method. * Domain knowledge and proper understanding of given data also help to make more informed decisions. * Since its manual exercise there is always a scope for ambiguous observations, in such cases we can also use Elbow Method ### Elbow Method * In Elbow method we run the K-Means algorithm multiple times over a loop, with an increasing number of cluster choice(say from 1 to 10) and then plotting a clustering score as a function of the number of clusters. * Clustering score is nothing but sum of squared distances of samples to their closest cluster center. * Elbow is the point on the plot where clustering score (distortion) slows down, and the value of cluster at that point gives us the optimum number of clusters to have. * But sometimes we dont get clear elbow point on the plot, in such cases its very hard to finalize the number of clusters."
Technical,ML,"Explain dimensionality reduction, and its benefits?",Dimensionality reduction referes to the process of converting a set of data having vast dimensions into data with lesser dimensions(features) to convey similar information concisely. * It helps in data compressing and reducing the storage space * It reduces computation time as less dimensions lead to less computing * It removes redundant features. E.g. There is no point in storing value in two different units
Technical,Statistics & Probability,How can you say that the time series data is stationary?,"For accurate analysis and forecasting, **trend and seasonality is removed** from the time series and converted it into stationary series. Time series data is said to be stationary when statistical properties like mean, standard deviation are constant and there is no seasonality. In other words statistical properties of the time series data should not be a function of time."
Technical,ML,How can you calculate the accuracy using confusion matrix?,Accuracy = (True Positive + true Negative) / Total Obervations Write the equations for the precision and recall?
Technical,ML,Write the equations for the precision and recall?,Precision = True Positive / (True Positive + False Positive) Recall = True Positive /(Total Positive + False Negative)
Technical,ML,"If a drawer containes 12 red socks, 16 blue socks, and 20 white socks, how many must you pull out to be sure of having a amcthing pair?","* There are three colors of socks- Red, Blue and White. No of socks is irrelevant here. * Suppose in our first pull we picked Red color sock * In second pull we picked Blue color sock * And in third pull we picked White color sock. * Now in our fourth pull, if we pick any color, match is guaranteed!! So the answer is 4!"
Technical,ML,Which of the following machine learning algorithm can be used for imputing missing values of both categorical and continuos variables?,K-means clustering - Linear regression - K-NN - Decision tress ``` Using KNN we can compute the missing variable value by using the nearest neighbors.
Technical,Statistics & Probability,"Given a box of matches and two ropes, not necessarily identical, measure a period of 45 minutes?","Note: Ropes are not uniform in natire and rope takes exactly 60 minutes to completly burn out * We have two ropes A and B * Ligt A from both the end and B from one end * When A finished burning we know that 30 minutes have elapsed and B has 30 minutes remaining * Now light the other end of B also, it will now burnout in 15 minutes * This we got 30 + 15 = 45 minutes"
Technical,ML,"After studying the behaviour of population, you have identified four specific individual types who are valueable to your study. You would like find all users who are most similar to each indivdual type. Which algorithm is most approprate for this study?","The most appropriate algorithm for this study is **K-Nearest Neighbors (KNN)**. Here's why KNN is well-suited for this task: 1. **Similarity-Based:** KNN explicitly focuses on finding the most similar data points (users in this case) to a given point (your identified individual types) based on their features or attributes. 2. **No Assumption about Data Distribution:** KNN is a non-parametric algorithm, meaning it doesn't make any assumptions about the underlying distribution of your data. This is beneficial when you're unsure how the data is distributed or if it doesn't fit a specific statistical model. 3. **Easy to Interpret:** KNN is relatively easy to understand and interpret. You simply define the number of neighbors (k) and a distance metric (e.g., Euclidean distance), and the algorithm finds the k closest data points to each of your individual types. 4. **Flexible:** KNN can handle both numerical and categorical data, making it adaptable to different types of features that might be relevant in your study."
Technical,Statistics & Probability,Your organization has a website where visitors randomly receive one of the two coupons. It is also possible that visitors to the website will not receive the coupon. You have been asked to determine if offering a coupon to the visitors to your website has any impact on their purchase decision. Which analysis method should you use?,"In this scenario, the most appropriate analysis method would be **A/B Testing** (or **Split Testing**). **Here's why A/B Testing is the best fit:** * **Controlled Experiment:** A/B Testing allows you to randomly assign visitors to different groups (control group with no coupon, group A with coupon type 1, group B with coupon type 2). This controlled experiment ensures that any differences in purchase behavior can be directly attributed to the presence and type of coupon. * **Measures Impact:** You can directly compare the conversion rates (percentage of visitors who make a purchase) between the groups to see if offering a coupon has a statistically significant impact on purchase decisions. * **Compares Coupon Types:** Additionally, you can analyze if one coupon type is more effective than the other in driving purchases. **How to Implement A/B Testing:** 1. **Define the Hypothesis:** State clearly what you want to test. For example: * Null Hypothesis: Offering a coupon has no impact on the purchase decision. * Alternative Hypothesis: Offering a coupon increases the likelihood of a purchase. 2. **Set up the Experiment:** * Randomly assign website visitors to one of the three groups (control, coupon A, coupon B). * Ensure the experiment runs for a sufficient duration to collect enough data for statistically significant results. 3. **Collect Data:** * Track the conversion rates for each group. * Gather additional data like coupon usage, time spent on site, etc., for further analysis. 4. **Analyze the Results:** * Use statistical tests (e.g., Chi-squared test, t-test) to compare the conversion rates between the groups and determine if the differences are statistically significant. * Calculate metrics like uplift (increase in conversion rate due to the coupon) and confidence intervals to understand the impact of the coupons. 5. **Draw Conclusions and Take Action:** * Based on the analysis, decide if offering coupons is beneficial. * If so, identify the most effective coupon type and implement it in your marketing strategy. **Important Considerations:** * **Sample Size:** Ensure you have a large enough sample size in each group to achieve statistically significant results. * **External Factors:** Be mindful of external factors (e.g., seasonality, promotions by competitors) that could influence purchase behavior during the experiment. * **Ethical Considerations:** Be transparent with visitors about the experiment and obtain necessary consent if required. A/B Testing provides a robust and data-driven way to understand the impact of coupons on purchase decisions, enabling you to make informed marketing decisions and optimize your website's conversion rate."
Technical,ML,Explain Principal Componenet Analysis?,"* Principal Component Analysis (PCA) is dimensionality reduction method, that is used to reduce dimensionality of large data sets, by transforming large set of variables into a smaller one that still contains most of the information in large set. * Principal component analysis is a technique for **feature extraction** so it combines our input variables in a specific way, then we can drop the **least important** variables while still retaining the most valuable parts of all of the variables! As an added benefit, each of the new variables after PCA are all **independent** of one another * Reducing the number of the variables of the datset naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. * By reducing the dimension of your feature space, you have fewer relationships between variables to consider and you are less likely to overfit your model"
Technical,ML,Difference between standardisation and normalization?,"**Standardization** * **What it does:** * Centers the data around zero (mean = 0) * Scales the data to have a standard deviation of one (std = 1) * **Transformation:** * `Z = (X - mean) / std_dev` * **When to use it:** * Algorithms that are sensitive to the scale of features (e.g., linear regression, logistic regression, support vector machines). * When you assume your data follows a normal (Gaussian) distribution (though not strictly required). * When outliers are present, as standardization is less affected by them. ( It means it includes the effect of outliers in scaling process) **Normalization** * **What it does:** * Scales the data to a specific range, typically between 0 and 1 * **Transformation:** * `X_norm = (X - X_min) / (X_max - X_min)` * **When to use it:** * Algorithms that rely on distance calculations (e.g., k-nearest neighbors, k-means clustering). * When you don't know the distribution of your data. * When you want to avoid outliers having a disproportionate impact on the scaling.( It means it tries to limit the impact of outliers on scaling process) **Key differences in a nutshell:** * **Transformation:** Standardization uses mean and standard deviation. Normalization uses minimum and maximum values. * **Distribution Assumption:** Standardization often assumes a normal distribution. Normalization makes no assumptions about the distribution. * **Outlier Sensitivity:** Standardization is less sensitive to outliers. Normalization is more sensitive to outliers. * **Range:** Standardization has no fixed range. Normalization scales to a specific range (usually 0 to 1). **Choosing the right one:** * **No one-size-fits-all answer:** The best choice depends on your specific data, algorithm, and problem. * **Experimentation is key:** Try both techniques and compare the results to see which one works better for your particular case."
Technical,ML,What is meant by Data Leakage?,"Data Leakage is the scenario where the Machine Learning Model is already aware of some part of test data after training.This causes the problem of overfitting. * In Machine learning, Data Leakage refers to a mistake that is made by the creator of a machine learning model in which they accidentally share the information between the test and training data sets. * Data leakage is a serious and widespread problem in data mining and machine learning which needs to be handled well to obtain a robust and generalized predictive model. **Examples of data leakage** * The most obvious and easy-to-understand cause of data leakage is to include the target variable as a feature. What happens is that after including the target variable as a feature, our purpose of prediction got destroyed. This is likely to be done by mistake but while modelling any ML model, you have to make sure that the target variable is differentiated from the set of features. * Another common cause of data leakage is to include test data with training data. > Above two cases are not very likely to occur because they can easily be spotted while doing the modelling. Below are few data leakage examples that are hard to troubleshoot. * Presence of Giveaway features * Lets we are working on a problem statement in which we have to build a model that predicts a certain medical condition. If we have a feature that indicates whether a patient had a surgery related to that medical condition, then it causes data leakage and we should never be included that as a feature in the training data. The indication of surgery is highly predictive of the medical condition and would probably not be available in all cases. If we already know that a patient had a surgery related to a medical condition, then we may not even require a predictive model to start with. * Lets we are working on a problem statement in which we have to build a model that predicts if a user will stay on a website. Including features that expose the information about future visits will cause the problem of data leakage. So, we have to use only features about the current session because information about the future sessions is not generally available after we deployed our model. * Leakage during Data preprocessing * While solving a Machine learning problem statement, firstly we do the data cleaning and preprocessing which involves the following steps: * Evaluating the parameters for normalizing or rescaling features * Finding the minimum and maximum values of a particular feature * Normalize the particular feature in our dataset * Removing the outliers * Fill or completely remove the missing data in our dataset * The above-described steps should be done using only the training set. If we use the entire dataset to perform these operations, data leakage may occur. * Applying preprocessing techniques to the entire dataset will cause the model to learn not only the training set but also the test set. As we all know that the test set should be new and previously unseen for any model."
Technical,ML,How to detect Data Leakage?,"Results are too good too true * In general, if we see that the model which we build is too good to be true (i.,e gives predicted and actual output the same), then we should get suspicious and data leakage cannot be ruled out. * At that time, the model might be somehow memorizing the relations between feature and target instead of learning and generalizing it for the unseen data. * So, it is advised that before the testing, the prior documented results are weighed against the expected results. * Using EDA * While doing the Exploratory Data Analysis (EDA), we may detect features that are very highly correlated with the target variable. Of course, some features are more correlated than others but a surprisingly high correlation needs to be checked and handled carefully. * We should pay close attention to those features. So, with the help of EDA, we can examine the raw data through statistical and visualization tools. * High weight features * After the completion of the model training, if features are having very high weights, then we should pay close attention. Those features might be leaky."
Technical,ML,How to fix the problem of Data Leakage?,"The main culprit behind this is the way we split our dataset and when. The following steps can prove to be very crucial in preventing data leakage: * Select the features such a way that they do not contain information about the target variable, which is not naturally available at the time of prediction. * Create a Separate Validation Set * To minimize or avoid the problem of data leakage, we should try to set aside a validation set in addition to training and test sets if possible. * The purpose of the validation set is to mimic the real-life scenario and can be used as a final step. * By doing this type of activity, we will identify if there is any possible case of overfitting which in turn can act as a caution warning against deploying models that are expected to underperform in the production environment. * Apply Data preprocessing Separately to both Train and Test subsets * While dealing with neural networks, it is a common practice that we normalize our input data firstly before feeding it into the model. * Generally, data normalization is done by dividing the data by its mean value. More often than not, this normalization is applied to the overall data set, which influences the training set from the information of the test set and eventually it results in data leakage. * Hence, to avoid data leakage, we have to apply any normalization technique separately to both training and test subsets. * Problem with the Time-Series Type of data * When dealing with time-series data, we should pay more attention to data leakage. For example, if we somehow use data from the future when doing computations for current features or predictions, it is highly likely to end up with a leaked model. * It generally happens when the data is randomly split into train and test subsets. * So, when working with time-series data, we put a cutoff value on time which might be very useful, as it prevents us from getting any information after the time of prediction. **Target Leakage:** This happens when predictors (features) include information that's only available after the target variable is known. **Example:** Predicting customer churn based on whether they canceled their subscription. The act of canceling is only known after they've churned, making it a leaky predictor."
Technical,Statistics & Probability,What is selection bias?,"Selection bias is the bias introduced by the selection of individuals, groups, or data for analysis in such a way that proper randomization is not achieved, thereby ensuring that the sample obtained is not representative of the population intended to be analyzed. It is sometimes referred to as the **selection effect**. * **Sampling bias** is usually classified as a subtype of selection bias, sampling bias is a bias in which a sample is collected in such a way that some members of the intended population have a lower or higher sampling probability than others. * Due to sampling bias, the probability distribution in the collected dataset deviates from its true natural distribution, which may affect ML models performance."
Technical,Statistics & Probability,What does it mean when distribution is left skew or right skew?,"In a **right-skewed** distribution, the tail on the right side is longer. This means most of the data is clustered on the left, with a few unusually large values pulling the average higher. Think of income distribution - most people earn less, but a few very high earners skew the average upwards. In a **left-skewed** distribution, the tail on the left side is longer. This means most of the data is clustered on the right, with a few unusually small values pulling the average lower. An example could be exam scores where most students do well, but a few low scores bring down the average."
Technical,Statistics & Probability,# What does the distribution looks like for the average time spend watching youtube per day?,"The distribution of average time spent watching YouTube per day is likely to be right-skewed. This means that most people watch YouTube for a relatively short amount of time each day, while a smaller number of users watch for much longer durations. The tail of the distribution extends to the right, indicating the presence of these high-usage viewers."
Technical,ML,Expalin covariance and correlation,"Covariance and Correlation are two mathematical concepts which are commonly used in the field of probability and statistics. Both concepts describe the relationship between two variables. * Covariance indicates the **direction of the linear relationship between variables**. Correlation on the other hand measures both the **strength and direction of the linear relationship between two variables**. * In case of High correlation, two sets of data are strongly linked together - Correlation is Positive when the values increase together, and - Correlation is Negative when one value decreases as the other increases"
Technical,ML,What is regularization. Why it is usefull?,Regularization is the process of adding tunning parameter(penalty term) to a model to induce smoothness in order to prevent overfitting. * The tunning parameter controls the excessively fluctuating function in such a way that coefficients dont take extreame values. * There are two types of regularization as follows: - L1 Regularization or Lasso Regularization. L1 Regularization or Lasso Regularization adds a penalty to the error function. The penalty is the sum of the absolute values of weights. - L2 Regularization or Ridge Regularization. L2 Regularization or Ridge Regularization also adds a penalty to the error function. But the penalty here is the sum of the squared values of weights.
Technical,ML,What are confouding varaiables?,"In statistics, confounder is a variable that influences both the dependent variable and independent avriable. * If you are researeching whether a lack of exercise leads to weight gain. In this case 'lack of exercise' is independent variable and 'weight gain' is dependent variable. A confounding varaible in this case would be 'age' which affect both of these variables."
Technical,ML,Explain ROC curve and AUC,"ROC Curve (Receiver Operating Characteristic Curve)** and **AUC (Area Under the Curve)** are tools used to evaluate the performance of a classification model, particularly when you want to understand how well the model separates two classes, such as ""spam"" and ""not spam."" ### ROC Curve: 1. **What is the ROC Curve?** - The ROC Curve is a graph that shows the trade-off between the **True Positive Rate (TPR)** and the **False Positive Rate (FPR)** of a model at various thresholds. - **True Positive Rate (TPR)**, also known as **Recall**, tells us how many of the actual positive cases (e.g., actual spam emails) were correctly predicted by the model. - **False Positive Rate (FPR)** tells us how many of the actual negative cases (e.g., non-spam emails) were incorrectly predicted as positive by the model. 2. **How to Read the ROC Curve?** - The **X-axis** represents the False Positive Rate (FPR), and the **Y-axis** represents the True Positive Rate (TPR). - A perfect model would reach the top left corner of the graph (high TPR and low FPR), indicating it correctly identifies all positives and has no false positives. ### AUC (Area Under the Curve): 1. **What is AUC?** - **AUC** stands for **Area Under the ROC Curve**. It provides a single number summary of the ROC Curve, which measures the overall ability of the model to distinguish between positive and negative classes. - **AUC values range from 0 to 1**: - An **AUC of 0.5** means the model performs no better than random guessing. - An **AUC close to 1** means the model has excellent performance, correctly distinguishing between the classes almost all the time. Why are ROC and AUC Important? Performance Comparison**: ROC and AUC help compare different models or different settings of the same model to see which one performs better in distinguishing between classes. - **Threshold Independence**: Unlike accuracy, which depends on a specific threshold, the ROC Curve and AUC show how a model performs across all possible thresholds, giving a more complete picture of its performance. - **Handling Imbalanced Data**: They are particularly useful when the data is imbalanced (e.g., detecting fraud in banking transactions), as they focus on the ability to correctly classify the minority class. ### Summary: The **ROC Curve** helps visualize the trade-off between detecting true positives and avoiding false positives at various thresholds, while the **AUC** gives a single score that summarizes the model's overall ability to separate classes. A higher AUC indicates a better model for distinguishing between the two classes. For more detailed explaination please refer. https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc # Explain Precision-Recall Curve The **Precision-Recall Curve** is a tool used to evaluate the performance of a classification model, especially when dealing with imbalanced datasets where one class is much more common than the other. ### Key Concepts: 1. **Precision**: - Measures how many of the positive predictions made by the model are actually correct. - **Example**: If a model predicts 10 emails as spam and 8 are actually spam, the precision is 8 out of 10, or 80%. 2. **Recall**: - Measures how well the model finds all the actual positive cases. - **Example**: If there are 12 spam emails and the model correctly identifies 8 of them, the recall is 8 out of 12, or about 67%."
Technical,ML,What is the Precision-Recall Curve?,"- The Precision-Recall Curve is a graph that shows the trade-off between precision and recall at different thresholds for deciding whether something is positive (like spam) or negative (not spam). - **X-axis**: Recall (how many actual positives we caught). - **Y-axis**: Precision (how many of our positive predictions were correct). Why is it Useful? 1. **Better for Imbalanced Data**: Unlike accuracy, which can be misleading with imbalanced data, the Precision-Recall Curve focuses on the performance of predicting the positive class, which is often the class of interest. 2. **Visualize Trade-offs**: The curve helps you see the trade-off between precision and recall. For example, if you want a model that catches almost all spam (high recall), the precision might drop (more false positives). The curve helps you find a good balance. 3. **Optimize Model Performance**: By looking at the curve, you can choose the best threshold that balances precision and recall for your specific needs. For example, in medical tests, you might want high recall to catch as many diseases as possible, even if it means more false alarms. ### Summary: The **Precision-Recall Curve** is a visual tool that helps you understand how well a model identifies the positive cases (like spam or a disease) and balances making correct predictions with avoiding false positives, especially in situations where one class is much rarer than the other."
Technical,Statistics & Probability,What is TF-IDF?,"TF-IDF (Term Frequency-Inverse Document Frequency)** is a method used in text analysis to determine how important a word is in a specific document compared to a whole collection of documents (called a corpus). It helps in identifying words that are most relevant to the content of a document. ### Key Concepts: 1. **Term Frequency (TF)**: - This measures how often a word appears in a document. A higher term frequency means that the word is more significant within that document. - **Simple Example**: If the word ""cat"" appears 3 times in a document with 100 words, the term frequency for ""cat"" is 3/100 = 0.03. 2. **Inverse Document Frequency (IDF)**: - This measures how rare or unique a word is across all documents in the corpus. A word that appears in many documents will have a low IDF, while a word that appears in only a few documents will have a high IDF. - **Simple Example**: If the word ""cat"" appears in only 1 out of 100 documents, its IDF is higher because its less common across the corpus. 3. **TF-IDF Score**: - This is the combination of TF and IDF. It gives a score that indicates how important a word is in a document, adjusted for how common it is across the entire corpus. - Words that are frequently used in a document but are rare in other documents get higher scores, highlighting their importance for that specific document. ### Why TF-IDF is Useful: - **Identifies Key Terms**: TF-IDF helps in identifying the most important words in a document, which are often key topics or themes. - **Improves Search Results**: In search engines, TF-IDF can be used to rank documents based on their relevance to a user's query by focusing on unique and significant words. - **Text Classification and Clustering**: It helps in categorizing or grouping documents by identifying unique terms that define different categories. ### In Simple Terms: TF-IDF helps find words that are important in a document but not too common across other documents. Its a tool to highlight what makes a document unique, which is useful for search engines, text analysis, and understanding the content better."
Technical,ML,What are Eigenvectors and Eigenvalues?,"* In linear algebra, an **eigenvector** is a special vector that, when a linear transformation is applied to it, only changes in scale (gets stretched or shrunk) but not in direction. * The **eigenvalue** associated with that eigenvector is the factor by which it is scaled. **Why are they important?** Eigenvectors and eigenvalues reveal the underlying structure and behavior of linear transformations. They have numerous applications across various fields: * **Image compression:** Eigenvectors can be used to represent images efficiently, leading to compression techniques like Principal Component Analysis (PCA). * **Facial recognition:** Eigenfaces, derived from eigenvectors, are used to represent and recognize faces. * **PageRank algorithm:** Google's PageRank algorithm uses eigenvectors to rank web pages based on their importance. * **Physics and engineering:** Eigenvalues and eigenvectors are used to analyze vibrations, stability, and other properties of systems. * **Machine learning:** They are used in dimensionality reduction techniques, clustering algorithms, and understanding the behavior of neural networks."
Technical,ML,Explain the scenario where both false positive and false negative are equally important,"1. **Medical Diagnosis (e.g., Cancer Screening)** * **False Positive:** A patient is told they have cancer when they don't. This leads to unnecessary anxiety, invasive procedures, and potential side effects from treatment. * **False Negative:** A patient with cancer is told they are healthy. This delays crucial treatment, potentially allowing the disease to progress and worsen the prognosis. 2. **Fraud Detection** * **False Positive:** A legitimate transaction is flagged as fraudulent. This inconveniences the customer, potentially disrupting their business or causing reputational damage. * **False Negative:** A fraudulent transaction goes undetected. This results in financial loss for the business or individual, and can enable further fraudulent activity."
Technical,ML,Why feature scalling is required in Gradient Descent Based Algorithms,"Machine learning algorithms like linear regression, logistic regression, neural network, etc. that use gradient descent as an optimization technique require data to be scaled. Take a look at the formula for gradient descent below: The presence of feature value X in the formula will affect the step size of the gradient descent. * The difference in ranges of features will cause different step sizes for each feature. * To ensure that the gradient descent moves smoothly towards the minima and that the steps for gradient descent are updated at the same rate for all the features, we scale the data before feeding it to the model. * Having features on a similar scale can help the gradient descent converge more quickly towards the minima."
Technical,ML,Why feature scalling is required in distance Based Algorithms,"Distance algorithms like KNN, K-means, and SVM are most affected by the range of features. This is because behind the scenes they are using distances between data points to determine their similarity. * For example, lets say we have data containing high school CGPA scores of students (ranging from 0 to 5) and their future incomes (in thousands Rupees): * Since both the features have different scales, there is a chance that higher weightage is given to features with higher magnitude. This will impact the performance of the machine learning algorithm and obviously, we do not want our algorithm to be biassed towards one feature. * Therefore, we scale our data before employing a distance based algorithm so that all the features contribute equally to the result. Scaling has brought both the features into the picture and the distances are now more comparable than they were before we applied scaling."
Technical,ML,Why feature scaling not required in tree based algorithms,"Imagine you're sorting a pile of apples and oranges into two baskets. You could sort them by color (red vs. not red) or by weight (heavy vs. light). * **Tree-based algorithms work like this:** They make decisions based on *thresholds* or *cut-offs* for each feature (like color or weight). They ask questions like: ""Is this fruit red?"" or ""Is this fruit heavier than 1 pound?"". * **Feature scaling doesn't matter here:** * **Color:** It doesn't matter if we represent ""red"" as the number 1 and ""not red"" as 0, or if we use some other scale. The decision (is it red or not?) remains the same. * **Weight:** Even if we change the units from pounds to kilograms, the relative heaviness of the fruits doesn't change. A heavy apple will still be heavier than a light orange, regardless of the units. **In simpler terms:** * Tree-based algorithms care about the *order* or *ranking* of the data, not the exact numerical values. * Scaling the features changes the numbers but doesn't change their order. A heavy fruit stays heavy, and a light fruit stays light, no matter what units we use. * This makes tree-based algorithms *invariant to monotonic transformations* of the features (transformations that preserve the order). **Therefore, feature scaling is generally not required for tree-based algorithms.** **Exceptions:** * **Some implementations:** Certain libraries or specific algorithms within the tree-based family might be sensitive to the scale of the features due to implementation details. It's always good to check the documentation or experiment to be sure. * **Distance-based calculations:** If your tree-based algorithm involves any calculations based on distances between data points, then scaling might be necessary to ensure all features contribute equally to the distance calculation. # Explain the difference between train, validation and test set * Training set is used for model training * Validation set is used for model fine tuning (tune the model's hyperparameters) * Test set is used for model testing. i.e. evaluating the models predictive power and generalization."
Technical,ML,What is Naive Bayes algorithm?,"The Naive Bayes algorithm is a simple but surprisingly effective classification algorithm in machine learning. It's based on Bayes' Theorem, which deals with conditional probabilities. **In simpler terms:** Imagine you're trying to decide if an email is spam or not. Naive Bayes looks at the words in the email and asks, ""If an email *is* spam, how likely is it to contain these words?"" It then does the same for non-spam emails. Finally, it compares these probabilities to make its best guess about whether the email is spam. **The ""Naive"" Assumption:** The key assumption here is that all the words in the email are *independent* of each other. This means it doesn't consider the order of the words or any relationships between them. It's a bit of a simplification, but it works surprisingly well in practice, especially for text classification tasks. **Why is it useful?** * **Simple and fast:** Naive Bayes is easy to understand and implement, and it can be trained very quickly on large datasets. * **Handles high-dimensional data:** It's good at dealing with lots of features (like all the different words in an email). * **Works well with text:** It's often used for tasks like spam filtering, sentiment analysis, and document classification. **Limitations:** * **The ""naive"" assumption:** In reality, features are often *not* completely independent, which can affect the accuracy in some cases. * **Sensitive to the absence of features:** If a feature is absent in the training data but appears in new data, it can throw off the model. **Overall:** Naive Bayes is a great starting point for classification tasks, especially when you need something simple, fast, and effective. While its ""naive"" assumption might seem limiting, it often performs surprisingly well, especially for text-related problems."
Technical,ML,What is the difference between MLOps and DevOps?,"MLOps & DevOps have a lot of things in common. However, DevOps include developing and deploying the software application code in production and this code is usually static and does not change rapidly. * MLOps on the other side also includes developing and deploying the ML code in production. However, here the data changes rapidly and the up-gradation of models has to happen more frequently than typical software application code."
Technical,ML,What are the risks associated with Data Science & how MLOps can overcome the same?,"In Data Science, several risks can impact projects, such as data quality issues, model deployment challenges, model performance degradation, lack of reproducibility, security concerns, and scalability issues. **MLOps (Machine Learning Operations)** helps mitigate these risks by: 1. **Automating Data Quality Checks and Monitoring**: Ensures consistent data quality and detects data drift, which helps maintain model accuracy over time. 2. **Streamlining Model Deployment and Environment Consistency**: Uses automated pipelines and containerization to deploy models smoothly across different environments, reducing errors and ensuring reliability. 3. **Continuous Monitoring and Automated Retraining**: Tracks model performance in real-time and triggers retraining when necessary, preventing performance degradation and adapting to new data patterns. 4. **Improving Reproducibility and Collaboration**: Incorporates version control for code, data, and models, making it easier to reproduce results and collaborate across teams. 5. **Enhancing Security and Compliance**: Implements strict access controls, auditing, and encryption to protect data and models, ensuring compliance with regulations. 6. **Optimizing Scalability and Resource Management**: Supports scalable infrastructure and provides insights into cost and resource use, ensuring models can handle large volumes and operate efficiently. By implementing MLOps, we can address these risks effectively, leading to more robust, reliable, and scalable machine-learning models."
Technical,Statistics & Probability,Please explain p-value to someone non-technical,"A p-value is a number that helps us understand if the results we see in an experiment or study are meaningful or if they might have happened just by chance. In simpler terms:The p-value is like a ""surprise meter"". The lower the p-value, the more surprised you'd be to see your results if the ""normal"" assumption were true.It helps you decide whether your results are strong enough to challenge the ""normal"" assumption or if they could just be due to random luck.Important Note:The p-value doesn't prove anything. It just gives you a measure of how surprising your results are.It's up to you to decide what level of ""surprise"" is enough to make you doubt the ""normal"" assumption. If the p-value is less than or equal to alpha(0.05), it means your results are statistically significant, and you have enough evidence to reject the null hypothesis. In other words, your data suggests that the ""normal"" assumption is probably not true."
Technical,ML,Explain the difference between L1 and L2 regularization and when you'd use each.,"In my graduate research project at Stanford, I compared L1 and L2 regularization on a high-dimensional text classification problem. L1 regularization (Lasso) helped create a sparse model by zeroing out less important features, reducing our feature set from 10,000 to 800 most relevant terms. This improved both model interpretability and runtime performance. With L2 regularization (Ridge), we maintained all features but prevented any single feature from dominating the model, which worked better for our image classification tasks where feature interactions were important. The key insight was that L1 works better for feature selection, while L2 is preferable when all features contribute to some degree."
Technical,ML,How would you detect and handle outliers in a dataset?,"During my internship at Tesla, I worked with sensor data that often contained outliers due to measurement errors. I implemented a multi-step approach: First, I used visualization techniques (box plots and scatter plots) to identify obvious anomalies. Then, I applied the IQR method to flag values beyond 1.5 times the interquartile range. For multivariate outliers, I used Mahalanobis distance. Rather than automatically removing outliers, I created separate models with and without them to understand their impact. This careful approach helped identify a sensor calibration issue that, when fixed, improved our model's accuracy by 25%."
Technical,ML,Design a simple recommendation system for an online bookstore.,"I'd start with a collaborative filtering approach using purchase history. First, I'd create a user-item matrix where each cell represents a user's rating or purchase of a book. To handle the sparse matrix, I'd use matrix factorization techniques like SVD to reduce dimensionality to about 100 latent factors. For new users with no history (cold start), I'd implement content-based filtering using book metadata (genre, author, keywords). The system would run daily batch updates using Apache Spark for scalability. I'd store the recommendations in Redis for fast retrieval, with a TTL of 24 hours. To evaluate performance, I'd track metrics like click-through rate and conversion rate."
Behavioral,Soft Skills,Tell me about a time when you had to learn a new technical skill quickly.,"During my internship at Facebook, I needed to learn PySpark quickly for a data processing task. The existing pipeline was taking 12 hours to process daily user engagement data. I dedicated my first week to completing Spark tutorials and pair programming with experienced team members. I broke down the learning process into small, manageable tasks: first understanding RDD operations, then DataFrames, and finally optimization techniques. Within two weeks, I successfully refactored the pipeline to run in 2 hours. This experience taught me how to rapidly acquire new skills while delivering results."
Technical,ML,How would you design and evaluate a multi-armed bandit system?,"At Booking.com, I implemented an epsilon-greedy bandit algorithm for hotel price optimization. I started with five arms representing different pricing strategies. The exploration phase (=0.2) ran for two weeks to gather baseline data. I then implemented Thompson sampling to balance exploration/exploitation more effectively. Key metrics included conversion rate and revenue per session. To validate the approach, I set up a proper A/B test comparing the bandit system against our static pricing. The bandit system achieved 15% higher revenue while automatically adapting to market changes. I also implemented safeguards to prevent extreme price variations."
Technical,ML,Design a real-time anomaly detection system for a large e-commerce platform.,"For this system, I'd implement a lambda architecture with three layers: - Speed Layer: Uses Kafka streams to process incoming transaction data in real-time, applying simple statistical rules for immediate flagging of obvious fraudulent patterns. - Batch Layer: Daily model retraining using Spark, incorporating more complex features and patterns identified from historical data. - Serving Layer: Combines insights from both layers using Redis for real-time serving."
Behavioral,Soft Skills,Describe a situation where you had to convince stakeholders to adopt a different approach based on data.,"At Twitter, I discovered that our current A/B testing approach was suffering from network effects, leading to underestimated treatment effects. I prepared a detailed analysis showing how user interactions were contaminating our control group. I created a simulation demonstrating the bias and proposed switching to cluster-based randomization. Initially, there was resistance due to reduced sample size. I organized workshops with product managers to explain the tradeoffs and long-term benefits. After a successful pilot, the new methodology became our standard approach, leading to more accurate measurement of social features' impact."
Technical,ML,How would you approach building a large-scale personalization system?,"At Netflix, I led the development of a personalized content ranking system. The architecture involved: - Feature Engineering Pipeline: Processing user behavior, content metadata, and contextual information using Apache Beam - Model Development: Implemented a two-tower neural network architecture separating user and content embeddings - Serving Infrastructure: Used TensorFlow Serving with Redis cache for low-latency predictions"
Technical,ML,How do you approach building and maintaining machine learning platforms for large organizations?,"At Airbnb, I architected our ML platform serving 100+ data scientists. The key components included: - Feature Store: Built on Spark and MongoDB for offline/online feature serving - Model Registry: Version control and A/B testing infrastructure - Automated Training Pipeline: Using Kubernetes for distributed training - Monitoring System: Real-time model performance and drift detection"
Technical,ML,Explain your approach to solving consistency issues in distributed machine learning systems.,"When leading PayPal's fraud detection system, I tackled consistency challenges across our globally distributed system. I implemented: - Two-phase commit protocol for model updates - Vector clock synchronization for feature consistency - Eventual consistency model with bounded staleness for non-critical features"
Technical,ML,Design a real-time ML feature platform supporting both batch and streaming use cases.,I implemented this at Uber for our pricing engine. The architecture consisted of: - Data Ingestion Layer:Kafka for real-time eventsAirflow for batch processingCustom consistency validation layer - Kafka for real-time events - Airflow for batch processing - Custom consistency validation layer - Processing Layer:Flink for stream processingSpark for batch processingFeature validation and monitoring - Flink for stream processing - Spark for batch processing - Feature validation and monitoring - Serving Layer:Redis for hot featuresCassandra for historical featuresgRPC for service communication - Redis for hot features - Cassandra for historical features - gRPC for service communication - Orchestration Layer:Kubernetes for container orchestrationCustom feature dependency resolverAutomated backfill capability - Kubernetes for container orchestration - Custom feature dependency resolver - Automated backfill capability
Behavioral,Soft Skills,Tell me about a time when you had to make a difficult technical decision that impacted multiple teams.,"At LinkedIn, I led the migration from our monolithic ML platform to a microservices architecture. The challenge was maintaining service while transitioning 200+ models used by 15 teams. I: - Created a detailed impact analysis and migration plan - Developed a phased approach with clear success metrics - Built a consensus through technical working groups - Implemented automated testing and rollback procedures"
Technical,ML,How do you approach designing experimentation frameworks for complex ecosystems with network effects?,"At Meta, I designed the experimentation framework for the News Feed ranking system. The challenges included: - Network effects contaminating control groups - Long-term effects not captured in standard A/B tests - Multiple competing objectives across different stakeholder groups"
Technical,ML,Design a next-generation ML platform that supports automated ML operations and governance.,"Based on my experience building Google's internal ML platform, I would design: - Infrastructure Layer:Cloud-agnostic resource abstractionAutomated infrastructure scalingMulti-region deployment support - Cloud-agnostic resource abstraction - Automated infrastructure scaling - Multi-region deployment support - ML Lifecycle Management:Automated feature discovery and validationNeural architecture search capabilitiesContinuous training and deployment - Automated feature discovery and validation - Neural architecture search capabilities - Continuous training and deployment - Governance Layer:Automated bias detection and mitigationModel explainability frameworkCompliance and audit trail system - Automated bias detection and mitigation - Model explainability framework - Compliance and audit trail system - Monitoring and Maintenance:Automated drift detection and adaptationSelf-healing capabilitiesPerformance optimization suggestions - Automated drift detection and adaptation - Self-healing capabilities - Performance optimization suggestions"
Behavioral,Soft Skills,Describe how you've influenced the technical strategy of your organization.,"As Principal Data Scientist at Microsoft, I led the transformation of our AI strategy across cloud services. Key initiatives: - Technical Strategy:Developed five-year roadmap for AI servicesCreated framework for responsible AI developmentEstablished technical governance structure - Developed five-year roadmap for AI services - Created framework for responsible AI development - Established technical governance structure - Organization Impact:Built Centers of Excellence for key AI domainsCreated mentorship program reaching 200+ scientistsEstablished cross-functional AI review board - Built Centers of Excellence for key AI domains - Created mentorship program reaching 200+ scientists - Established cross-functional AI review board - External Impact:Published 5 papers at top conferencesFiled 12 patentsEstablished industry partnerships - Published 5 papers at top conferences - Filed 12 patents - Established industry partnerships"
Technical,ML,How do you approach building and scaling data science teams?,"At Amazon, I grew the ML team from 5 to 50 members through: - Talent Strategy:Created specialized tracks (Research, Applied, ML Platform)Developed comprehensive interview frameworkEstablished promotion criteria aligned with business impact - Created specialized tracks (Research, Applied, ML Platform) - Developed comprehensive interview framework - Established promotion criteria aligned with business impact - Team Structure:Implemented pod-based organizationCreated rotation program for knowledge sharingEstablished technical advisory board - Implemented pod-based organization - Created rotation program for knowledge sharing - Established technical advisory board - Development Framework:Built internal training curriculumCreated mentorship programEstablished conference presentation opportunities - Built internal training curriculum - Created mentorship program - Established conference presentation opportunities"
Technical,Deep Learning,What are autoencoders? Explain the different layers of autoencoders and mention three practical usages of them?,"Autoencoders are one of the deep learning types used for unsupervised learning. There are key layers of autoencoders, which are the input layer, encoder, bottleneck hidden layer, decoder, and output. The three layers of the autoencoder are:- 1) Encoder - Compresses the input data to an encoded representation which is typically much smaller than the input data. 2) Latent Space Representation/ Bottleneck/ Code - Compact summary of the input containing the most important features 3) Decoder - Decompresses the knowledge representation and reconstructs the data back from its encoded form. Then a loss function is used at the top to compare the input and output images. NOTE- It's a requirement that the dimensionality of the input and output be the same. Everything in the middle can be played with. Autoencoders have a wide variety of usage in the real world. The following are some of the popular ones: 1. Transformers and Big Bird (Autoencoders is one of these components in both algorithms): Text Summarizer, Text Generator 2. Image compression 3. Nonlinear version of PCA"
Technical,Deep Learning,What is an activation function and discuss the use of an activation function? Explain three different types of activation functions?,"In mathematical terms, the activation function serves as a gate between the current neuron input and its output, going to the next level. Basically, it decides whether neurons should be activated or not. It is used to introduce non-linearity into a model. Activation functions are added to introduce non-linearity to the network, it doesn't matter how many layers or how many neurons your net has, the output will be linear combinations of the input in the absence of activation functions. In other words, activation functions are what make a linear regression model different from a neural network. We need non-linearity, to capture more complex features and model more complex variations that simple linear models can not capture. There are a lot of activation functions: * Sigmoid function: f(x) = 1/(1+exp(-x)) The output value of it is between 0 and 1, we can use it for classification. It has some problems like the gradient vanishing on the extremes, also it is computationally expensive since it uses exp. * Relu: f(x) = max(0,x) it returns 0 if the input is negative and the value of the input if the input is positive. It solves the problem of vanishing gradient for the positive side, however, the problem is still on the negative side. It is fast because we use a linear function in it. * Leaky ReLU: F(x)= ax, x<0 F(x)= x, x>=0 It solves the problem of vanishing gradient on both sides by returning a value a on the negative side and it does the same thing as ReLU for the positive side. * Softmax: it is usually used at the last layer for a classification problem because it returns a set of probabilities, where the sum of them is 1. Moreover, it is compatible with cross-entropy loss, which is usually the loss function for classification problems."
Technical,Deep Learning,"You are using a deep neural network for a prediction task. After training your model, you notice that it is strongly overfitting the training set and that the performance on the test isnt good. What can you do to reduce overfitting? ### To reduce overfitting in a deep neural network changes can be made in three places/stages: The input data to the network, the network architecture, and the training process: 1. The input data to the network: * Check if all the features are available and reliable * Check if the training sample distribution is the same as the validation and test set distribution. Because if there is a difference in validation set distribution then it is hard for the model to predict as these complex patterns are unknown to the model. * Check for train / valid data contamination (or leakage) * The dataset size is enough, if not try data augmentation to increase the data size * The dataset is balanced 2. Network architecture: * Overfitting could be due to model complexity. Question each component: * can fully connect layers be replaced with convolutional + pooling layers? * what is the justification for the number of layers and number of neurons chosen? Given how hard it is to tune these, can a pre-trained model be used? * Add regularization - lasso (l1), ridge (l2), elastic net (both) * Add dropouts * Add batch normalization 3. The training process: * Improvements in validation losses should decide when to stop training. Use callbacks for early stopping when there are no significant changes in the validation loss and restore_best_weights. ### Q4: Why should we use Batch Normalization? ### Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. Usually, a dataset is fed into the network in the form of batches where the distribution of the data differs for every batch size. By doing this, there might be chances of vanishing gradient or exploding gradient when it tries to backpropagate. In order to combat these issues, we can use BN (with irreducible error) layer mostly on the inputs to the layer before the activation function in the previous layer and after fully connected layers. Batch Normalisation has the following effects on the Neural Network: 1. Robust Training of the deeper layers of the network. 2. Better covariate-shift proof NN Architecture. 3. Has a slight regularisation effect. 4. Centred and Controlled values of Activation. 5. Tries to Prevent exploding/vanishing gradient. 6. Faster Training/Convergence to the minimum loss function ![Alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/Batch%20normalization.jpg) ### Q5: How to know whether your model is suffering from the problem of Exploding Gradients? ### By taking incremental steps towards the minimal value, the gradient descent algorithm aims to minimize the error. The weights and biases in a neural network are updated using these processes. However, at times, the steps grow excessively large, resulting in increased updates to weights and bias terms to the point where the weights overflow (or become NaN, that is, Not a Number). An exploding gradient is the result of this, and it is an unstable method. There are some subtle signs that you may be suffering from exploding gradients during the training of your network, such as: 1. The model is unable to get traction on your training data (e g. poor loss). 2. The model is unstable, resulting in large changes in loss from update to update. 3. The model loss goes to NaN during training. If you have these types of problems, you can dig deeper to see if you have a problem with exploding gradients. There are some less subtle signs that you can use to confirm that you have exploding gradients: 1. The model weights quickly become very large during training. 2. The model weights go to NaN values during training. 3. The error gradient values are consistently above 1.0 for each node and layer during training. ### Q6: Can you name and explain a few hyperparameters used for training a neural network?","Hyperparameters are any parameter in the model that affects the performance but is not learned from the data unlike parameters ( weights and biases), the only way to change it is manually by the user. 1. Number of nodes: number of inputs in each layer. 2. Batch normalization: normalization/standardization of inputs in a layer. 3. Learning rate: the rate at which weights are updated. 4. Dropout rate: percent of nodes to drop temporarily during the forward pass. 5. Kernel: matrix to perform dot product of image array with 6. Activation function: defines how the weighted sum of inputs is transformed into outputs (e.g. tanh, sigmoid, softmax, Relu, etc) 7. Number of epochs: number of passes an algorithm has to perform for training 8. Batch size: number of samples to pass through the algorithm individually. E.g. if the dataset has 1000 records and we set a batch size of 100 then the dataset will be divided into 10 batches which will be propagated to the algorithm one after another. 9. Momentum: Momentum can be seen as a learning rate adaptation technique that adds a fraction of the past update vector to the current update vector. This helps damps oscillations and speed up progress towards the minimum. 10. Optimizers: They focus on getting the learning rate right. * Adagrad optimizer: Adagrad uses a large learning rate for infrequent features and a smaller learning rate for frequent features. * Other optimizers, like Adadelta, RMSProp, and Adam, make further improvements to fine-tuning the learning rate and momentum to get to the optimal weights and bias. Thus getting the learning rate right is key to well-trained models. 11. Learning Rate: Controls how much to update weights & bias (w+b) terms after training on each batch. Several helpers are used to getting the learning rate right."
Technical,Deep Learning,Can you explain the parameter sharing concept in deep learning?,"Parameter sharing is the method of sharing weights by all neurons in a particular feature map. Therefore helps to reduce the number of parameters in the whole system, making it computationally cheap. It basically means that the same parameters will be used to represent different transformations in the system. This basically means the same matrix elements may be updated multiple times during backpropagation from varied gradients. The same set of elements will facilitate transformations at more than one layer instead of those from a single layer as conventional. This is usually done in architectures like Siamese that tend to have parallel trunks trained simultaneously. In that case, using shared weights in a few layers( usually the bottom layers) helps the model converge better. This behavior, as observed, can be attributed to more diverse feature representations learned by the system. Since neurons corresponding to the same features are triggered in varied scenarios. Helps to model to generalize better. Note that sometimes the parameter sharing assumption may not make sense. This is especially the case when the input images to a ConvNet have some specific centered structure, where we should expect, for example, that completely different features should be learned on one side of the image than another. One practical example is when the input is faces that have been centered in the image. You might expect that different eye-specific or hair-specific features could (and should) be learned in different spatial locations. In that case, it is common to relax the parameter sharing scheme, and instead, simply call the layer a Locally-Connected Layer."
Technical,Deep Learning,Describe the architecture of a typical Convolutional Neural Network (CNN)?,"In a typical CNN architecture, a few convolutional layers are connected in a cascade style. Each convolutional layer is followed by a Rectified Linear Unit (ReLU) layer or other activation function, then a pooling layer*, then one or more convolutional layers (+ReLU), then another pooling layer. The output from each convolution layer is a set of objects called feature maps, generated by a single kernel filter. The feature maps are used to define a new input to the next layer. A common trend is to keep on increasing the number of filters as the size of the image keeps dropping as it passes through the Convolutional and Pooling layers. The size of each kernel filter is usually 33 kernel because it can extract the same features which extract from large kernels and faster than them. After that, the final small image with a large number of filters(which is a 3D output from the above layers) is flattened and passed through fully connected layers. At last, we use a softmax layer with the required number of nodes for classification or use the output of the fully connected layers for some other purpose depending on the task. The number of these layers can increase depending on the complexity of the data and when they increase you need more data. Stride, Padding, Filter size, Type of Pooling, etc all are Hyperparameters and need to be chosen (maybe based on some previously built successful models) *Pooling: it is a way to reduce the number of features by choosing a number to represent its neighbor. And it has many types max-pooling, average pooling, and global average. * Max pooling: it takes the max number of window 22 as an example and represents this window by using the max number in it then slides on the image to make the same operation. * Average pooling: it is the same as max-pooling but takes the average of the window."
Technical,Deep Learning,What is the Vanishing Gradient Problem in Artificial Neural Networks and How to fix it?,"The vanishing gradient problem is encountered in artificial neural networks with gradient-based learning methods and backpropagation. In these learning methods, each of the weights of the neural network receives an update proportional to the partial derivative of the error function with respect to the current weight in each iteration of training. Sometimes when gradients become vanishingly small, this prevents the weight to change value. When the neural network has many hidden layers, the gradients in the earlier layers will become very low as we multiply the derivatives of each layer. As a result, learning in the earlier layers becomes very slow. . This problem of vanishing gradient descent happens when training neural networks with many layers because the gradient diminishes dramatically as it propagates backward through the network. Some ways to fix it are: 1. Use skip/residual connections. 2. Using ReLU or Leaky ReLU over sigmoid and tanh activation functions. 3. Use models that help propagate gradients to earlier time steps like in GRUs and LSTMs."
Technical,Deep Learning,"When it comes to training an artificial neural network, what could be the reason why the loss doesn't decrease in a few epochs?",Some of the reasons why the loss doesn't decrease after a few Epochs are: a) The model is under-fitting the training data. b) The learning rate of the model is large. c) The initialization is not proper (like all the weights initialized with 0 doesn't make the network learn any function) d) The Regularisation hyper-parameter is quite large. e). The classic case of vanishing gradients
Technical,Deep Learning,Why Sigmoid or Tanh is not preferred to be used as the activation function in the hidden layer of the neural network?,"A common problem with Tanh or Sigmoid functions is that they saturate. Once saturated, the learning algorithms cannot adapt to the weights and enhance the performance of the model. Thus, Sigmoid or Tanh activation functions prevent the neural network from learning effectively leading to a vanishing gradient problem. The vanishing gradient problem can be addressed with the use of Rectified Linear Activation Function (ReLu) instead of sigmoid and Tanh."
Technical,Deep Learning,Discuss in what context it is recommended to use transfer learning and when it is not.,"Transfer learning is a machine learning method where a model developed for a task is reused as the starting point for a model on a second task. It is a popular approach in deep learning where pre-trained models are used as the starting point for computer vision and natural language processing tasks given the vast computing and time resources required to develop neural network models on these problems and from the huge jumps in a skill that they provide on related problems. Transfer learning is used for tasks where the data is too little to train a full-scale model from the beginning. In transfer learning, well-trained, well-constructed networks are used which have learned over large sets and can be used to boost the performance of a dataset. : 1. The downstream task has a very small amount of data available, then we can try using pre-trained model weights by switching the last layer with new layers which we will train. 2. In some cases, like in vision-related tasks, the initial layers have a common behavior of detecting edges, then a little more complex but still abstract features and so on which is common in all vision tasks, and hence a pre-trained model's initial layers can be used directly. The same thing holds for Language Models too, for example, a model trained in a large Hindi corpus can be transferred and used for other Indo-Aryan Languages with low resources available. : 1. The first and most important is the ""COST"". So is it cost-effective or we can have a similar performance without using it. 2. The pre-trained model has no relation to the downstream task. 3. If the latency is a big constraint (Mostly in NLP ) then transfer learning is not the best option. However Now with the TensorFlow lite kind of platform and Model Distillation, Latency is not a problem anymore."
Technical,Deep Learning,Discuss the vanishing gradient in RNN and How they can be solved.,"In Sequence to Sequence models such as RNNs, the input sentences might have long-term dependencies for example we might say ""The boy who was wearing a red t-shirt, blue jeans, black shoes, and a white cap and who lives at ... and is 10 years old ...... etc, is genius"" here the verb (is) in the sentence depends on the (boy) i.e if we say (The boys, ......, are genius"". When training an RNN we do backward propagation both through layers and backward through time. Without focusing too much on mathematics, during backward propagation we tend to multiply gradients that are either > 1 or < 1, if the gradients are < 1 and we have about 100 steps backward in time then multiplying 100 numbers that are < 1 will result in a very very tiny gradient causing no change in the weights as we go backward in time (0.1 * 0.1 * 0.1 * .... a 100 times = 10^(-100)) such that in our previous example the word ""is"" doesn't affect its main dependency the word ""boy"" during learning the meanings of the word due to the long description in between. Models like the Gated Recurrent Units (GRUs) and the Long short-term memory (LSTMs) were proposed, the main idea of these models is to use gates to help the network determine which information to keep and which information to discard during learning. Then Transformers were proposed depending on the self-attention mechanism to catch the dependencies between words in the sequence."
Technical,Deep Learning,What are the main gates in LSTM and what are their tasks?,"There are 3 main types of gates in a LSTM Model, as follows: - Forget Gate - Input/Update Gate - Output Gate 1) Forget Gate:- It helps in deciding which data to keep or thrown out 2) Input Gate:- it helps in determining whether new data should be added in long term memory cell given by previous hidden state and new input data 3) Output Gate:- this gate gives out the new hidden state Common things for all these gates are they all take take inputs as the current temporal state/input/word/observation and the previous hidden state output and sigmoid activation is mostly used in all of these."
Technical,Deep Learning,Is it a good idea to use CNN to classify1Dsignal?,"For time-series data, where we assume temporal dependence between the values, then convolutional neural networks (CNN) are one of the possible approaches. However the most popular approach to such data is to use recurrent neural networks (RNN), but you can alternatively use CNNs, or a hybrid approach (quasi-recurrent neural networks, QRNN). With **CNN**, you would use sliding windows of some width, that would look at certain (learned) patterns in the data, and stack such windows on top of each other, so that higher-level windows would look for patterns within the lower-level patterns. Using such sliding windows may be helpful for finding things such as repeating patterns within the data. One drawback is that it doesn't take into account the temporal or sequential aspect of the 1D signals, which can be very important for prediction. With **RNN**, you would use a cell that takes as input the previous hidden state and current input value, to return output and another hidden form, so the information flows via the hidden states and takes into account the temporal dependencies. **QRNN** layers mix both approaches."
Technical,Deep Learning,How does L1/L2 regularization affect a neural network?,"Overfitting occurs in more complex neural network models (many layers, many neurons) and the complexity of the neural network can be reduced by using L1 and L2 regularization as well as dropout , Data augmenration and Dropaout. L1 regularization forces the weight parameters to become zero. L2 regularization forces the weight parameters towards zero (but never exactly zero|| weight deccay ) Smaller weight parameters make some neurons neglectable therfore neural network becomes less complex and less overfitting. Regularisation has the following benefits: - Reducing the variance of the model over unseen data. - Makes it feasible to fit much more complicated models without overfitting. - Reduces the magnitude of weights and biases. - L1 learns sparse models that is many weights turn out to be 0. -"
Technical,Deep Learning,- ?,"Using transfer learning where we can use our knowledge about one task to do another. First set of layers of a neural network are usually feature extraction layers and will be useful for all tasks with the same input distribution. So, we should replace the last fully connected layer and Softmax responsible for classification with one neuron for regression-or fully connected-layer for correction then one neuron for regression. We can optionally freeze the first set of layers if we have few data or to converge fast. Then we can train the network with the data we have and using the suitable loss for the regression problem, making use of the robust feature extraction -first set of layers- of a pre-trained model on huge data."
Technical,Deep Learning,"What might happen if you set the momentum hyperparameter too close to 1 (e.g., 0.9999) when using an SGD optimizer?","If the momentum hyperparameter is set too close to 1 (e.g., 0.99999) when using an SGD optimizer, then the algorithm will likely pick up a lot of speed, hopefully moving roughly toward the global minimum, but its momentum will carry it right past the minimum. Then it will slow down and come back, accelerate again, overshoot again, and so on. It may oscillate this way many times before converging, so overall it will take much longer to converge than with a smaller momentum value. Also since the momentum is used to update the weights based on an ""exponential moving average"" of all the previous gradients instead of the current gradient only, this in some sense, combats the instability of the gradients that comes with stochastic gradient descent, the higher the momentum term, the stronger the influence of previous gradients to the current optimization step (with the more recent gradients having even stronger influence), setting a momentum term close to 1, will result in a gradient that is almost a sum of all the previous gradients basically, which might result in an exploding gradient scenario."
Technical,Deep Learning,What are the hyperparameters that can be optimized for the batch normalization layer?,"The $\gamma$ and $\beta$ hyperparameters for the batch normalization layer are learned end to end by the network. In batch-normalization, the outputs of the intermediate layers are normalized to have a mean of 0 and standard deviation of 1. Rescaling by $\gamma$ and shifting by $\beta$ helps us change the mean and standard deviation to other values."
Technical,Deep Learning,What is the effect of dropout on the training and prediction speed of your deep learning model?,"Dropout is a regularization technique, which zeroes down some weights and scales up the rest of the weights by a factor of 1/(1-p). Let's say if Dropout layer is initialized with p=0.5, that means half of the weights will zeroed down, and rest will be scaled by a factor of 2. This layer is only enabled during training and is disabled during validation and testing. Hence validation and testing is faster. The reason why it works only during training is, we want to reduce the complexity of the model so that model doesn't overfit. Once the model is trained, it doesn't make sense to keep that layer enabled."
Technical,Deep Learning,What is the advantage of deep learning over traditional machine learning?,"Deep learning offers several advantages over traditional machine learning approaches, including: 1. Ability to process large amounts of data: Deep learning models can analyze and process massive amounts of data quickly and accurately, making it ideal for tasks such as image recognition or natural language processing. 2. Automated feature extraction: In traditional machine learning, feature engineering is a crucial step in the model building process. Deep learning models, on the other hand, can automatically learn and extract features from the raw data, reducing the need for human intervention. 3. Better accuracy: Deep learning models have shown to achieve higher accuracy levels in complex tasks such as speech recognition and image classification when compared to traditional machine learning models. 4. Adaptability to new data: Deep learning models can adapt and learn from new data, making them suitable for use in dynamic and ever-changing environments. While deep learning does have its advantages, it also has some limitations, such as requiring large amounts of data and computational resources, making it unsuitable for some applications."
Technical,Deep Learning,What is a depthwise Separable layer and what are its advantages?,"Standard neural network Convolution layers involve a lot of multiplications that make them unsuitable for deployment. In this above scenario, we have an input image of 12x12x3 pixels and we apply a 5x5 convolution(no padding, stride = 1). We stack 256 such kernels so that we get an output of dimensions 8x8x256. Here, there are 256 5x5x3 kernels that move 8x8 times which leads to 256x3x5x5x8x8 = 1,28,800 multiplications. Depthwise separable convolution separates this process into two parts: a depthwise convolution and a pointwise convolution. In depthwise convolution, we apply a kernel parallelly to each channel of the image. We end up getting 3 different outputs (representing 3 channels of the image) to get an 8x8x1 image. These are stacked together to form a 8x8x3 image. Pointwise Convolution now converts this 8x8x3 image input from the depthwise convolution back to an 8x8x1 output. Stacking 256 1x1x3 kernels give us the final output as the standard convolution. Total Number of multiplications: For Depthwise convolution, we have 3 5x5x1 kernels moving 8x8 times, totalling 3x5x5x8x8=4800 multiplications. In Pointwise convolution, we have 256 1x1x3 kernels moving 8x8 times, which is a total of 256x1x1x3x8x8=49152 multiplications. Total number of multiplications = 4800 + 49152 = 53952 multiplications which is way lower than the standard convolution case. Reference: # Natural Language Processing # ## Q23: What is transformer architecture, and why is it widely used in natural language processing tasks? ## Answer: The key components of a transformer architecture are as follows: 1. Encoder: The encoder processes the input sequence, such as a sentence or a document, and transforms it into a set of representations that capture the contextual information of each input element. The encoder consists of multiple identical layers, each containing a self-attention mechanism and position-wise feed-forward neural networks. The self-attention mechanism allows the model to attend to different parts of the input sequence while encoding it. 2. Decoder: The decoder takes the encoded representations generated by the encoder and generates an output sequence. It also consists of multiple identical layers, each containing a self-attention mechanism and additional cross-attention mechanisms. The cross-attention mechanisms enable the decoder to attend to relevant parts of the encoded input sequence when generating the output. 3. Self-Attention: Self-attention is a mechanism that allows the transformer to weigh the importance of different elements in the input sequence when generating representations. It computes attention scores between each element and every other element in the sequence, resulting in a weighted sum of the values. This process allows the model to capture dependencies and relationships between different elements in the sequence. 4. Positional Encoding: Transformers incorporate positional encoding to provide information about the order or position of elements in the input sequence. This encoding is added to the input embeddings and allows the model to understand the sequential nature of the data. 5. Feed-Forward Networks: Transformers utilize feed-forward neural networks to process the representations generated by the attention mechanisms. These networks consist of multiple layers of fully connected neural networks with activation functions, enabling non-linear transformations of the input representations. The transformer architecture is widely used in NLP tasks due to several reasons: Self-Attention Mechanism: Transformers leverage a self-attention mechanism that allows the model to focus on different parts of the input sequence during processing. This mechanism enables the model to capture long-range dependencies and contextual information efficiently, making it particularly effective for tasks that involve understanding and generating natural language. Parallelization: Transformers can process the elements of a sequence in parallel, as opposed to recurrent neural networks (RNNs) that require sequential processing. This parallelization greatly accelerates training and inference, making transformers more computationally efficient. Scalability: Transformers scale well with the length of input sequences, thanks to the self-attention mechanism. Unlike RNNs, transformers do not suffer from the vanishing or exploding gradient problem, which can hinder the modeling of long sequences. This scalability makes transformers suitable for tasks that involve long texts or documents. Transfer Learning: Transformers have shown great success in pre-training and transfer learning. Models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer) are pre-trained on massive amounts of text data, enabling them to learn rich representations of language. These pre-trained models can then be fine-tuned on specific downstream tasks with comparatively smaller datasets, leading to better generalization and improved performance. Contextual Understanding: Transformers excel in capturing the contextual meaning of words and sentences. By considering the entire input sequence simultaneously, transformers can generate more accurate representations that incorporate global context, allowing for better language understanding and generation. ## Q24: Explain the key components of a transformer model. ## Answer: A transformer model consists of several key components that work together to process and generate representations for input sequences. The main components of a transformer model are as follows: * Encoder: The encoder is responsible for processing the input sequence and generating representations that capture the contextual information of each element. It consists of multiple identical layers, typically stacked on top of each other. Each layer contains two sub-layers: a self-attention mechanism and a position-wise feed-forward neural network. * Self-Attention Mechanism: This mechanism allows the model to attend to different parts of the input sequence while encoding it. It computes attention scores between each element and every other element in the sequence, resulting in a weighted sum of values. This process allows the model to capture dependencies and relationships between different elements. * Position-wise Feed-Forward Neural Network: After the self-attention mechanism, a feed-forward neural network is applied to each position separately. It consists of fully connected layers with activation functions, enabling non-linear transformations of the input representations. * Decoder: The decoder takes the encoded representations generated by the encoder and generates an output sequence. It also consists of multiple identical layers, each containing sub-layers such as self-attention, cross-attention, and position-wise feed-forward networks. * Self-Attention Mechanism: Similar to the encoder, the decoder uses self-attention to attend to different parts of the decoded sequence while generating the output. It allows the decoder to consider the previously generated elements in the output sequence when generating the next element. * Cross-Attention Mechanism: In addition to self-attention, the decoder employs cross-attention to attend to relevant parts of the encoded input sequence. It allows the decoder to align and extract information from the encoded sequence when generating the output. * Self-Attention and Cross-Attention: These attention mechanisms are fundamental components of the transformer architecture. They enable the model to weigh the importance of different elements in the input and output sequences when generating representations. Attention scores are computed by measuring the compatibility between elements, and the weighted sum of values is used to capture contextual dependencies. * Positional Encoding: Transformers incorporate positional encoding to provide information about the order or position of elements in the input sequence. It is added to the input embeddings and allows the model to understand the sequential nature of the data. * Residual Connections and Layer Normalization: Transformers employ residual connections and layer normalization to facilitate the flow of information and improve gradient propagation. Residual connections enable the model to capture both high-level and low-level features, while layer normalization normalizes the inputs to each layer, improving the stability and performance of the model. These components collectively enable the transformer model to process and generate representations for input sequences in an efficient and effective manner. The self-attention mechanisms, along with the feed-forward networks and positional encoding, allow the model to capture long-range dependencies, handle the parallel processing, and generate high-quality representations, making transformers highly successful in natural language processing tasks. ## Q25: What is self-attention, and how does it work in transformers? ## Answer: ## Q26: What are the advantages of transformers over traditional sequence-to-sequence models? ## Answer: Transformers have several advantages over traditional sequence-to-sequence models, such as recurrent neural networks (RNNs), when it comes to natural language processing tasks. Here are some key advantages: * Long-range dependencies: Transformers are capable of capturing long-range dependencies in sequences more effectively compared to RNNs. This is because RNNs suffer from vanishing or exploding gradient problems when processing long sequences, which limits their ability to capture long-term dependencies. Transformers address this issue by using self-attention mechanisms that allow for capturing relationships between any two positions in a sequence, regardless of their distance. * Parallelization: Transformers can process inputs in parallel, making them more efficient in terms of computational time compared to RNNs. In RNNs, the sequential nature of computation limits parallelization since each step depends on the previous step's output. Transformers, on the other hand, process all positions in a sequence simultaneously, enabling efficient parallelization across different positions. * Scalability: Transformers are highly scalable and can handle larger input sequences without significantly increasing computational requirements. In RNNs, the computational complexity grows linearly with the length of the input sequence, making it challenging to process long sequences efficiently. Transformers, with their parallel processing and self-attention mechanisms, maintain a constant computational complexity, making them suitable for longer sequences. * Global context understanding: Transformers capture global context information effectively due to their attention mechanisms. Each position in the sequence attends to all other positions, allowing for a comprehensive understanding of the entire sequence during the encoding and decoding process. This global context understanding aids in various NLP tasks, such as machine translation, where the translation of a word can depend on the entire source sentence. * Transfer learning and fine-tuning: Transformers facilitate transfer learning and fine-tuning, which is the ability to pre-train models on large-scale datasets and then adapt them to specific downstream tasks with smaller datasets. Pretraining transformers on massive amounts of data, such as in models like BERT or GPT, helps capture rich language representations that can be fine-tuned for a wide range of NLP tasks, providing significant performance gains. ## Q27: How does the attention mechanism help transformers capture long-range dependencies in sequences? ## Answer: The attention mechanism in transformers plays a crucial role in capturing long-range dependencies in sequences. It allows each position in a sequence to attend to other positions, enabling the model to focus on relevant parts of the input during both the encoding and decoding stages. Here's how the attention mechanism works in transformers: * Self-Attention: Self-attention, also known as intra-attention, is the key component of the attention mechanism in transformers. It computes the importance, or attention weight, that each position in the sequence should assign to other positions. This attention weight determines how much information a position should gather from other positions. * Query, Key, and Value: To compute self-attention, each position in the sequence is associated with three learned vectors: query, key, and value. These vectors are derived from the input embeddings and transformed through linear transformations. The query vector is used to search for relevant information, the key vector represents the positions to which the query attends, and the value vector holds the information content of each position. * Attention Scores: The attention mechanism calculates attention scores between the query vector of a position and the key vectors of all other positions in the sequence. The attention scores quantify the relevance or similarity between positions. They are obtained by taking the dot product between the query and key vectors and scaling it by a factor of the square root of the dimensionality of the key vectors. * Attention Weights: The attention scores are then normalized using the softmax function to obtain attention weights. These weights determine the contribution of each position to the final representation of the current position. Positions with higher attention weights have a stronger influence on the current position's representation. * Weighted Sum: Finally, the attention weights are used to compute a weighted sum of the value vectors. This aggregation of values gives the current position a comprehensive representation that incorporates information from all relevant positions, capturing the long-range dependencies effectively. By allowing each position to attend to other positions, the attention mechanism provides a mechanism for information to flow across the entire sequence. This enables transformers to capture dependencies between distant positions, even in long sequences, without suffering from the limitations of vanishing or exploding gradients that affect traditional recurrent neural networks. Consequently, transformers excel in modeling complex relationships and dependencies in sequences, making them powerful tools for various tasks, including natural language processing and computer vision. ## Q28: What are the limitations of transformers, and what are some potential solutions? ## Answer: While transformers have revolutionized many natural language processing tasks, they do have certain limitations. Here are some notable limitations of transformers and potential solutions: * Sequential Computation: Transformers process the entire sequence in parallel, which limits their ability to model sequential information explicitly. This can be a disadvantage when tasks require strong sequential reasoning. Potential solutions include incorporating recurrent connections into transformers or using hybrid models that combine the strengths of transformers and recurrent neural networks. * Memory and Computational Requirements: Transformers consume more memory and computational resources compared to traditional sequence models, especially for large-scale models and long sequences. This limits their scalability and deployment on resource-constrained devices. Solutions involve developing more efficient architectures, such as sparse attention mechanisms or approximations, to reduce memory and computational requirements without sacrificing performance significantly. * Lack of Interpretability: Transformers are often considered as black-box models, making it challenging to interpret the reasoning behind their predictions. Understanding the decision-making process of transformers is an ongoing research area. Techniques such as attention visualization, layer-wise relevance propagation, and saliency maps can provide insights into the model's attention and contribution to predictions, enhancing interpretability. * Handling Out-of-Distribution Data: Transformers can struggle with data that significantly deviates from the distribution seen during training. They may make overconfident predictions or produce incorrect outputs when faced with out-of-distribution samples. Solutions include exploring uncertainty estimation techniques, robust training approaches, or incorporating external knowledge sources to improve generalization and handle out-of-distribution scenarios. * Limited Contextual Understanding: Transformers rely heavily on context information to make predictions. However, they can still struggle with understanding the broader context, especially in scenarios with complex background knowledge or multi-modal data. Incorporating external knowledge bases, leveraging graph neural networks, or combining transformers with other modalities like images or graphs can help improve contextual understanding and capture richer representations. * Training Data Requirements: Transformers typically require large amounts of labeled data for effective training due to their high capacity. Acquiring labeled data can be expensive and time-consuming, limiting their applicability to domains with limited labeled datasets. Solutions include exploring semi-supervised learning, active learning, or transfer learning techniques to mitigate the data requirements and leverage pretraining on large-scale datasets. Researchers and practitioners are actively working on addressing these limitations to further enhance the capabilities and applicability of transformers in various domains. As the field progresses, we can expect continued advancements and novel solutions to overcome these challenges. ## Q29: How are transformers trained, and what is the role of pre-training and fine-tuning? ## Answer: ## Q30: What is BERT (Bidirectional Encoder Representations from Transformers), and how does it improve language understanding tasks? ## Answer: BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based neural network model introduced by Google in 2018. It is designed to improve the understanding of natural language in various language processing tasks, such as question answering, sentiment analysis, named entity recognition, and more. BERT differs from previous language models in its ability to capture the context of a word by considering both the left and right context in a sentence. Traditional language models, like the ones based on recurrent neural networks, process text in a sequential manner, making it difficult to capture the full context. BERT, on the other hand, is a ""pre-trained"" model that is trained on a large corpus of unlabeled text data. During pre-training, BERT learns to predict missing words in sentences by considering the surrounding words on both sides. This bidirectional training allows BERT to capture contextual information effectively. Once pre-training is complete, BERT is fine-tuned on specific downstream tasks. This fine-tuning involves training the model on labeled data from a particular task, such as sentiment analysis or named entity recognition. During fine-tuning, BERT adapts its pre-trained knowledge to the specific task, further improving its understanding and performance. The key advantages of BERT include: 1. Contextual understanding: BERT can capture the contextual meaning of words by considering both the preceding and following words in a sentence, leading to better language understanding. 2. Transfer learning: BERT is pre-trained on a large corpus of unlabeled data, enabling it to learn general language representations. These pre-trained representations can then be fine-tuned for specific tasks, even with limited labeled data. 3. Versatility: BERT can be applied to a wide range of natural language processing tasks. By fine-tuning the model on specific tasks, it can achieve state-of-the-art performance in tasks such as question answering, text classification, and more. 4. Handling ambiguity: BERT's bidirectional nature helps it handle ambiguous language constructs more effectively. It can make more informed predictions by considering the context from both directions. ## Q31: Describe the process of generating text using a transformer-based language model. ## Answer: ## Q32: What are some challenges or ethical considerations associated with large language models? ## Answer: ## Q33: Explain the concept of transfer learning and how it can be applied to transformers. ## Answer: Transfer learning is a machine learning technique where knowledge gained from training on one task is leveraged to improve performance on another related task. Instead of training a model from scratch on a specific task, transfer learning enables the use of pre-trained models as a starting point for new tasks. In the context of transformers, transfer learning has been highly successful, particularly with models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer). Here's how transfer learning is applied to transformers: 1. Pre-training: In the pre-training phase, a transformer model is trained on a large corpus of unlabeled text data. The model is trained to predict missing words in a sentence (masked language modeling) or to predict the next word in a sequence (causal language modeling). This process enables the model to learn general language patterns, syntactic structures, and semantic relationships. 2. Fine-tuning: Once the transformer model is pre-trained, it can be fine-tuned on specific downstream tasks with smaller labeled datasets. Fine-tuning involves retraining the pre-trained model on task-specific labeled data. The model's parameters are adjusted to optimize performance on the specific task, while the pre-trained knowledge acts as a strong initialization for the fine-tuning process. a. Task-specific architecture: During fine-tuning, the architecture of the pre-trained transformer model is often modified or extended to accommodate the specific requirements of the downstream task. For example, in sentiment analysis, an additional classification layer may be added on top of the pre-trained model to classify text sentiment. b. Few-shot or zero-shot learning: Transfer learning with transformers allows for few-shot or even zero-shot learning scenarios. Few-shot learning refers to training a model on a small amount of labeled data, which is beneficial when data availability is limited. Zero-shot learning refers to using the pre-trained model directly on a task for which it hasn't been explicitly trained, but the model can still generate meaningful predictions based on its understanding of language. Transfer learning with transformers offers several advantages: 1. Reduced data requirements: Pre-training on large unlabeled datasets allows the model to capture general language understanding, reducing the need for massive amounts of labeled task-specific data. 2. Improved generalization: The pre-trained model has learned rich representations of language from extensive pre-training, enabling it to generalize well to new tasks and domains. 3. Efficient training: Fine-tuning a pre-trained model requires less computational resources and training time compared to training from scratch. 4. State-of-the-art performance: Transfer learning with transformers has achieved state-of-the-art performance on a wide range of NLP tasks, including text classification, named entity recognition, question answering, machine translation, and more. By leveraging the knowledge encoded in pre-trained transformers, transfer learning enables faster and more effective development of models for specific NLP tasks, even with limited labeled data. ## Q34: How can transformers be used for tasks other than natural language processing, such as computer vision? ## Answer: # Computer Vision # ## Q35: What is computer vision, and why is it important? Answer: ## Q36: Explain the concept of image segmentation and its applications. Answer: ## Q37: What is object detection, and how does it differ from image classification? Answer: ## Q38: Describe the steps involved in building an image recognition system. Answer: ## Q39: What are the challenges in implementing real-time object tracking? Answer: ## Q40: Can you explain the concept of feature extraction in computer vision? Answer: ## Q41: What is optical character recognition (OCR), and what are its main applications? Answer: ## Q42: How does a convolutional neural network (CNN) differ from a traditional neural network in the context of computer vision? Answer: ## Q43: What is the purpose of data augmentation in computer vision, and what techniques can be used? Answer: The purpose of data augmentation in computer vision is to artificially increase the size and diversity of a training dataset by applying various transformations to the original images. Data augmentation helps prevent overfitting and improves the generalization ability of deep learning models by exposing them to a broader range of variations and patterns present in the data. It also reduces the risk of the model memorizing specific examples in the training data. By applying different augmentation techniques, the model becomes more robust and capable of handling variations in the real-world test data that may not be present in the original training set. Common data augmentation techniques include: 1. Horizontal Flipping: Flipping images horizontally, i.e., left to right, or vice versa. This is particularly useful for tasks where the orientation of objects doesn't affect their interpretation, such as object detection or image classification. 2. Vertical Flipping: Similar to horizontal flipping but flipping images from top to bottom. 3. Random Rotation: Rotating images by a random angle. This can be helpful to simulate objects at different angles and orientations. 4. Random Crop: Taking random crops from the input images. This forces the model to focus on different parts of the image and helps in handling varying object scales. 5. Scaling and Resizing: Rescaling images to different sizes or resizing them while maintaining the aspect ratio. This augmentation helps the model handle objects of varying sizes. 6. Color Jittering: Changing the brightness, contrast, saturation, and hue of the images randomly. This augmentation can help the model become more robust to changes in lighting conditions. 7. Gaussian Noise: Adding random Gaussian noise to the images, which simulates noisy environments and enhances the model's noise tolerance. 8. Elastic Transformations: Applying local deformations to the image, simulating distortions that might occur due to variations in the imaging process. 9. Cutout: Randomly masking out portions of the image with black pixels. This helps the model learn to focus on other informative parts of the image. 10. Mixup: Combining two or more images and their corresponding labels in a weighted manner to create new training examples. This encourages the model to learn from the combined patterns of multiple images. It's important to note that the choice of data augmentation techniques depends on the specific computer vision task and the characteristics of the dataset. Additionally, augmentation should be applied only during the training phase and not during testing or evaluation to ensure that the model generalizes well to unseen data. ## Q44: Discuss some popular deep learning frameworks or libraries used for computer vision tasks. Answer:"
